[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Seasonal Forecast Skill Assessment",
    "section": "",
    "text": "Introduction\nThis book evaluates the skill of seasonal precipitation forecasts for detecting drought conditions in Chiquimula, Guatemala - a key region in the Central American Dry Corridor. We compare two forecast sources:",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#agricultural-seasons",
    "href": "index.html#agricultural-seasons",
    "title": "Seasonal Forecast Skill Assessment",
    "section": "Agricultural Seasons",
    "text": "Agricultural Seasons\nGuatemala’s dry corridor has two main agricultural seasons:\n\n\n\nSeason\nMonths\nTypical Crops\n\n\n\n\nPrimera\nMay-August (MJJA)\nFirst maize planting\n\n\nPostrera\nSeptember-November (SON)\nSecond maize planting",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#drought-definition",
    "href": "index.html#drought-definition",
    "title": "Seasonal Forecast Skill Assessment",
    "section": "Drought Definition",
    "text": "Drought Definition\nWe define drought using a return period of 4 years (RP4), meaning:\n\nApproximately 6 drought years expected over the 25-year baseline (2000-2024)\nThreshold calculated separately for each forecast model and observation source\nThis aligns with typical anticipatory action trigger frameworks",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#comparison-frameworks",
    "href": "index.html#comparison-frameworks",
    "title": "Seasonal Forecast Skill Assessment",
    "section": "Comparison Frameworks",
    "text": "Comparison Frameworks\nThroughout this analysis, we use two comparison frameworks:\n\nNative Comparison: Each forecast validated against its typical observation source\n\nSEAS5 vs ERA5 (global reanalysis)\nINSIVUMEH vs ENACTS (station-blended satellite product)\n\nENACTS-only Comparison: All forecasts validated against ENACTS\n\nAllows direct model-to-model comparison\nENACTS is the operational observation source for Guatemala",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#book-structure",
    "href": "index.html#book-structure",
    "title": "Seasonal Forecast Skill Assessment",
    "section": "Book Structure",
    "text": "Book Structure\nChapter 1: Binary Classification Metrics Traditional skill assessment using F1 score, precision, recall. Treats forecasts as binary predictions: drought vs no-drought.\nChapter 2: Continuous Metrics A more nuanced view using correlation, error magnitude, and ROC-AUC. Binary metrics can be noisy with small samples; continuous metrics provide additional perspective on forecast skill. ROC-AUC is particularly interpretable - it directly answers whether forecasts can discriminate drought from non-drought years.\nChapter 3: CHIRPS Tie-Breaker Introduces CHIRPS as a third independent observation source to break ties and increase confidence in recommendations. Uses rank-based metrics (Spearman, AUC) that allow fair comparison across observation sources with different climatologies.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#key-questions",
    "href": "index.html#key-questions",
    "title": "Seasonal Forecast Skill Assessment",
    "section": "Key Questions",
    "text": "Key Questions\n\nWhich forecast source should be used for Primera? For Postrera?\nAt which leadtimes do forecasts show genuine skill?\nDo forecasts beat random guessing?\nWhen forecasts are wrong, how wrong are they?",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_binary_metrics.html",
    "href": "01_binary_metrics.html",
    "title": "1  Binary Classification Metrics",
    "section": "",
    "text": "2 Introduction\nThis chapter evaluates forecast skill using binary classification metrics - the traditional approach where forecasts are judged as either “correct” or “incorrect” based on whether they cross a drought threshold.\nThis analysis compares the skill of INSIVUMEH (regional) and SEAS5 (global) seasonal precipitation forecasts for detecting drought conditions in Chiquimula, Guatemala. We evaluate forecasts for two agricultural seasons:\nWe use two comparison frameworks:\nDrought is defined using a return period of 4 years (RP4), meaning approximately 6 drought years are expected over the 25-year baseline (2000-2024).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#native-comparison",
    "href": "01_binary_metrics.html#native-comparison",
    "title": "1  Binary Classification Metrics",
    "section": "3.1 Native Comparison",
    "text": "3.1 Native Comparison\nSEAS5 validated against ERA5; INSIVUMEH models validated against ENACTS.\n\n\nCode\ncreate_f1_heatmap(\n  df_skill_native,\n  title = \"F1 Score: Native Observation Sources\",\n  subtitle = \"Black outline = best performer per leadtime\",\n  caption = paste0(\"SEAS5 vs ERA5, INSIVUMEH vs ENACTS (\", aoi_name, \")\")\n)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#enacts-only-comparison",
    "href": "01_binary_metrics.html#enacts-only-comparison",
    "title": "1  Binary Classification Metrics",
    "section": "3.2 ENACTS-Only Comparison",
    "text": "3.2 ENACTS-Only Comparison\nAll models validated against ENACTS for direct comparison.\n\n\nCode\ncreate_f1_heatmap(\n  df_skill_enacts,\n  title = \"F1 Score: All Models vs ENACTS\",\n  subtitle = \"Black outline = best performer per leadtime\",\n  caption = paste0(\"All forecasts vs ENACTS (\", aoi_name, \")\")\n)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#primera-season",
    "href": "01_binary_metrics.html#primera-season",
    "title": "1  Binary Classification Metrics",
    "section": "4.1 Primera Season",
    "text": "4.1 Primera Season\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\nplot_yearlt_heatmap(df_yearlt_native, \"primera\", \"native\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_yearlt_heatmap(df_yearlt_enacts, \"primera\", \"enacts_only\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#postrera-season",
    "href": "01_binary_metrics.html#postrera-season",
    "title": "1  Binary Classification Metrics",
    "section": "4.2 Postrera Season",
    "text": "4.2 Postrera Season\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\nplot_yearlt_heatmap(df_yearlt_native, \"postrera\", \"native\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_yearlt_heatmap(df_yearlt_enacts, \"postrera\", \"enacts_only\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#native-comparison-3",
    "href": "01_binary_metrics.html#native-comparison-3",
    "title": "1  Binary Classification Metrics",
    "section": "5.1 Native Comparison",
    "text": "5.1 Native Comparison\n\n\nCode\ncreate_corr_heatmap(\n  df_corr_native,\n  title = \"Anomaly Correlation: Native Observation Sources\",\n  subtitle = \"Black outline = best performer. Higher = better captures interannual variability.\",\n  caption = \"SEAS5 vs ERA5, INSIVUMEH vs ENACTS\"\n)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#enacts-only-comparison-1",
    "href": "01_binary_metrics.html#enacts-only-comparison-1",
    "title": "1  Binary Classification Metrics",
    "section": "5.2 ENACTS-Only Comparison",
    "text": "5.2 ENACTS-Only Comparison\n\n\nCode\ncreate_corr_heatmap(\n  df_corr_enacts,\n  title = \"Anomaly Correlation: All Models vs ENACTS\",\n  subtitle = \"Black outline = best performer. Higher = better captures interannual variability.\",\n  caption = \"All models vs ENACTS\"\n)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#scatterplots",
    "href": "01_binary_metrics.html#scatterplots",
    "title": "1  Binary Classification Metrics",
    "section": "5.3 Scatterplots",
    "text": "5.3 Scatterplots\nScatterplots show the relationship between forecast and observed anomalies for each model-leadtime combination.\n\n\nScatterplot function with R² and p-value\ncreate_corr_scatter &lt;- function(df_anom, window_name, title_suffix) {\n  df_plot &lt;- df_anom |&gt;\n    filter(window == window_name) |&gt;\n    mutate(\n      model_short = str_remove(forecast_source, \"INSIVUMEH_\"),\n      facet_label = paste0(model_short, \" (LT\", leadtime, \")\")\n    )\n\n  # Calculate r and p-value for each facet\n  df_stats &lt;- df_plot |&gt;\n    group_by(forecast_source, leadtime, facet_label) |&gt;\n    summarise(\n      r = cor(fcst_anom, obs_anom, use = \"complete.obs\"),\n      p_value = cor.test(fcst_anom, obs_anom)$p.value,\n      .groups = \"drop\"\n    ) |&gt;\n    mutate(\n      label = paste0(\"r = \", sprintf(\"%.2f\", r), \"\\n\",\n                     \"p = \", ifelse(p_value &lt; 0.001, \"&lt;0.001\", sprintf(\"%.3f\", p_value)))\n    )\n\n  ggplot(df_plot, aes(x = obs_anom, y = fcst_anom)) +\n    geom_hline(yintercept = 0, color = \"grey70\", linetype = \"dashed\") +\n    geom_vline(xintercept = 0, color = \"grey70\", linetype = \"dashed\") +\n    geom_smooth(method = \"lm\", se = TRUE, color = \"#007CE1\", fill = \"#007CE1\", alpha = 0.2) +\n    geom_point(alpha = 0.4, size = 2) +\n    geom_text(\n      data = df_stats,\n      aes(x = Inf, y = Inf, label = label),\n      hjust = 1.1, vjust = 1.3, size = 3, color = \"grey20\", fontface = \"bold\",\n      inherit.aes = FALSE\n    ) +\n    facet_wrap(~facet_label, scales = \"free\") +\n    labs(\n      title = paste0(\"Forecast vs Observed Anomalies: \", str_to_title(window_name)),\n      subtitle = title_suffix,\n      x = \"Observed Anomaly (z-score)\",\n      y = \"Forecast Anomaly (z-score)\"\n    ) +\n    theme_minimal() +\n    theme(\n      strip.text = element_text(face = \"bold\", size = 9),\n      panel.grid.minor = element_blank()\n    )\n}\n\n\n\n5.3.1 Primera Season\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\ncreate_corr_scatter(df_anom_native, \"primera\", \"SEAS5 vs ERA5, INSIVUMEH vs ENACTS\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_corr_scatter(df_anom_enacts, \"primera\", \"All models vs ENACTS\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.2 Postrera Season\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\ncreate_corr_scatter(df_anom_native, \"postrera\", \"SEAS5 vs ERA5, INSIVUMEH vs ENACTS\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_corr_scatter(df_anom_enacts, \"postrera\", \"All models vs ENACTS\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#primera-season-2",
    "href": "01_binary_metrics.html#primera-season-2",
    "title": "1  Binary Classification Metrics",
    "section": "6.1 Primera Season",
    "text": "6.1 Primera Season\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\ncreate_random_detail_plot(df_comparison_native, \"primera\", random_percentiles, \"native\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_random_detail_plot(df_comparison_enacts, \"primera\", random_percentiles, \"enacts_only\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#postrera-season-2",
    "href": "01_binary_metrics.html#postrera-season-2",
    "title": "1  Binary Classification Metrics",
    "section": "6.2 Postrera Season",
    "text": "6.2 Postrera Season\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\ncreate_random_detail_plot(df_comparison_native, \"postrera\", random_percentiles, \"native\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_random_detail_plot(df_comparison_enacts, \"postrera\", random_percentiles, \"enacts_only\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#summary-by-model",
    "href": "01_binary_metrics.html#summary-by-model",
    "title": "1  Binary Classification Metrics",
    "section": "6.3 Summary by Model",
    "text": "6.3 Summary by Model\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\ncreate_random_summary_plot(df_comparison_native, random_percentiles, \"native\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_random_summary_plot(df_comparison_enacts, random_percentiles, \"enacts_only\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#exploring-larger-spatial-scales",
    "href": "01_binary_metrics.html#exploring-larger-spatial-scales",
    "title": "1  Binary Classification Metrics",
    "section": "9.1 Exploring Larger Spatial Scales",
    "text": "9.1 Exploring Larger Spatial Scales\nBoth SEAS5 and INSIVUMEH may perform poorly at the admin-1 (Chiquimula) scale because seasonal forecasts typically have better skill at larger spatial scales. HydroBASINS provides standardized watershed boundaries at multiple levels that could serve as alternative analysis units.\n\n\nLoad HydroBASINS and Chiquimula boundary\nlibrary(sf)\n\n# Disable s2 for this section (HydroBASINS has some geometry issues with s2)\nsf_use_s2(FALSE)\n\n# Load Guatemala boundaries\nsf_gtm_adm0 &lt;- cumulus::download_fieldmaps_sf(iso3 = \"GTM\", layer = \"gtm_adm0\")$gtm_adm0\nsf_gtm_adm1 &lt;- cumulus::download_fieldmaps_sf(iso3 = \"GTM\", layer = \"gtm_adm1\")$gtm_adm1 |&gt;\n janitor::clean_names()\nsf_chiquimula &lt;- sf_gtm_adm1 |&gt; filter(adm1_pcode == \"GT20\")\n\n# Path to HydroBASINS data\nhybas_dir &lt;- \"../../data/hybas_na_lev01-12_v1c\"\n\n# Load levels 5 and 6\nsf_hybas_5 &lt;- st_read(file.path(hybas_dir, \"hybas_na_lev05_v1c.shp\"), quiet = TRUE)\nsf_hybas_6 &lt;- st_read(file.path(hybas_dir, \"hybas_na_lev06_v1c.shp\"), quiet = TRUE)\n\n# Ensure same CRS\nsf_chiquimula &lt;- st_transform(sf_chiquimula, st_crs(sf_hybas_5))\n\n# Get bounding box of Chiquimula with buffer for context\nchiq_bbox &lt;- st_bbox(sf_chiquimula)\nbbox_buffer &lt;- 1  # degrees\n\n# Create bbox polygon for filtering\nbbox_poly &lt;- st_as_sfc(st_bbox(c(\n  xmin = as.numeric(chiq_bbox[\"xmin\"]) - bbox_buffer,\n  xmax = as.numeric(chiq_bbox[\"xmax\"]) + bbox_buffer,\n  ymin = as.numeric(chiq_bbox[\"ymin\"]) - bbox_buffer,\n  ymax = as.numeric(chiq_bbox[\"ymax\"]) + bbox_buffer\n), crs = st_crs(sf_hybas_5)))\n\n# Filter basins to region around Chiquimula using intersection\nsf_hybas_5_region &lt;- sf_hybas_5[st_intersects(sf_hybas_5, bbox_poly, sparse = FALSE)[,1], ]\nsf_hybas_6_region &lt;- sf_hybas_6[st_intersects(sf_hybas_6, bbox_poly, sparse = FALSE)[,1], ]\n\n# Find basins that intersect Chiquimula\nsf_hybas_5_intersect &lt;- sf_hybas_5_region[st_intersects(sf_hybas_5_region, sf_chiquimula, sparse = FALSE)[,1], ]\nsf_hybas_6_intersect &lt;- sf_hybas_6_region[st_intersects(sf_hybas_6_region, sf_chiquimula, sparse = FALSE)[,1], ]\n\ncat(\"Level 5 basins intersecting Chiquimula:\", nrow(sf_hybas_5_intersect), \"\\n\")\n\n\nLevel 5 basins intersecting Chiquimula: 2 \n\n\nLoad HydroBASINS and Chiquimula boundary\ncat(\"Level 6 basins intersecting Chiquimula:\", nrow(sf_hybas_6_intersect), \"\\n\")\n\n\nLevel 6 basins intersecting Chiquimula: 2 \n\n\n\n\nMap comparing basin levels 5 and 6 over Chiquimula\nlibrary(patchwork)\n\n# Calculate areas for labels\nsf_hybas_5_intersect &lt;- sf_hybas_5_intersect |&gt;\n  mutate(area_km2 = as.numeric(st_area(geometry)) / 1e6)\n\nsf_hybas_6_intersect &lt;- sf_hybas_6_intersect |&gt;\n  mutate(area_km2 = as.numeric(st_area(geometry)) / 1e6)\n\nchiq_area &lt;- as.numeric(st_area(sf_chiquimula)) / 1e6\n\n# Transform admin0 to match CRS\nsf_gtm_adm0_t &lt;- st_transform(sf_gtm_adm0, st_crs(sf_hybas_5))\n\n# Create base plot function\ncreate_basin_map &lt;- function(sf_basins, level, sf_admin, sf_country) {\n\n  # Get centroids for labels\n  basin_centroids &lt;- st_centroid(sf_basins) |&gt;\n    mutate(\n      x = st_coordinates(geometry)[,1],\n      y = st_coordinates(geometry)[,2]\n    )\n\n  ggplot() +\n    geom_sf(data = sf_country, fill = \"grey95\", color = \"grey50\", linewidth = 0.3) +\n    geom_sf(data = sf_basins, aes(fill = area_km2), color = \"darkblue\", linewidth = 0.8) +\n    geom_sf(data = sf_admin, fill = NA, color = \"red\", linewidth = 1.2) +\n    geom_sf_text(\n      data = basin_centroids,\n      aes(label = paste0(round(area_km2), \" km²\")),\n      size = 3, fontface = \"bold\", color = \"white\"\n    ) +\n    scale_fill_viridis_c(option = \"mako\", direction = -1, name = \"Area (km²)\") +\n    labs(\n      title = paste0(\"HydroBASINS Level \", level),\n      subtitle = paste0(nrow(sf_basins), \" basin(s) intersecting Chiquimula\"),\n      caption = paste0(\"Red outline = Chiquimula (\", round(chiq_area), \" km²)\")\n    ) +\n    theme_minimal() +\n    theme(\n      legend.position = \"bottom\",\n      plot.caption = element_text(color = \"red\", face = \"bold\")\n    )\n}\n\np_lev5 &lt;- create_basin_map(sf_hybas_5_intersect, 5, sf_chiquimula, sf_gtm_adm0_t)\np_lev6 &lt;- create_basin_map(sf_hybas_6_intersect, 6, sf_chiquimula, sf_gtm_adm0_t)\n\np_lev5 + p_lev6 +\n  plot_annotation(\n    title = \"HydroBASINS Comparison: Potential Analysis Units\",\n    subtitle = \"Comparing basin scales that could provide better forecast skill than admin-1 boundaries\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Summary comparison\ntibble(\n  `Spatial Unit` = c(\n    \"Chiquimula (Admin-1)\",\n    paste0(\"Level 5 (\", nrow(sf_hybas_5_intersect), \" basin\", ifelse(nrow(sf_hybas_5_intersect) &gt; 1, \"s\", \"\"), \")\"),\n    paste0(\"Level 6 (\", nrow(sf_hybas_6_intersect), \" basin\", ifelse(nrow(sf_hybas_6_intersect) &gt; 1, \"s\", \"\"), \")\")\n  ),\n  `Total Area (km²)` = c(\n    round(chiq_area),\n    round(sum(sf_hybas_5_intersect$area_km2)),\n    round(sum(sf_hybas_6_intersect$area_km2))\n  ),\n  `Mean Basin Area (km²)` = c(\n    round(chiq_area),\n    round(mean(sf_hybas_5_intersect$area_km2)),\n    round(mean(sf_hybas_6_intersect$area_km2))\n  ),\n  Notes = c(\n    \"Current analysis unit\",\n    \"Larger basins - may have better forecast skill\",\n    \"Medium basins - balance of skill and resolution\"\n  )\n) |&gt;\n  knitr::kable(caption = \"Comparison of spatial units for forecast skill assessment\")\n\n\n\nComparison of spatial units for forecast skill assessment\n\n\n\n\n\n\n\n\nSpatial Unit\nTotal Area (km²)\nMean Basin Area (km²)\nNotes\n\n\n\n\nChiquimula (Admin-1)\n2415\n2415\nCurrent analysis unit\n\n\nLevel 5 (2 basins)\n225446\n112723\nLarger basins - may have better forecast skill\n\n\nLevel 6 (2 basins)\n34487\n17243\nMedium basins - balance of skill and resolution\n\n\n\n\n\n\n9.1.1 Interpretation\n\nLevel 5 provides larger drainage basins that may better match the spatial resolution at which seasonal forecasts have skill\nLevel 6 offers a middle ground between admin boundaries and larger regional basins\nNext step: Re-run the skill assessment using basin-aggregated forecasts and observations to test if skill improves at larger scales",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html",
    "href": "02_continuous_metrics.html",
    "title": "2  Continuous Metrics",
    "section": "",
    "text": "2.1 Beyond Binary: A More Nuanced View\nThe binary metrics in Chapter 1 (F1 score, precision, recall) provide one lens on forecast skill, but they can be noisy - especially with only ~25 years of data and ~6 drought events. A single misclassified year can swing F1 scores dramatically.\nConsider an analogy: judging a temperature forecast as “wrong” for predicting 33°F when it was actually 31°F. The forecast told you to expect cold, even if it missed the freezing threshold. Binary metrics don’t distinguish between a near-miss and being completely off.\nThis chapter introduces continuous metrics that provide a more nuanced picture:\nOf these, ROC-AUC is particularly interpretable for operational decisions. An AUC of 0.75 means: “If you randomly select a drought year and a non-drought year, there’s a 75% chance the forecast correctly identifies which is drier.” This directly answers whether the forecast can discriminate drought from non-drought conditions - the core operational question.\nSetup: Libraries and data loading\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(cumulus)\nlibrary(yardstick)\nlibrary(gghdx)\ngghdx()\n\nbox::use(../../R/enacts)\nbox::use(../../R/seas5)\n\nset.seed(42)\n\n# Configuration\nBASELINE_START &lt;- 2000\nBASELINE_END &lt;- 2024\nPRIMERA_MONTHS &lt;- 5:8\nPOSTRERA_MONTHS &lt;- 9:11\nPRIMERA_ISSUED_MONTHS &lt;- c(3, 4, 5)\nPOSTRERA_ISSUED_MONTHS &lt;- c(6, 7, 8, 9)\n\n# Helper function\ncalc_rp_threshold &lt;- function(x, rp_target = 4, direction = -1) {\n  x &lt;- x[!is.na(x)]\n  n &lt;- length(x)\n  if (n &lt; 3) return(NA_real_)\n  ranks &lt;- rank(x * -direction, ties.method = \"average\")\n  rp &lt;- (n + 1) / ranks\n  approx(rp, x, xout = rp_target, rule = 2)$y\n}\nLoad forecast and observation data\n# Load forecasts\ndf_insiv &lt;- cumulus::blob_read(\n  name = \"ds-aa-lac-dry-corridor/data/processed/insivumeh_special/insivumeh_special_models_zonal_seasonal_chiquimula.parquet\",\n  container = \"projects\"\n)\ndf_enacts &lt;- enacts$load_enacts_seasonal(\"chiquimula\")\ndf_seas5 &lt;- seas5$load_seas5_seasonal()\n\n# Load ERA5\ncon &lt;- pg_con()\ndf_era5_raw &lt;- tbl(con, \"era5\") |&gt;\n  filter(pcode == \"GT20\") |&gt;\n  collect() |&gt;\n  mutate(\n    year = year(valid_date),\n    month = month(valid_date),\n    mean = mean * days_in_month(valid_date)\n  )\nDBI::dbDisconnect(con)\n\n# Aggregate ERA5 to seasonal\naggregate_obs_seasonal &lt;- function(df, window_name) {\n  months &lt;- if (window_name == \"primera\") PRIMERA_MONTHS else POSTRERA_MONTHS\n  df |&gt;\n    filter(month %in% months) |&gt;\n    group_by(year) |&gt;\n    summarise(obs_mm = sum(value, na.rm = TRUE), .groups = \"drop\") |&gt;\n    mutate(window = window_name)\n}\n\ndf_era5 &lt;- bind_rows(\n  aggregate_obs_seasonal(df_era5_raw |&gt; rename(value = mean), \"primera\"),\n  aggregate_obs_seasonal(df_era5_raw |&gt; rename(value = mean), \"postrera\")\n)\n\n# Combine forecasts\ndf_fcst_all &lt;- bind_rows(df_insiv, df_seas5) |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END)\n\n# Filter to operational leadtimes\ndf_fcst_filtered &lt;- df_fcst_all |&gt;\n  mutate(issued_month = month(issued_date)) |&gt;\n  filter(\n    (window == \"primera\" & issued_month %in% PRIMERA_ISSUED_MONTHS) |\n    (window == \"postrera\" & issued_month %in% POSTRERA_ISSUED_MONTHS)\n  )",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#beyond-binary-a-more-nuanced-view",
    "href": "02_continuous_metrics.html#beyond-binary-a-more-nuanced-view",
    "title": "2  Continuous Metrics",
    "section": "",
    "text": "Correlation metrics measure how well forecasts track year-to-year variability\nError metrics quantify the typical magnitude of forecast errors\nROC-AUC assesses ranking skill: do lower forecasts correspond to drier years?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#comparison-frameworks",
    "href": "02_continuous_metrics.html#comparison-frameworks",
    "title": "2  Continuous Metrics",
    "section": "2.2 Comparison Frameworks",
    "text": "2.2 Comparison Frameworks\n\n\n\n\n\n\nNoteTwo Ways to Validate Forecasts\n\n\n\nNative Comparison: Each model validated against its “natural” observation source\n\nSEAS5 vs ERA5 (both from ECMWF)\nINSIVUMEH vs ENACTS (regional data)\n\nENACTS-only: All models validated against ENACTS\n\nAllows direct model-to-model comparison\nENACTS is the operational observation source",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#method-1-correlation-metrics",
    "href": "02_continuous_metrics.html#method-1-correlation-metrics",
    "title": "2  Continuous Metrics",
    "section": "2.3 Method 1: Correlation Metrics",
    "text": "2.3 Method 1: Correlation Metrics\n\n2.3.1 What It Measures\nCorrelation measures whether forecasts correctly identify which years are wetter or drier than average. A forecast doesn’t need to predict exact rainfall amounts - it just needs to rank years correctly.\n\n\n2.3.2 Metrics\n\n\n\n\n\n\n\n\nMetric\nFormula\nInterpretation\n\n\n\n\nSpearman ρ\nRank correlation\nDid wet years get higher forecasts? Robust to outliers.\n\n\nPearson r\nLinear correlation\nSame, but sensitive to extreme values.\n\n\n\n\n\n2.3.3 Strengths & Weaknesses\n\n\nStrengths\n\nMeasures interannual variability skill\nNo threshold dependence\nWorks on any units (mm or z-scores)\nSpearman is robust to outliers\n\n\nWeaknesses\n\nDoesn’t measure absolute accuracy\nA forecast could rank years perfectly but have huge bias\nSample size matters (n=25 years)\n\n\n\n\n\nCalculate standardized anomalies\n# Forecast anomalies\ndf_fcst_anom &lt;- df_fcst_filtered |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  mutate(\n    fcst_mean = mean(value, na.rm = TRUE),\n    fcst_sd = sd(value, na.rm = TRUE),\n    fcst_anom = (value - fcst_mean) / fcst_sd\n  ) |&gt;\n  ungroup() |&gt;\n  select(year, window, leadtime, forecast_source, value, fcst_anom)\n\n# Observation anomalies\ncalc_obs_anom &lt;- function(df, obs_name) {\n  df |&gt;\n    filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n    group_by(window) |&gt;\n    mutate(\n      obs_mean = mean(obs_mm, na.rm = TRUE),\n      obs_sd = sd(obs_mm, na.rm = TRUE),\n      obs_anom = (obs_mm - obs_mean) / obs_sd\n    ) |&gt;\n    ungroup() |&gt;\n    mutate(obs_source = obs_name) |&gt;\n    select(year, window, obs_mm, obs_anom, obs_source)\n}\n\ndf_enacts_anom &lt;- calc_obs_anom(df_enacts, \"ENACTS\")\ndf_era5_anom &lt;- calc_obs_anom(df_era5, \"ERA5\")\n\n# Join - Native\ndf_anom_native &lt;- bind_rows(\n  df_fcst_anom |&gt; filter(forecast_source == \"SEAS5\") |&gt;\n    inner_join(df_era5_anom, by = c(\"year\", \"window\")),\n  df_fcst_anom |&gt; filter(str_detect(forecast_source, \"INSIVUMEH\")) |&gt;\n    inner_join(df_enacts_anom, by = c(\"year\", \"window\"))\n)\n\n# Join - ENACTS-only\ndf_anom_enacts &lt;- df_fcst_anom |&gt;\n  inner_join(df_enacts_anom, by = c(\"year\", \"window\"))\n\n\n\n\nCalculate continuous metrics\ncalc_continuous_metrics &lt;- function(df) {\n  df |&gt;\n    group_by(forecast_source, window, leadtime, obs_source) |&gt;\n    summarise(\n      n = n(),\n      spearman = cor(fcst_anom, obs_anom, method = \"spearman\", use = \"complete.obs\"),\n      pearson = cor(fcst_anom, obs_anom, method = \"pearson\", use = \"complete.obs\"),\n      rmse = sqrt(mean((fcst_anom - obs_anom)^2, na.rm = TRUE)),\n      mae = mean(abs(fcst_anom - obs_anom), na.rm = TRUE),\n      bias_mm = mean(value - obs_mm, na.rm = TRUE),\n      .groups = \"drop\"\n    )\n}\n\ndf_metrics_native &lt;- calc_continuous_metrics(df_anom_native)\ndf_metrics_enacts &lt;- calc_continuous_metrics(df_anom_enacts)\n\n\n\n\n2.3.4 Results: Spearman Correlation\n\n\nHeatmap function\ncreate_metric_heatmap &lt;- function(df, metric_col, metric_name, title, caption,\n                                  midpoint = 0, lower_is_better = FALSE) {\n  df_plot &lt;- df |&gt;\n    filter(!is.na(!!sym(metric_col))) |&gt;\n    mutate(window = factor(window, levels = c(\"primera\", \"postrera\"))) |&gt;\n    group_by(window, leadtime) |&gt;\n    mutate(is_best = if (lower_is_better) {\n      !!sym(metric_col) == min(!!sym(metric_col), na.rm = TRUE)\n    } else {\n      !!sym(metric_col) == max(!!sym(metric_col), na.rm = TRUE)\n    }) |&gt;\n    ungroup()\n\n  ggplot(df_plot, aes(x = factor(leadtime), y = forecast_source)) +\n    geom_tile(aes(fill = !!sym(metric_col)), color = \"white\", linewidth = 0.5) +\n    geom_tile(\n      data = df_plot |&gt; filter(is_best),\n      fill = NA, color = \"black\", linewidth = 1.5\n    ) +\n    geom_text(aes(label = sprintf(\"%.2f\", !!sym(metric_col))), size = 4, fontface = \"bold\", color = \"black\") +\n    facet_wrap(~window) +\n    scale_fill_gradient2(\n      low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n      midpoint = midpoint, name = metric_name\n    ) +\n    labs(title = title, x = \"Leadtime (months)\", y = NULL, caption = caption) +\n    theme_minimal() +\n    theme(legend.position = \"right\", panel.grid = element_blank(),\n          plot.caption = element_text(hjust = 0))\n}\n\n\n\nNative ComparisonENACTS-only\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_metrics_native, \"spearman\", \"Spearman ρ\",\n  \"Spearman Rank Correlation - NATIVE\",\n  \"NATIVE: SEAS5 vs ERA5; INSIVUMEH vs ENACTS. Black outline = best per leadtime.\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_metrics_enacts, \"spearman\", \"Spearman ρ\",\n  \"Spearman Rank Correlation - ENACTS-ONLY\",\n  \"ENACTS-ONLY: All models vs ENACTS. Black outline = best per leadtime.\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.5 Interpretation: Correlation\n\n\n\n\n\n\nTipKey Findings\n\n\n\nPrimera (May-Aug)\n\nSEAS5 shows strong correlation (ρ = 0.56-0.60 at LT0-2)\nINSIVUMEH models competitive at LT2, especially CESM1 (ρ = 0.58)\nAll models beat random (ρ = 0 expected by chance)\n\nPostrera (Sep-Nov)\n\nWeaker correlations across all models (ρ = 0.2-0.5)\nSEAS5 slightly better at LT1 (ρ = 0.51)\nCCSM4 shows unexpected strength at LT3 (ρ = 0.48)\nMore variability between leadtimes - less reliable signal",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#method-2-error-magnitude-rmse-mae",
    "href": "02_continuous_metrics.html#method-2-error-magnitude-rmse-mae",
    "title": "2  Continuous Metrics",
    "section": "2.4 Method 2: Error Magnitude (RMSE, MAE)",
    "text": "2.4 Method 2: Error Magnitude (RMSE, MAE)\n\n2.4.1 What It Measures\nRoot Mean Square Error (RMSE) and Mean Absolute Error (MAE) measure how far forecasts deviate from observations in standardized units.\n\n\n2.4.2 Metrics\n\n\n\n\n\n\n\n\nMetric\nFormula\nInterpretation\n\n\n\n\nRMSE\n√(mean((fcst - obs)²))\nPenalizes large errors more heavily\n\n\nMAE\nmean(|fcst - obs|)\nEqual penalty per unit error\n\n\n\n\n\n2.4.3 Strengths & Weaknesses\n\n\nStrengths\n\nMeasures absolute accuracy\nRMSE sensitive to outliers (useful for flagging catastrophic misses)\nMAE more robust, easier to interpret\n\n\nWeaknesses\n\nDepends on units (z-scores vs mm)\nCan be dominated by a few bad years\nDoesn’t distinguish bias from random error\n\n\n\n\n\n2.4.4 Results: RMSE\n\nNative ComparisonENACTS-only\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_metrics_native, \"rmse\", \"RMSE\",\n  \"RMSE (Standardized Anomalies) - NATIVE\",\n  \"NATIVE: SEAS5 vs ERA5; INSIVUMEH vs ENACTS. Lower = better. Black outline = best.\",\n  midpoint = 1, lower_is_better = TRUE\n) +\n  scale_fill_gradient2(low = \"#1A9850\", mid = \"#FFFFBF\", high = \"#D73027\",\n                       midpoint = 1, name = \"RMSE\", limits = c(0.5, 1.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_metrics_enacts, \"rmse\", \"RMSE\",\n  \"RMSE (Standardized Anomalies) - ENACTS-ONLY\",\n  \"ENACTS-ONLY: All models vs ENACTS. Lower = better. Black outline = best.\",\n  midpoint = 1, lower_is_better = TRUE\n) +\n  scale_fill_gradient2(low = \"#1A9850\", mid = \"#FFFFBF\", high = \"#D73027\",\n                       midpoint = 1, name = \"RMSE\", limits = c(0.5, 1.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.5 Interpretation: RMSE\n\n\n\n\n\n\nTipKey Findings\n\n\n\n\nSEAS5 has lowest RMSE for Primera (0.81-0.95 vs 0.95-1.13 for INSIVUMEH)\nFor Postrera, CCSM4 often has lower RMSE than SEAS5\nRMSE &gt; 1.0 means error exceeds one standard deviation - forecasts add noise rather than signal\nCFSv2 Postrera LT3 has RMSE = 1.44 - actively harmful predictions",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#method-3-bias",
    "href": "02_continuous_metrics.html#method-3-bias",
    "title": "2  Continuous Metrics",
    "section": "2.5 Method 3: Bias",
    "text": "2.5 Method 3: Bias\n\n2.5.1 What It Measures\nBias measures systematic over- or under-prediction. A forecast with zero bias is “calibrated” on average, even if individual predictions are wrong.\n\n\n2.5.2 Formula\n\\[\\text{Bias} = \\text{mean}(\\text{forecast} - \\text{observed})\\]\n\nPositive bias: Forecast too wet\nNegative bias: Forecast too dry\n\n\n\n2.5.3 Strengths & Weaknesses\n\n\nStrengths\n\nEasy to interpret (in mm)\nCan be corrected with simple adjustments\nReveals systematic model issues\n\n\nWeaknesses\n\nHides random error (a model can have zero bias but huge scatter)\nMust use raw mm, not z-scores (z-score bias is always ~0 by construction)\n\n\n\n\n\n\n\n\n\nWarningImportant Note on Units\n\n\n\nBias must be calculated on raw millimeters, not standardized anomalies. When you standardize to z-scores, you force the mean to zero - so bias between two z-scored series is mathematically guaranteed to be ~0, which is meaningless.\n\n\n\n\n2.5.4 Results: Bias (Raw mm)\n\n\nBias heatmap function\ncreate_bias_heatmap &lt;- function(df, title, caption) {\n  df_plot &lt;- df |&gt;\n    filter(!is.na(bias_mm)) |&gt;\n    mutate(window = factor(window, levels = c(\"primera\", \"postrera\"))) |&gt;\n    group_by(window, leadtime) |&gt;\n    mutate(is_best = abs(bias_mm) == min(abs(bias_mm), na.rm = TRUE)) |&gt;\n    ungroup()\n\n  bias_max &lt;- max(abs(df_plot$bias_mm), na.rm = TRUE)\n\n  ggplot(df_plot, aes(x = factor(leadtime), y = forecast_source)) +\n    geom_tile(aes(fill = bias_mm), color = \"white\", linewidth = 0.5) +\n    geom_tile(\n      data = df_plot |&gt; filter(is_best),\n      fill = NA, color = \"black\", linewidth = 1.5\n    ) +\n    geom_text(aes(label = sprintf(\"%.0f\", bias_mm)), size = 4, fontface = \"bold\", color = \"black\") +\n    facet_wrap(~window) +\n    scale_fill_gradient2(\n      low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n      midpoint = 0, limits = c(-bias_max, bias_max), name = \"Bias (mm)\"\n    ) +\n    labs(title = title, x = \"Leadtime (months)\", y = NULL, caption = caption) +\n    theme_minimal() +\n    theme(legend.position = \"right\", panel.grid = element_blank(),\n          plot.caption = element_text(hjust = 0))\n}\n\n\n\nNative ComparisonENACTS-only\n\n\n\n\nCode\ncreate_bias_heatmap(\n  df_metrics_native,\n  \"Mean Bias - NATIVE\",\n  \"NATIVE: SEAS5 vs ERA5; INSIVUMEH vs ENACTS. Units: mm. Black outline = closest to zero.\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_bias_heatmap(\n  df_metrics_enacts,\n  \"Mean Bias - ENACTS-ONLY\",\n  \"ENACTS-ONLY: All models vs ENACTS. Units: mm. Black outline = closest to zero.\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.5 Interpretation: Bias\n\n\n\n\n\n\nTipKey Findings\n\n\n\nPrimera\n\nAll models are biased wet (positive bias)\nSEAS5: +68 to +132mm too wet\nINSIVUMEH models: +244 to +262mm too wet (severe overestimation)\nThis means forecasts systematically predict more rain than actually falls\n\nPostrera\n\nAll models are biased dry (negative bias)\nSEAS5: -113 to -132mm too dry\nINSIVUMEH models: -21 to -37mm too dry (much smaller bias)\nFor Postrera, INSIVUMEH has better calibration than SEAS5",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#method-4-distance-to-threshold",
    "href": "02_continuous_metrics.html#method-4-distance-to-threshold",
    "title": "2  Continuous Metrics",
    "section": "2.6 Method 4: Distance-to-Threshold",
    "text": "2.6 Method 4: Distance-to-Threshold\n\n2.6.1 What It Measures\nInstead of binary hit/miss, measure how far each forecast and observation was from their respective drought thresholds. Then correlate these distances.\n\n\n2.6.2 Why This Matters\nConsider two “false positive” cases:\n\nForecast: 10mm below threshold. Observed: 5mm above threshold. (Near miss)\nForecast: 100mm below threshold. Observed: 150mm above threshold. (Confident and wrong)\n\nBinary metrics score both as equally wrong. Distance-to-threshold reveals that Case 1 was almost right, while Case 2 was a confident failure.\n\n\n2.6.3 Calculation\nfcst_distance = forecast_value - forecast_threshold\nobs_distance = observed_value - observed_threshold\n\ndistance_correlation = cor(fcst_distance, obs_distance)\n\nNegative distance = below threshold (drought-like)\nPositive distance = above threshold (normal)\n\n\n\n2.6.4 Strengths & Weaknesses\n\n\nStrengths\n\nGives partial credit for near-misses\nOperationally relevant (close calls matter)\nUses actual mm, not abstract scores\n\n\nWeaknesses\n\nStill depends on threshold choice\nCorrelation can be high even if absolute distances don’t match\n\n\n\n\n\nCalculate distance-to-threshold\n# Observation thresholds\nobs_thresh_enacts &lt;- df_enacts |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n  group_by(window) |&gt;\n  summarise(obs_thresh = calc_rp_threshold(obs_mm, 4, -1), .groups = \"drop\")\n\nobs_thresh_era5 &lt;- df_era5 |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n  group_by(window) |&gt;\n  summarise(obs_thresh = calc_rp_threshold(obs_mm, 4, -1), .groups = \"drop\")\n\n# Forecast thresholds\nfcst_thresholds &lt;- df_fcst_filtered |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  summarise(fcst_thresh = calc_rp_threshold(value, 4, -1), .groups = \"drop\")\n\n# Build distance data\nbuild_distance_data &lt;- function(df_fcst, df_obs, obs_thresholds, obs_name) {\n  df_obs_filtered &lt;- df_obs |&gt; filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END)\n\n  df_fcst |&gt;\n    left_join(df_obs_filtered |&gt; select(year, window, obs_mm), by = c(\"year\", \"window\")) |&gt;\n    left_join(obs_thresholds, by = \"window\") |&gt;\n    left_join(fcst_thresholds, by = c(\"forecast_source\", \"window\", \"leadtime\")) |&gt;\n    mutate(\n      fcst_distance_mm = value - fcst_thresh,\n      obs_distance_mm = obs_mm - obs_thresh,\n      fcst_drought = value &lt;= fcst_thresh,\n      obs_drought = obs_mm &lt;= obs_thresh,\n      obs_source = obs_name\n    ) |&gt;\n    filter(!is.na(obs_mm), !is.na(fcst_thresh))\n}\n\ndf_dist_native &lt;- bind_rows(\n  build_distance_data(df_fcst_filtered |&gt; filter(forecast_source == \"SEAS5\"),\n                      df_era5, obs_thresh_era5, \"ERA5\"),\n  build_distance_data(df_fcst_filtered |&gt; filter(str_detect(forecast_source, \"INSIVUMEH\")),\n                      df_enacts, obs_thresh_enacts, \"ENACTS\")\n)\n\ndf_dist_enacts &lt;- build_distance_data(df_fcst_filtered, df_enacts, obs_thresh_enacts, \"ENACTS\")\n\n# Calculate distance metrics\ncalc_distance_metrics &lt;- function(df) {\n  df |&gt;\n    group_by(forecast_source, window, leadtime, obs_source) |&gt;\n    summarise(\n      dist_corr = cor(fcst_distance_mm, obs_distance_mm, use = \"complete.obs\"),\n      mae_dist = mean(abs(fcst_distance_mm - obs_distance_mm), na.rm = TRUE),\n      .groups = \"drop\"\n    )\n}\n\ndf_dist_metrics_native &lt;- calc_distance_metrics(df_dist_native)\ndf_dist_metrics_enacts &lt;- calc_distance_metrics(df_dist_enacts)\n\n\n\n\n2.6.5 Results: Distance Correlation\n\nNative ComparisonENACTS-only\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_dist_metrics_native, \"dist_corr\", \"Correlation\",\n  \"Distance-to-Threshold Correlation - NATIVE\",\n  \"NATIVE: SEAS5 vs ERA5; INSIVUMEH vs ENACTS. Higher = forecast distances match observed distances.\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_dist_metrics_enacts, \"dist_corr\", \"Correlation\",\n  \"Distance-to-Threshold Correlation - ENACTS-ONLY\",\n  \"ENACTS-ONLY: All models vs ENACTS. Higher = forecast distances match observed distances.\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.6.6 Scatter Plot: Distance Comparison\n\n\nDistance scatter plots\ndf_dist_enacts |&gt;\n  filter(window == \"primera\", leadtime %in% c(1, 2)) |&gt;\n  mutate(\n    model = str_remove(forecast_source, \"INSIVUMEH_\"),\n    facet = paste0(model, \" LT\", leadtime)\n  ) |&gt;\n  ggplot(aes(x = obs_distance_mm, y = fcst_distance_mm)) +\n  geom_hline(yintercept = 0, color = \"grey70\", linetype = \"dashed\") +\n  geom_vline(xintercept = 0, color = \"grey70\", linetype = \"dashed\") +\n  geom_abline(slope = 1, intercept = 0, color = \"#B2182B\", linetype = \"dotted\", linewidth = 0.8) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"#007CE1\", fill = \"#007CE1\", alpha = 0.2) +\n  geom_point(aes(color = obs_drought), alpha = 0.6, size = 2.5) +\n  facet_wrap(~facet, ncol = 4) +\n  scale_color_manual(\n    values = c(\"TRUE\" = \"#D73027\", \"FALSE\" = \"#4575B4\"),\n    labels = c(\"TRUE\" = \"Drought year\", \"FALSE\" = \"Normal year\"),\n    name = \"Observed\"\n  ) +\n  labs(\n    title = \"Primera LT1-2: Forecast vs Observed Distance from Threshold\",\n    subtitle = \"Dotted red = perfect agreement. Blue = linear fit. Dashed lines = threshold (distance = 0).\",\n    x = \"Observed Distance from Threshold (mm)\",\n    y = \"Forecast Distance from Threshold (mm)\",\n    caption = \"ENACTS-ONLY comparison. Negative = below threshold (drought-like).\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\", plot.caption = element_text(hjust = 0))\n\n\n\n\n\n\n\n\n\n\n\n2.6.7 Interpretation: Distance-to-Threshold\n\n\n\n\n\n\nTipKey Findings\n\n\n\nPrimera\n\nSEAS5 LT1 has the strongest distance correlation (r = 0.66)\nWhen SEAS5 says “drier than usual”, observations tend to agree\nINSIVUMEH models also show skill (r = 0.5-0.53)\n\nPostrera\n\nCCSM4 shows best distance skill at LT1 (r = 0.51)\nSEAS5 Postrera distance correlation drops sharply at LT2 (r = 0.16)\nCFSv2 LT3 has negative correlation (r = -0.08) - actively misleading",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#method-5-roc-curves-auc",
    "href": "02_continuous_metrics.html#method-5-roc-curves-auc",
    "title": "2  Continuous Metrics",
    "section": "2.7 Method 5: ROC Curves & AUC",
    "text": "2.7 Method 5: ROC Curves & AUC\n\n2.7.1 What It Measures\nROC (Receiver Operating Characteristic) curves measure ranking skill: did lower forecast values correspond to actual drought years?\nUnlike binary metrics, ROC doesn’t require choosing a specific threshold. It asks: “If I ranked all years by forecast value, would drought years tend to be at the bottom?”\n\n\n2.7.2 How to Read an ROC Curve\n\nDiagonal line: Random guessing (AUC = 0.5)\nCurve above diagonal: Skill (drought years get lower forecasts)\nCurve below diagonal: Inverted skill (drought years get higher forecasts - very bad)\n\n\n\n2.7.3 AUC Interpretation\n\n\n\nAUC\nInterpretation\n\n\n\n\n0.9-1.0\nOutstanding\n\n\n0.8-0.9\nExcellent\n\n\n0.7-0.8\nAcceptable\n\n\n0.5-0.7\nPoor\n\n\n&lt; 0.5\nWorse than random\n\n\n\n\n\n2.7.4 Strengths & Weaknesses\n\n\nStrengths\n\nThreshold-independent\nMeasures ranking/discrimination skill\nWell-understood statistically\n\n\nWeaknesses\n\nDoesn’t measure calibration\nCan be high even with severe bias\nSmall sample size (6 drought years) limits precision\n\n\n\n\n\nCalculate ROC and AUC\n# Build ROC data\nbuild_roc_data &lt;- function(df_fcst, df_obs, obs_thresholds, obs_name) {\n  df_obs_filtered &lt;- df_obs |&gt; filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END)\n\n  df_fcst |&gt;\n    left_join(df_obs_filtered |&gt; select(year, window, obs_mm), by = c(\"year\", \"window\")) |&gt;\n    left_join(obs_thresholds, by = \"window\") |&gt;\n    mutate(\n      truth = factor(obs_mm &lt;= obs_thresh, c(TRUE, FALSE), c(\"drought\", \"no_drought\")),\n      drought_score = -value,  # Lower forecast = higher drought score\n      obs_source = obs_name\n    ) |&gt;\n    filter(!is.na(truth))\n}\n\ndf_roc_native &lt;- bind_rows(\n  build_roc_data(df_fcst_filtered |&gt; filter(forecast_source == \"SEAS5\"),\n                 df_era5, obs_thresh_era5, \"ERA5\"),\n  build_roc_data(df_fcst_filtered |&gt; filter(str_detect(forecast_source, \"INSIVUMEH\")),\n                 df_enacts, obs_thresh_enacts, \"ENACTS\")\n)\n\ndf_roc_enacts &lt;- build_roc_data(df_fcst_filtered, df_enacts, obs_thresh_enacts, \"ENACTS\")\n\n# Calculate AUC\ncalc_auc &lt;- function(df) {\n  if (length(unique(df$truth)) &lt; 2) return(NA_real_)\n  roc_auc(df, truth = truth, drought_score, event_level = \"first\")$.estimate\n}\n\ndf_auc_native &lt;- df_roc_native |&gt;\n  group_by(forecast_source, window, leadtime, obs_source) |&gt;\n  summarise(auc = calc_auc(pick(everything())), .groups = \"drop\")\n\ndf_auc_enacts &lt;- df_roc_enacts |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  summarise(auc = calc_auc(pick(everything())), obs_source = \"ENACTS\", .groups = \"drop\")\n\n\n\n\n2.7.5 Results: AUC Heatmaps\n\nNative ComparisonENACTS-only\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_auc_native, \"auc\", \"AUC\",\n  \"ROC-AUC - NATIVE\",\n  \"NATIVE: SEAS5 vs ERA5; INSIVUMEH vs ENACTS. AUC &gt; 0.7 = acceptable skill.\",\n  midpoint = 0.5\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_auc_enacts, \"auc\", \"AUC\",\n  \"ROC-AUC - ENACTS-ONLY\",\n  \"ENACTS-ONLY: All models vs ENACTS. AUC &gt; 0.7 = acceptable skill.\",\n  midpoint = 0.5\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.7.6 ROC Curves: Primera\n\n\nROC curves\n# Calculate ROC curves\ncalc_roc_curve &lt;- function(df) {\n  if (length(unique(df$truth)) &lt; 2) return(tibble(specificity = NA, sensitivity = NA))\n  roc_curve(df, truth = truth, drought_score, event_level = \"first\")\n}\n\ndf_roc_curves_enacts &lt;- df_roc_enacts |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  group_modify(~calc_roc_curve(.x)) |&gt;\n  ungroup()\n\ndf_roc_curves_enacts |&gt;\n  filter(window == \"primera\", !is.na(specificity)) |&gt;\n  mutate(\n    model = str_remove(forecast_source, \"INSIVUMEH_\"),\n    facet = paste0(model, \" LT\", leadtime)\n  ) |&gt;\n  left_join(\n    df_auc_enacts |&gt; filter(window == \"primera\") |&gt;\n      mutate(model = str_remove(forecast_source, \"INSIVUMEH_\"),\n             facet = paste0(model, \" LT\", leadtime),\n             label = paste0(\"AUC = \", sprintf(\"%.2f\", auc))),\n    by = c(\"forecast_source\", \"window\", \"leadtime\", \"facet\", \"model\")\n  ) |&gt;\n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"grey50\") +\n  geom_path(color = \"#007CE1\", linewidth = 1) +\n  geom_text(aes(x = 0.7, y = 0.2, label = label), hjust = 0, size = 3, fontface = \"bold\",\n            color = \"#007CE1\", check_overlap = TRUE) +\n  facet_wrap(~facet, ncol = 4) +\n  coord_equal() +\n  labs(\n    title = \"ROC Curves: Primera - ENACTS-ONLY\",\n    subtitle = \"Dashed line = random guessing. Curve above diagonal = skill.\",\n    x = \"False Positive Rate (1 - Specificity)\",\n    y = \"True Positive Rate (Sensitivity)\",\n    caption = \"ENACTS-ONLY: All models vs ENACTS.\"\n  ) +\n  theme_minimal() +\n  theme(strip.text = element_text(face = \"bold\"), plot.caption = element_text(hjust = 0))\n\n\n\n\n\n\n\n\n\n\n\n2.7.7 Interpretation: ROC-AUC\n\n\n\n\n\n\nTipKey Findings\n\n\n\nPrimera\n\nSEAS5 and CESM1 show excellent discrimination (AUC = 0.87-0.88 at LT1-2)\nCCSM4 also strong (AUC = 0.81 at LT1)\nAll models beat random guessing\n\nPostrera\n\nMuch weaker AUC across all models (0.5-0.8 range)\nCCSM4 LT3 surprisingly good (AUC = 0.83)\nSEAS5 LT2 near random (AUC = 0.54)\nCFSv2 LT3 inverted (AUC = 0.40) - lower forecasts correspond to wetter years",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#summary-which-model-wins",
    "href": "02_continuous_metrics.html#summary-which-model-wins",
    "title": "2  Continuous Metrics",
    "section": "2.8 Summary: Which Model Wins?",
    "text": "2.8 Summary: Which Model Wins?\n\n\nSummary comparison\ndf_summary &lt;- df_metrics_enacts |&gt;\n  left_join(df_dist_metrics_enacts, by = c(\"forecast_source\", \"window\", \"leadtime\", \"obs_source\")) |&gt;\n  left_join(df_auc_enacts, by = c(\"forecast_source\", \"window\", \"leadtime\", \"obs_source\")) |&gt;\n  filter(leadtime %in% c(1, 2)) |&gt;\n  mutate(model = str_remove(forecast_source, \"INSIVUMEH_\")) |&gt;\n  select(model, window, leadtime, spearman, rmse, bias_mm, dist_corr, auc) |&gt;\n  mutate(across(c(spearman, rmse, dist_corr, auc), ~round(.x, 2)),\n         bias_mm = round(bias_mm, 0))\n\n\n\n2.8.1 Primera (MJJA)\n\n\nCode\ndf_summary |&gt;\n  filter(window == \"primera\") |&gt;\n  arrange(leadtime, desc(auc)) |&gt;\n  knitr::kable(\n    col.names = c(\"Model\", \"Window\", \"LT\", \"Spearman\", \"RMSE\", \"Bias (mm)\", \"Dist Corr\", \"AUC\"),\n    caption = \"Primera: All metrics at LT1-2 (ENACTS-only)\"\n  )\n\n\n\nPrimera: All metrics at LT1-2 (ENACTS-only)\n\n\nModel\nWindow\nLT\nSpearman\nRMSE\nBias (mm)\nDist Corr\nAUC\n\n\n\n\nSEAS5\nprimera\n1\n0.56\n0.81\n96\n0.66\n0.88\n\n\nCESM1\nprimera\n1\n0.48\n0.95\n261\n0.53\n0.87\n\n\nCCSM4\nprimera\n1\n0.48\n0.96\n258\n0.52\n0.81\n\n\nCFSv2\nprimera\n1\n0.39\n0.98\n253\n0.50\n0.65\n\n\nSEAS5\nprimera\n2\n0.56\n0.94\n68\n0.54\n0.88\n\n\nCESM1\nprimera\n2\n0.58\n1.02\n259\n0.46\n0.86\n\n\nCFSv2\nprimera\n2\n0.47\n1.00\n245\n0.48\n0.75\n\n\nCCSM4\nprimera\n2\n0.32\n1.13\n248\n0.34\n0.70\n\n\n\n\n\n\n\n\n\n\n\nNotePrimera Recommendation: SEAS5\n\n\n\n\nHighest AUC (0.88 at both LT1 and LT2)\nHighest distance correlation (0.66 at LT1)\nLowest RMSE (0.81 at LT1)\nSmallest bias (+68 to +96mm vs +244-262mm for INSIVUMEH)\nCESM1 is competitive but has much larger bias\n\n\n\n\n\n2.8.2 Postrera (SON)\n\n\nCode\ndf_summary |&gt;\n  filter(window == \"postrera\") |&gt;\n  arrange(leadtime, desc(auc)) |&gt;\n  knitr::kable(\n    col.names = c(\"Model\", \"Window\", \"LT\", \"Spearman\", \"RMSE\", \"Bias (mm)\", \"Dist Corr\", \"AUC\"),\n    caption = \"Postrera: All metrics at LT1-2 (ENACTS-only)\"\n  )\n\n\n\nPostrera: All metrics at LT1-2 (ENACTS-only)\n\n\nModel\nWindow\nLT\nSpearman\nRMSE\nBias (mm)\nDist Corr\nAUC\n\n\n\n\nSEAS5\npostrera\n1\n0.51\n1.14\n-131\n0.32\n0.69\n\n\nCCSM4\npostrera\n1\n0.50\n0.97\n-37\n0.51\n0.64\n\n\nCESM1\npostrera\n1\n0.19\n1.28\n-33\n0.14\n0.64\n\n\nCFSv2\npostrera\n1\n0.19\n1.26\n-23\n0.18\n0.58\n\n\nCCSM4\npostrera\n2\n0.26\n1.31\n-28\n0.10\n0.79\n\n\nCFSv2\npostrera\n2\n0.06\n1.28\n-28\n0.15\n0.68\n\n\nCESM1\npostrera\n2\n0.08\n1.17\n-21\n0.29\n0.61\n\n\nSEAS5\npostrera\n2\n0.29\n1.27\n-128\n0.16\n0.54\n\n\n\n\n\n\n\n\n\n\n\nNotePostrera Recommendation: Ambiguous\n\n\n\nNo clear winner - different metrics favor different models:\n\n\n\nMetric\nLT1 Winner\nLT2 Winner\n\n\n\n\nSpearman\nSEAS5 (0.51)\nSEAS5 (0.29)\n\n\nRMSE\nCCSM4 (0.97)\nCESM1 (1.17)\n\n\nBias\nCFSv2 (-23mm)\nCESM1 (-21mm)\n\n\nDist Corr\nCCSM4 (0.51)\nCESM1 (0.29)\n\n\nAUC\nSEAS5 (0.69)\nCCSM4 (0.79)\n\n\n\nIf forced to choose one model: CCSM4 has the best average AUC and distance correlation, but no model shows strong, consistent skill.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#conclusions",
    "href": "02_continuous_metrics.html#conclusions",
    "title": "2  Continuous Metrics",
    "section": "2.9 Conclusions",
    "text": "2.9 Conclusions\n\n2.9.1 Key Takeaways\n\nPrimera has genuine forecast skill across multiple metrics. SEAS5 is the clear winner with AUC values around 0.80 - meaning 80% probability of correctly ranking a drought vs non-drought year.\nPostrera remains challenging - no model shows consistent, strong skill. AUC values hover near 0.5-0.6 (barely above random guessing). Consider supplementing with monitoring-based triggers.\nROC-AUC provides the clearest picture - it directly answers “can this forecast discriminate drought?” without depending on threshold choices. Binary F1 scores can be variable with small samples; AUC is more stable.\nBias matters operationally - INSIVUMEH models systematically over-predict Primera rainfall by ~250mm. This could be corrected with simple bias adjustment if these models are used.\nCorrelation and AUC tell consistent stories - both favor SEAS5 for Primera, show weaker skill for Postrera. When multiple metrics agree, we can be more confident in the assessment.\n\n\n\n2.9.2 Operational Recommendations\n\n\n\n\n\n\n\n\n\nSeason\nRecommended Model\nConfidence\nNotes\n\n\n\n\nPrimera LT0\nSEAS5\nHigh\nOnly option for May activation\n\n\nPrimera LT1-2\nSEAS5\nHigh\nBest on all metrics\n\n\nPostrera LT1-2\nCCSM4 or SEAS5\nLow\nConsider ensemble or monitoring triggers",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "03_chirps_tiebreaker.html",
    "href": "03_chirps_tiebreaker.html",
    "title": "3  CHIRPS Tie-Breaker",
    "section": "",
    "text": "3.1 The Need for a Third Opinion\nChapters 1 and 2 evaluated forecasts against two observation sources:\nThese showed consistent results: SEAS5 performs well for Primera, all models struggle with Postrera. But what if both observation sources happen to favor SEAS5? To break potential ties and increase confidence, we introduce a third independent observation source: CHIRPS.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>CHIRPS Tie-Breaker</span>"
    ]
  },
  {
    "objectID": "03_chirps_tiebreaker.html#the-need-for-a-third-opinion",
    "href": "03_chirps_tiebreaker.html#the-need-for-a-third-opinion",
    "title": "3  CHIRPS Tie-Breaker",
    "section": "",
    "text": "ERA5: Global reanalysis (SEAS5’s “native” validation)\nENACTS: Station-blended satellite product (operational standard for Guatemala)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>CHIRPS Tie-Breaker</span>"
    ]
  },
  {
    "objectID": "03_chirps_tiebreaker.html#observational-data-sets",
    "href": "03_chirps_tiebreaker.html#observational-data-sets",
    "title": "3  CHIRPS Tie-Breaker",
    "section": "3.2 Observational Data Sets",
    "text": "3.2 Observational Data Sets\nThis analysis uses three independent observation sources:\nCHIRPS (Climate Hazards Group InfraRed Precipitation with Station data) is a quasi-global rainfall dataset that blends satellite imagery with station data. Key characteristics:\n\n\n\n\n\n\n\n\n\nFeature\nCHIRPS\nENACTS\nERA5\n\n\n\n\nResolution\n0.05° (~5km)\n0.05° (~5km)\n0.25° (~25km)\n\n\nSource\nSatellite + stations\nSatellite + stations\nModel reanalysis\n\n\nCoverage\n50°S-50°N\nRegional\nGlobal\n\n\nTemporal\n1981-present\nVaries\n1940-present\n\n\n\nCHIRPS and ENACTS both blend satellite and station data, but use different algorithms and station networks. This makes CHIRPS a useful independent check - if forecasts perform well against multiple observation sources, we can be more confident in the skill assessment.\n\n\nSetup: Libraries and data loading\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(cumulus)\nlibrary(yardstick)\nlibrary(gghdx)\ngghdx()\n\nbox::use(../../R/enacts)\nbox::use(../../R/seas5)\n\nset.seed(42)\n\n# Configuration\nBASELINE_START &lt;- 2000\nBASELINE_END &lt;- 2024\nPRIMERA_MONTHS &lt;- 5:8\nPOSTRERA_MONTHS &lt;- 9:11\nPRIMERA_ISSUED_MONTHS &lt;- c(3, 4, 5)\nPOSTRERA_ISSUED_MONTHS &lt;- c(6, 7, 8, 9)\n\n# Helper function\ncalc_rp_threshold &lt;- function(x, rp_target = 4, direction = -1) {\n  x &lt;- x[!is.na(x)]\n  n &lt;- length(x)\n  if (n &lt; 3) return(NA_real_)\n  ranks &lt;- rank(x * -direction, ties.method = \"average\")\n  rp &lt;- (n + 1) / ranks\n  approx(rp, x, xout = rp_target, rule = 2)$y\n}\n\n\n\n\nLoad forecast and CHIRPS data\n# Load forecasts\ndf_insiv &lt;- cumulus::blob_read(\n  name = \"ds-aa-lac-dry-corridor/data/processed/insivumeh_special/insivumeh_special_models_zonal_seasonal_chiquimula.parquet\",\n  container = \"projects\"\n)\ndf_seas5 &lt;- seas5$load_seas5_seasonal()\n\n# Load CHIRPS\ndf_chirps_raw &lt;- cumulus::blob_read(\n  name = \"ds-aa-lac-dry-corridor/raw/chirps/2026_cadc_drought_v3_aoi_chirps_monthly_historical.parquet\",\n  container = \"projects\"\n)\n\n# Wrangle CHIRPS to seasonal totals\ndf_chirps &lt;- df_chirps_raw |&gt;\n  filter(ADM1_NAME == \"Chiquimula\") |&gt;\n  mutate(\n    year = year(date),\n    month = month(date),\n    window = case_when(\n      month %in% PRIMERA_MONTHS ~ \"primera\",\n      month %in% POSTRERA_MONTHS ~ \"postrera\",\n      TRUE ~ NA_character_\n    )\n  ) |&gt;\n  filter(!is.na(window), year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n  group_by(year, window) |&gt;\n  summarise(obs_mm = sum(value, na.rm = TRUE), .groups = \"drop\")\n\n# Combine forecasts\ndf_fcst_all &lt;- bind_rows(df_insiv, df_seas5) |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END)\n\n# Filter to operational leadtimes\ndf_fcst_filtered &lt;- df_fcst_all |&gt;\n  mutate(issued_month = month(issued_date)) |&gt;\n  filter(\n    (window == \"primera\" & issued_month %in% PRIMERA_ISSUED_MONTHS) |\n    (window == \"postrera\" & issued_month %in% POSTRERA_ISSUED_MONTHS)\n  )\n\n# Join forecasts with CHIRPS\ndf_joined &lt;- df_fcst_filtered |&gt;\n  left_join(df_chirps, by = c(\"year\", \"window\")) |&gt;\n  filter(!is.na(obs_mm))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>CHIRPS Tie-Breaker</span>"
    ]
  },
  {
    "objectID": "03_chirps_tiebreaker.html#metrics-against-chirps",
    "href": "03_chirps_tiebreaker.html#metrics-against-chirps",
    "title": "3  CHIRPS Tie-Breaker",
    "section": "3.3 Metrics Against CHIRPS",
    "text": "3.3 Metrics Against CHIRPS\n\n\n\n\n\n\nNoteWhy Rank-Based Metrics Work Across Different Observation Sources\n\n\n\nBecause CHIRPS and ENACTS have different climatologies (CHIRPS shows ~300mm higher Primera totals), comparing raw mm errors across sources would be misleading. Instead, we focus on rank-based metrics:\n\nSpearman correlation: Based on ranks, not absolute values. A correlation of 0.7 means the same thing whether rainfall is measured in mm or inches.\nROC-AUC: Asks “do lower forecasts correspond to drought years?” where drought is defined by each source’s own RP4 threshold. This is purely about ranking.\n\nThese metrics allow fair comparison of forecast skill across observation sources with different absolute values.\n\n\n\n\nCalculate thresholds and metrics\n# Forecast thresholds\ndf_fcst_thresh &lt;- df_joined |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  summarise(fcst_thresh = calc_rp_threshold(value), .groups = \"drop\")\n\n# CHIRPS observation thresholds\ndf_obs_thresh &lt;- df_chirps |&gt;\n  group_by(window) |&gt;\n  summarise(obs_thresh = calc_rp_threshold(obs_mm), .groups = \"drop\")\n\n# Join thresholds\ndf_analysis &lt;- df_joined |&gt;\n  left_join(df_fcst_thresh, by = c(\"forecast_source\", \"window\", \"leadtime\")) |&gt;\n  left_join(df_obs_thresh, by = \"window\") |&gt;\n  mutate(\n    fcst_drought = value &lt; fcst_thresh,\n    obs_drought = obs_mm &lt; obs_thresh\n  )\n\n# Continuous metrics\ndf_metrics &lt;- df_analysis |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  summarise(\n    spearman = cor(value, obs_mm, method = \"spearman\", use = \"complete.obs\"),\n    bias_mm = mean(value - obs_mm, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# ROC-AUC\ncalc_auc &lt;- function(df) {\n  if (length(unique(df$truth)) &lt; 2) return(NA_real_)\n  roc_auc(df, truth = truth, drought_score, event_level = \"first\")$.estimate\n}\n\ndf_roc &lt;- df_analysis |&gt;\n  mutate(\n    truth = factor(obs_drought, levels = c(TRUE, FALSE), labels = c(\"drought\", \"no_drought\")),\n    drought_score = -value\n  )\n\ndf_auc &lt;- df_roc |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  summarise(auc = calc_auc(pick(everything())), .groups = \"drop\")\n\n\n\n3.3.1 Spearman Correlation\n\n\nCode\ncreate_metric_heatmap &lt;- function(df, metric_col, metric_name, title, caption, midpoint = 0) {\n  df_plot &lt;- df |&gt;\n    group_by(window, leadtime) |&gt;\n    mutate(is_best = !!sym(metric_col) == max(!!sym(metric_col), na.rm = TRUE)) |&gt;\n    ungroup()\n\n  ggplot(df_plot, aes(x = factor(leadtime), y = forecast_source)) +\n    geom_tile(aes(fill = !!sym(metric_col)), color = \"white\", linewidth = 0.5) +\n    geom_tile(\n      data = df_plot |&gt; filter(is_best),\n      fill = NA, color = \"black\", linewidth = 1.5\n    ) +\n    geom_text(aes(label = sprintf(\"%.2f\", !!sym(metric_col))), size = 4, fontface = \"bold\", color = \"black\") +\n    facet_wrap(~window) +\n    scale_fill_gradient2(\n      low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n      midpoint = midpoint, name = metric_name\n    ) +\n    labs(title = title, x = \"Leadtime (months)\", y = NULL, caption = caption) +\n    theme_minimal() +\n    theme(legend.position = \"right\", panel.grid = element_blank(),\n          plot.caption = element_text(hjust = 0))\n}\n\ncreate_metric_heatmap(\n  df_metrics, \"spearman\", \"Spearman ρ\",\n  \"Spearman Correlation - CHIRPS\",\n  \"All forecasts validated against CHIRPS. Black border = best per leadtime/season.\"\n)\n\n\n\n\n\n\n\n\n\nSurprise finding: INSIVUMEH_CESM1 shows the strongest Spearman correlation for Primera at LT1 (0.72) and LT2 (0.52), outperforming SEAS5. This is the opposite of what we saw with ERA5 and ENACTS.\n\n\n3.3.2 ROC-AUC\n\n\nCode\ncreate_metric_heatmap(\n  df_auc, \"auc\", \"AUC\",\n  \"ROC-AUC - CHIRPS\",\n  \"All forecasts validated against CHIRPS. AUC &gt; 0.7 = acceptable skill. Black border = best.\",\n  midpoint = 0.5\n)\n\n\n\n\n\n\n\n\n\nAUC tells a similar story: CESM1 achieves an exceptional 0.93 AUC at LT1 for Primera. SEAS5 remains competitive, especially at LT0 and LT2.\n\n\n3.3.3 F1 Score (Binary)\nFor completeness, here’s the binary F1 score against CHIRPS - the same metric used in Chapter 1.\n\n\nCode\n# Calculate F1\ndf_f1 &lt;- df_analysis |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  summarise(\n    tp = sum(fcst_drought & obs_drought),\n    fp = sum(fcst_drought & !obs_drought),\n    fn = sum(!fcst_drought & obs_drought),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    precision = tp / (tp + fp),\n    recall = tp / (tp + fn),\n    f1 = 2 * precision * recall / (precision + recall),\n    f1 = if_else(is.nan(f1), 0, f1)\n  )\n\n# F1 heatmap\ndf_plot &lt;- df_f1 |&gt;\n  group_by(window, leadtime) |&gt;\n  mutate(is_best = f1 == max(f1, na.rm = TRUE)) |&gt;\n  ungroup()\n\nggplot(df_plot, aes(x = factor(leadtime), y = forecast_source)) +\n  geom_tile(aes(fill = f1), color = \"white\", linewidth = 0.5) +\n  geom_tile(\n    data = df_plot |&gt; filter(is_best),\n    fill = NA, color = \"black\", linewidth = 1.5\n  ) +\n  geom_text(aes(label = sprintf(\"%.0f%%\", f1 * 100)), size = 4, fontface = \"bold\", color = \"black\") +\n  facet_wrap(~window) +\n  scale_fill_gradientn(\n    colors = c(\"#D73027\", \"#FC8D59\", \"#FEE08B\", \"#D9EF8B\", \"#91CF60\", \"#1A9850\"),\n    limits = c(0, 1), labels = scales::percent, name = \"F1\"\n  ) +\n  labs(\n    title = \"F1 Score - CHIRPS\",\n    x = \"Leadtime (months)\", y = NULL,\n    caption = \"All forecasts validated against CHIRPS. Black border = best per leadtime/season.\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\", panel.grid = element_blank(),\n        plot.caption = element_text(hjust = 0))\n\n\n\n\n\n\n\n\n\n\n\n3.3.4 Bias\n\n\nCode\ndf_plot &lt;- df_metrics |&gt;\n  group_by(window, leadtime) |&gt;\n  mutate(is_best = abs(bias_mm) == min(abs(bias_mm), na.rm = TRUE)) |&gt;\n  ungroup()\n\nbias_max &lt;- max(abs(df_plot$bias_mm), na.rm = TRUE)\n\nggplot(df_plot, aes(x = factor(leadtime), y = forecast_source)) +\n  geom_tile(aes(fill = bias_mm), color = \"white\", linewidth = 0.5) +\n  geom_tile(\n    data = df_plot |&gt; filter(is_best),\n    fill = NA, color = \"black\", linewidth = 1.5\n  ) +\n  geom_text(aes(label = sprintf(\"%.0f\", bias_mm)), size = 4, fontface = \"bold\", color = \"black\") +\n  facet_wrap(~window) +\n  scale_fill_gradient2(\n    low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n    midpoint = 0, limits = c(-bias_max, bias_max), name = \"Bias (mm)\"\n  ) +\n  labs(\n    title = \"Forecast Bias - CHIRPS\",\n    x = \"Leadtime (months)\", y = NULL,\n    caption = \"Units: millimeters. Bias = mean(forecast - observed). Negative = dry bias.\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\", panel.grid = element_blank(),\n        plot.caption = element_text(hjust = 0))\n\n\n\n\n\n\n\n\n\nAgainst CHIRPS, all models have dry bias for Primera and wet bias for Postrera. SEAS5 has the largest dry bias for Primera (~280mm under), while INSIVUMEH models are closer (~120mm under).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>CHIRPS Tie-Breaker</span>"
    ]
  },
  {
    "objectID": "03_chirps_tiebreaker.html#synthesis-three-observation-sources",
    "href": "03_chirps_tiebreaker.html#synthesis-three-observation-sources",
    "title": "3  CHIRPS Tie-Breaker",
    "section": "3.4 Synthesis: Three Observation Sources",
    "text": "3.4 Synthesis: Three Observation Sources\n\n\nCreate synthesis across all observation sources\n# Load metrics from previous chapters (recalculate for consistency)\ndf_enacts_obs &lt;- enacts$load_enacts_seasonal(\"chiquimula\")\n\n# ERA5\ncon &lt;- pg_con()\ndf_era5_raw &lt;- tbl(con, \"era5\") |&gt;\n  filter(pcode == \"GT20\") |&gt;\n  collect() |&gt;\n  mutate(\n    year = year(valid_date),\n    month = month(valid_date),\n    mean = mean * days_in_month(valid_date)\n  )\nDBI::dbDisconnect(con)\n\ndf_era5 &lt;- bind_rows(\n  df_era5_raw |&gt; rename(value = mean) |&gt;\n    filter(month %in% PRIMERA_MONTHS) |&gt;\n    group_by(year) |&gt;\n    summarise(obs_mm = sum(value, na.rm = TRUE), .groups = \"drop\") |&gt;\n    mutate(window = \"primera\"),\n  df_era5_raw |&gt; rename(value = mean) |&gt;\n    filter(month %in% POSTRERA_MONTHS) |&gt;\n    group_by(year) |&gt;\n    summarise(obs_mm = sum(value, na.rm = TRUE), .groups = \"drop\") |&gt;\n    mutate(window = \"postrera\")\n)\n\n# Function to calculate metrics for an obs source\ncalc_obs_metrics &lt;- function(df_obs, obs_name) {\n  df_joined &lt;- df_fcst_filtered |&gt;\n    left_join(df_obs, by = c(\"year\", \"window\")) |&gt;\n    filter(!is.na(obs_mm))\n\n  # Thresholds\n  obs_thresh &lt;- df_obs |&gt;\n    filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n    group_by(window) |&gt;\n    summarise(obs_thresh = calc_rp_threshold(obs_mm), .groups = \"drop\")\n\n  df_analysis &lt;- df_joined |&gt;\n    left_join(obs_thresh, by = \"window\") |&gt;\n    mutate(obs_drought = obs_mm &lt; obs_thresh)\n\n  # Spearman\n  df_spearman &lt;- df_analysis |&gt;\n    group_by(forecast_source, window, leadtime) |&gt;\n    summarise(spearman = cor(value, obs_mm, method = \"spearman\", use = \"complete.obs\"), .groups = \"drop\")\n\n  # AUC\n  df_auc &lt;- df_analysis |&gt;\n    mutate(\n      truth = factor(obs_drought, levels = c(TRUE, FALSE), labels = c(\"drought\", \"no_drought\")),\n      drought_score = -value\n    ) |&gt;\n    group_by(forecast_source, window, leadtime) |&gt;\n    summarise(auc = calc_auc(pick(everything())), .groups = \"drop\")\n\n  df_spearman |&gt;\n    left_join(df_auc, by = c(\"forecast_source\", \"window\", \"leadtime\")) |&gt;\n    mutate(obs_source = obs_name)\n}\n\n# Calculate for all three\ndf_all_metrics &lt;- bind_rows(\n  calc_obs_metrics(df_era5, \"ERA5\"),\n  calc_obs_metrics(df_enacts_obs, \"ENACTS\"),\n  calc_obs_metrics(df_chirps, \"CHIRPS\")\n)\n\n\n\n3.4.1 Primera: Who Wins?\n\n\nCode\ndf_primera &lt;- df_all_metrics |&gt;\n  filter(window == \"primera\", leadtime %in% c(1, 2)) |&gt;\n  select(forecast_source, leadtime, obs_source, auc) |&gt;\n  mutate(\n    model = str_remove(forecast_source, \"INSIVUMEH_\"),\n    obs_source = factor(obs_source, levels = c(\"ERA5\", \"ENACTS\", \"CHIRPS\"))\n  )\n\ndf_primera |&gt;\n  ggplot(aes(x = obs_source, y = auc, fill = model)) +\n  geom_col(position = \"dodge\", alpha = 0.8) +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"grey50\") +\n  geom_hline(yintercept = 0.7, linetype = \"dotted\", color = \"grey30\") +\n  facet_wrap(~paste0(\"Leadtime \", leadtime)) +\n  scale_fill_manual(values = c(\"SEAS5\" = \"#007CE1\", \"CFSv2\" = \"#F2645A\",\n                               \"CCSM4\" = \"#1EBFB3\", \"CESM1\" = \"#FFC627\")) +\n  scale_y_continuous(limits = c(0, 1), labels = scales::percent) +\n  labs(\n    title = \"Primera AUC: Three Observation Sources\",\n    subtitle = \"Dashed = random (0.5), Dotted = acceptable skill (0.7)\",\n    x = \"Observation Source\", y = \"ROC-AUC\", fill = \"Model\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_primera_summary &lt;- df_all_metrics |&gt;\n  filter(window == \"primera\") |&gt;\n  group_by(obs_source, leadtime) |&gt;\n  slice_max(auc, n = 1) |&gt;\n  select(obs_source, leadtime, winner = forecast_source, auc) |&gt;\n  mutate(winner = str_remove(winner, \"INSIVUMEH_\"))\n\ndf_primera_summary |&gt;\n  pivot_wider(names_from = obs_source, values_from = c(winner, auc)) |&gt;\n  arrange(leadtime) |&gt;\n  knitr::kable(\n    col.names = c(\"LT\", \"ERA5 Winner\", \"ENACTS Winner\", \"CHIRPS Winner\",\n                  \"ERA5 AUC\", \"ENACTS AUC\", \"CHIRPS AUC\"),\n    digits = 2,\n    caption = \"Primera: Best model by observation source\"\n  )\n\n\n\nPrimera: Best model by observation source\n\n\n\n\n\n\n\n\n\n\n\nLT\nERA5 Winner\nENACTS Winner\nCHIRPS Winner\nERA5 AUC\nENACTS AUC\nCHIRPS AUC\n\n\n\n\n0\nSEAS5\nSEAS5\nSEAS5\n0.89\n0.80\n0.87\n\n\n1\nCESM1\nSEAS5\nCESM1\n0.93\n0.88\n0.91\n\n\n2\nSEAS5\nSEAS5\nCCSM4\n0.85\n0.88\n0.76\n\n\n\n\n\nPrimera verdict:\n\nLT0: SEAS5 (only option at this leadtime)\nLT1: Split decision - SEAS5 wins on ERA5/ENACTS, CESM1 wins on CHIRPS\nLT2: SEAS5 wins 2 of 3 (ERA5, CHIRPS); CESM1 wins ENACTS\n\nSEAS5 is the safer choice, but CESM1 shows surprisingly strong performance against CHIRPS.\n\n\n3.4.2 Postrera: CCSM4 Emerges as “Least Bad”\n\n\nCode\ndf_postrera &lt;- df_all_metrics |&gt;\n  filter(window == \"postrera\", leadtime %in% c(1, 2, 3)) |&gt;\n  select(forecast_source, leadtime, obs_source, auc) |&gt;\n  mutate(\n    model = str_remove(forecast_source, \"INSIVUMEH_\"),\n    obs_source = factor(obs_source, levels = c(\"ERA5\", \"ENACTS\", \"CHIRPS\"))\n  )\n\ndf_postrera |&gt;\n  filter(leadtime %in% c(1, 2)) |&gt;\n  ggplot(aes(x = obs_source, y = auc, fill = model)) +\n  geom_col(position = \"dodge\", alpha = 0.8) +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"grey50\") +\n  geom_hline(yintercept = 0.7, linetype = \"dotted\", color = \"grey30\") +\n  facet_wrap(~paste0(\"Leadtime \", leadtime)) +\n  scale_fill_manual(values = c(\"SEAS5\" = \"#007CE1\", \"CFSv2\" = \"#F2645A\",\n                               \"CCSM4\" = \"#1EBFB3\", \"CESM1\" = \"#FFC627\")) +\n  scale_y_continuous(limits = c(0, 1), labels = scales::percent) +\n  labs(\n    title = \"Postrera AUC: Three Observation Sources\",\n    subtitle = \"Dashed = random (0.5), Dotted = acceptable skill (0.7)\",\n    x = \"Observation Source\", y = \"ROC-AUC\", fill = \"Model\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Count wins across all obs sources and leadtimes\ndf_postrera_wins &lt;- df_all_metrics |&gt;\n  filter(window == \"postrera\") |&gt;\n  group_by(obs_source, leadtime) |&gt;\n  slice_max(auc, n = 1) |&gt;\n  ungroup() |&gt;\n  count(forecast_source, name = \"wins\") |&gt;\n  mutate(model = str_remove(forecast_source, \"INSIVUMEH_\")) |&gt;\n  arrange(desc(wins))\n\ndf_postrera_wins |&gt;\n  knitr::kable(\n    col.names = c(\"Model\", \"Wins (of 12)\", \"Short Name\"),\n    caption = \"Postrera: Win count across all observation sources and leadtimes\"\n  )\n\n\n\nPostrera: Win count across all observation sources and leadtimes\n\n\nModel\nWins (of 12)\nShort Name\n\n\n\n\nINSIVUMEH_CCSM4\n6\nCCSM4\n\n\nSEAS5\n4\nSEAS5\n\n\nINSIVUMEH_CFSv2\n2\nCFSv2\n\n\n\n\n\nPostrera verdict: No model shows reliable skill. While CCSM4 wins 6 of 12 obs/leadtime combinations, the pattern is suspicious:\n\nCCSM4 shows worse skill at LT1 than at LT2-3\nThis is backwards - forecast skill should degrade with leadtime, not improve\nThis inverted pattern suggests we may be seeing noise rather than genuine skill\nAt LT1 (most actionable), all models hover near AUC 0.5-0.7 - barely above random\n\nThe tie-breaker does not resolve Postrera. No model can be recommended with confidence.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>CHIRPS Tie-Breaker</span>"
    ]
  },
  {
    "objectID": "03_chirps_tiebreaker.html#conclusions",
    "href": "03_chirps_tiebreaker.html#conclusions",
    "title": "3  CHIRPS Tie-Breaker",
    "section": "3.5 Conclusions",
    "text": "3.5 Conclusions\n\n3.5.1 What the Tie-Breaker Tells Us\n\nPrimera skill is robust: Multiple models show genuine skill (AUC &gt; 0.7) across all three observation sources. This isn’t an artifact of one particular dataset.\nSEAS5 vs CESM1 for Primera: SEAS5 performs consistently well across observation sources. CESM1 shows exceptional performance against CHIRPS specifically (AUC 0.93 at LT1), but weaker against ERA5/ENACTS. If operational observations are closer to CHIRPS characteristics, CESM1 might be worth considering.\nPostrera remains unresolved: No model shows reliable, consistent skill. CCSM4’s apparent better performance at longer leadtimes is suspicious - skill should not improve with leadtime. This inverted pattern is likely noise from the small sample (only ~6 drought events in 25 years).\n\n\n\n3.5.2 Final Recommendations\n\n\n\n\n\n\n\n\n\n\nSeason\nLeadtime\nRecommended\nConfidence\nRationale\n\n\n\n\nPrimera\nLT0\nSEAS5\nHigh\nOnly option, strong skill (AUC ~0.87)\n\n\nPrimera\nLT1\nSEAS5\nMedium-High\nWins on ERA5/ENACTS; CESM1 competitive on CHIRPS\n\n\nPrimera\nLT2\nSEAS5\nMedium-High\nConsistent across obs sources\n\n\nPostrera\nAll\nNone\nLow\nNo model shows reliable skill; consider monitoring-based triggers\n\n\n\n\n\n3.5.3 The Postrera Problem\nPostrera forecasts do not pass the basic sanity check: skill should not improve at longer leadtimes. The apparent “wins” for CCSM4 at LT2-3 are likely spurious. With only ~6 drought events in the 25-year record, we simply don’t have enough data to distinguish signal from noise for Postrera.\nRecommendation: For Postrera, consider monitoring-based triggers (e.g., observed rainfall deficits during the season) rather than relying on seasonal forecasts issued months in advance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>CHIRPS Tie-Breaker</span>"
    ]
  },
  {
    "objectID": "03_chirps_tiebreaker.html#appendix-chirps-vs-enacts-comparison",
    "href": "03_chirps_tiebreaker.html#appendix-chirps-vs-enacts-comparison",
    "title": "3  CHIRPS Tie-Breaker",
    "section": "3.6 Appendix: CHIRPS vs ENACTS Comparison",
    "text": "3.6 Appendix: CHIRPS vs ENACTS Comparison\nHow different are CHIRPS and ENACTS for Chiquimula? Understanding this helps interpret why forecast skill might differ across observation sources.\n\n\nCompare CHIRPS and ENACTS observations\n# Load ENACTS for comparison\ndf_enacts_compare &lt;- enacts$load_enacts_seasonal(\"chiquimula\")\n\ndf_compare &lt;- df_chirps |&gt;\n  rename(chirps = obs_mm) |&gt;\n  left_join(\n    df_enacts_compare |&gt; select(year, window, enacts = obs_mm),\n    by = c(\"year\", \"window\")\n  ) |&gt;\n  filter(!is.na(enacts))\n\n# Correlation\ncorr_primera &lt;- df_compare |&gt; filter(window == \"primera\") |&gt;\n  summarise(r = cor(chirps, enacts)) |&gt; pull(r)\ncorr_postrera &lt;- df_compare |&gt; filter(window == \"postrera\") |&gt;\n  summarise(r = cor(chirps, enacts)) |&gt; pull(r)\n\ndf_compare |&gt;\n  ggplot(aes(x = enacts, y = chirps)) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"grey50\") +\n  geom_point(alpha = 0.6, size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"#007CE1\", fill = \"#007CE1\", alpha = 0.2) +\n  facet_wrap(~str_to_title(window), scales = \"free\") +\n  labs(\n    title = \"CHIRPS vs ENACTS: Same Region, Different Estimates\",\n    subtitle = sprintf(\"Primera r = %.2f, Postrera r = %.2f\", corr_primera, corr_postrera),\n    x = \"ENACTS (mm)\",\n    y = \"CHIRPS (mm)\",\n    caption = \"Dashed line = 1:1 agreement. CHIRPS consistently higher for Primera.\"\n  )\n\n\n\n\n\n\n\n\n\nCHIRPS and ENACTS are correlated but not identical. CHIRPS tends to estimate higher rainfall for Primera (~300mm more on average). This means a forecast calibrated to one source may not match the other perfectly - making the three-source comparison a meaningful robustness check.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>CHIRPS Tie-Breaker</span>"
    ]
  }
]