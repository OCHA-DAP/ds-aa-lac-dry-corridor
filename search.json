[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Seasonal Forecast Skill Assessment",
    "section": "",
    "text": "Key Findings\nThis book evaluates seasonal precipitation forecasts for drought prediction in Chiquimula, Guatemala, comparing SEAS5 (ECMWF global) against INSIVUMEH (Guatemala national service) for two agricultural seasons: Primera (May-Aug) and Postrera (Sep-Nov). The low Postrera skill observed for Guatemala raised concerns for Honduras and El Salvador—framework countries that rely solely on SEAS5—prompting an expanded regional analysis that yielded a breakthrough finding.",
    "crumbs": [
      "Key Findings"
    ]
  },
  {
    "objectID": "index.html#decision-steps",
    "href": "index.html#decision-steps",
    "title": "Seasonal Forecast Skill Assessment",
    "section": "Decision Steps",
    "text": "Decision Steps\nChapters 1-6: Guatemala Forecast Comparison\n\n\n\n\n\n\n\n\nStep\nQuestion\nFinding\n\n\n\n\n1. Binary Metrics\nDo forecasts beat random guessing?\nPrimera yes (SEAS5 wins), Postrera inconclusive\n\n\n2. Continuous Metrics\nDoes skill hold with different metrics?\nPrimera confirmed (SEAS5 wins), Postrera ambiguous\n\n\n3. CHIRPS Tie-Breaker\nIs skill robust across observation sources?\nPrimera robust, Postrera still ambiguous\n\n\n4. Temporal Drift\nAre trends/drift affecting skill estimates?\nMild drift, unlikely to explain poor Postrera skill\n\n\n5. Internal Consistency\nAre forecasts internally coherent?\nINSIVUMEH Postrera forecasts lack signal\n\n\n6. ENSO Stratification\nDoes skill vary by ENSO phase?\nPrimera consistent; Postrera El Niño challenging\n\n\n\nChapter 7: Regional Expansion → Breakthrough\nThe poor Postrera skill in Guatemala had implications for Honduras and El Salvador, which use SEAS5 without a national forecast alternative. Expanding the analysis across the Dry Corridor and testing different baseline periods revealed that Postrera skill was masked by the shorter 2000-2024 baseline. With the full 1981-2024 record, SEAS5 Postrera achieves usable skill at LT0-1.",
    "crumbs": [
      "Key Findings"
    ]
  },
  {
    "objectID": "index.html#recommendations",
    "href": "index.html#recommendations",
    "title": "Seasonal Forecast Skill Assessment",
    "section": "Recommendations",
    "text": "Recommendations\n\n\n\n\n\n\n\n\n\n\nSeason\nForecast\nLeadtimes\nConfidence\nRationale\n\n\n\n\nPrimera\nSEAS5\nLT0-2\nHigh\nClear skill advantage across all metrics and observation sources\n\n\nPostrera\nSEAS5\nLT0-1\nModerate\nWith 1981-2024 baseline, ROC-AUC 0.72-0.87 at LT0-1\n\n\n\n\n\n\n\n\n\nImportantBreakthrough: Postrera Skill Revealed with Longer Baseline\n\n\n\nExtending from 2000-2024 (n=25) to 1981-2024 (n=44) dramatically improves Postrera skill estimates:\n\n\n\nLeadtime\nIssued\nROC-AUC (1981-2024)\n\n\n\n\nLT0\nSeptember\n0.80-0.87\n\n\nLT1\nAugust\n0.72-0.79\n\n\n\nOperational recommendation: Use LT1 for planning (~1 month lead), LT0 to confirm as season begins. Use the 1981-2024 baseline for threshold calculation.\n\n\n\n\n\n\n\n\nNoteAbstract\n\n\n\n\n\nThis evaluation informs forecast selection for anticipatory action triggers in Chiquimula, Guatemala (2026). We compared ECMWF SEAS5 and INSIVUMEH (CFSv2, CCSM4, CESM1) seasonal precipitation forecasts using 25 years (2000–2024) of hindcasts validated against three independent observation sources (ERA5, ENACTS, CHIRPS). For Primera (May–August), SEAS5 is recommended based on consistently stronger performance across both binary and continuous skill metrics (F1, ROC-AUC, Spearman correlation), with ROC-AUC values of 0.85–0.90 at 1–2 month lead times that remain robust across all observation sources. For Postrera (September–November), INSIVUMEH forecasts are eliminated due to weak internal consistency: near-zero inter-leadtime correlations (r = -0.1 to 0.2) and coefficient of variation 3–7× lower than observations indicate negligible predictive signal. Initial SEAS5 Postrera analysis using the 2000-2024 baseline also showed limited skill. However, expanding the analysis to Honduras and El Salvador (which rely solely on SEAS5) and extending the baseline to 1981-2024 revealed substantially stronger Postrera skill: ROC-AUC of 0.80-0.87 at LT0 and 0.72-0.79 at LT1. We recommend SEAS5 for both seasons, with Postrera forecasts using LT0-1 and the 1981-2024 baseline for threshold calculation.",
    "crumbs": [
      "Key Findings"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Intro",
    "section": "",
    "text": "1.1 The Decision/Question\nWhich seasonal forecast should Guatemala’s anticipatory action framework use for drought prediction in Chiquimula? Two options are available:\nThis book documents the reserch to answer this question for two agricultural seasons:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro</span>"
    ]
  },
  {
    "objectID": "intro.html#the-decisionquestion",
    "href": "intro.html#the-decisionquestion",
    "title": "1  Intro",
    "section": "",
    "text": "SEAS5: ECMWF’s global seasonal forecasting system\nINSIVUMEH: Regional forecasts from Guatemala’s national meteorological service (CFSv2, CCSM4, and CESM1 models)\n\n\n\n\n\nSeason\nMonths\nDescription\n\n\n\n\nPrimera\nMay-August (MJJA)\nFirst maize planting\n\n\nPostrera\nSeptember-November (SON)\nSecond maize planting",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro</span>"
    ]
  },
  {
    "objectID": "intro.html#research-progression",
    "href": "intro.html#research-progression",
    "title": "1  Intro",
    "section": "1.2 Research Progression",
    "text": "1.2 Research Progression\n\n1.2.1 Step 1: Start with Standard Metrics\nThe analysis began with binary classification metrics (Chapter 1) - the traditional approach where forecasts are scored as hits or misses against a drought threshold (return period of 4 years).\nPrimera results were encouraging: SEAS5 showed F1 scores around 50-60%, significantly better than random guessing. Several model-leadtime combinations beat the 90th percentile of bootstrap random simulations.\nPostrera was troubling: No model showed statistically significant skill. F1 scores hovered near random guessing levels. Different models won at different leadtimes with no clear pattern.\nRemaining question: Are the Postrera results genuinely inconclusive, or are binary metrics too noisy with only ~6 drought events in 25 years?\n\n\n1.2.2 Step 2: Try Continuous Metrics\nChapter 2 introduced continuous metrics that don’t require threshold choices: Spearman correlation, RMSE, and ROC-AUC. These provide partial credit for near-misses and are more stable with small samples.\nPrimera confirmation: SEAS5 dominated across all continuous metrics. AUC values around 0.88 - meaning 88% probability of correctly ranking a drought year below a non-drought year. The signal was real.\nPostrera remained ambiguous: Different metrics favored different models:\n\nSpearman: SEAS5 best at LT1\nRMSE: CCSM4 best\nAUC: SEAS5 at LT1, CCSM4 at LT2\n\nNo model showed AUC consistently above 0.7 (general threshold for “acceptable” discrimination skill).\nRemaining question: Are these results sensitive to which observation dataset we validate against? Would a different “ground truth” give the same answer?\n\n\n1.2.3 Step 3: Bring in a Third Opinion\nChapter 3 introduced CHIRPS as an independent observation source alongside ERA5 and ENACTS. If forecasts perform well against multiple observation sources, the skill assessment is more credible.\nPrimera verdict: Skill was robust across all three observation sources. SEAS5 won most comparisons.\nPostrera raised a red flag: CCSM4 showed better skill at longer leadtimes (LT2-3) than shorter ones (LT1). This is backwards - forecast skill should degrade with increasing leadtime, not improve. The pattern suggested we might be seeing noise rather than genuine skill.\nRemaining question: What’s causing the inverted skill-leadtime relationship for Postrera?\n\n\n1.2.4 Step 4: Check for Drift\nChapter 4 investigated whether temporal trends in forecasts or observations could explain the strange patterns.\nFindings:\n\nENACTS showed a drying trend for Primera, but other sources showed similar patterns\nForecast error drift was modest (~5-8 mm/year)\nSplit-half stability was the key finding: Primera correlations were stable between 2000-2012 and 2013-2024, but Postrera correlations were wildly different between periods\n\nThe drift analysis didn’t explain the inverted skill pattern.\n\n\n1.2.5 Step 5: The Decider - Forecast Internal Consistency\nChapter 5 applied internal consistency diagnostics - asking not “do forecasts predict observations?” but “do forecasts behave like forecasts?”\nInter-leadtime correlations: For a well-behaved forecast, LT1 and LT2 predictions for the same target season should correlate - they’re trying to predict the same thing.\n\nSEAS5: LT1-LT2 correlations of 0.6-0.9 (expected)\nINSIVUMEH Primera: Moderate correlations of 0.3-0.7 (acceptable)\nINSIVUMEH Postrera: Near-zero or negative correlations - when LT1 predicts wet, LT2 might predict dry for the same season\n\nCoefficient of Variation (CV): Forecasts should show interannual variability approaching observed variability.\n\nObserved Postrera CV: ~23%\nSEAS5 Postrera CV: ~15% (reasonable)\nINSIVUMEH Postrera CV: 3-7% - forecasts barely varied from climatology\n\nThis eliminated INSIVUMEH from consideration for Postrera - their forecasts lack predictive signal. But SEAS5 Postrera skill also appeared limited. This raised a concern: Honduras and El Salvador rely solely on SEAS5 without a national forecast alternative.\n\n\n1.2.6 Step 6: ENSO Stratification\nChapter 6 examined whether forecast skill varies by ENSO phase. The key finding: Primera skill is consistent across ENSO phases, but Postrera El Niño years remain challenging for all models.\n\n\n1.2.7 Step 7: The Breakthrough - Regional Comparison & Baseline Extension\nThe poor Postrera skill had implications beyond Guatemala. Honduras and El Salvador use SEAS5 for their anticipatory action frameworks—if SEAS5 Postrera lacks skill, these frameworks are affected too.\nChapter 7 expanded the analysis across the Dry Corridor (Guatemala, Honduras, El Salvador) and tested different baseline periods. This revealed a breakthrough finding:\nThe 2000-2024 baseline was masking Postrera skill.\nExtending to 1981-2024 (44 years) dramatically improved apparent Postrera skill:\n\n\n\nLeadtime\nROC-AUC (2000-2024)\nROC-AUC (1981-2024)\n\n\n\n\nLT0\n0.66-0.81\n0.80-0.87\n\n\nLT1\n0.55-0.79\n0.72-0.79\n\n\nLT2\n0.48-0.62\n0.63-0.77\n\n\n\nThe longer baseline provides more stable threshold estimates and reveals skill that was obscured by the shorter record.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro</span>"
    ]
  },
  {
    "objectID": "intro.html#final-recommendations",
    "href": "intro.html#final-recommendations",
    "title": "1  Intro",
    "section": "1.3 Final Recommendations",
    "text": "1.3 Final Recommendations\n\n\n\n\n\n\n\n\n\n\nSeason\nRecommended\nLeadtimes\nConfidence\nRationale\n\n\n\n\nPrimera\nSEAS5\nLT0-2\nHigh\nClear skill advantage across all metrics and observation sources\n\n\nPostrera\nSEAS5\nLT0-1\nModerate\nWith 1981-2024 baseline, ROC-AUC 0.72-0.87 at LT0-1\n\n\n\n\n1.3.1 INSIVUMEH: Eliminated\nINSIVUMEH forecasts are not recommended for either season due to weak internal consistency. For Postrera specifically, their forecasts exhibit:\n\nNear-zero inter-leadtime correlations (r = -0.1 to 0.2)\nCoefficient of variation 3-7× lower than observations\nNo meaningful predictive signal\n\n\n\n1.3.2 The Postrera Solution\nThe initial Postrera analysis suggested limited skill, but extending the baseline period revealed usable skill at shorter leadtimes:\n\nLT1 (August-issued): Use for initial planning with ~1 month lead time (ROC-AUC 0.72-0.79)\nLT0 (September-issued): Use to confirm/update the forecast as season begins (ROC-AUC 0.80-0.87)\nLT2-3: Skill degrades substantially; less reliable for operational decisions\n\nKey operational requirements:\n\nUse the 1981-2024 baseline for drought threshold calculation (not 2000-2024)\nFocus on LT0 and LT1 for Postrera decisions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro</span>"
    ]
  },
  {
    "objectID": "intro.html#book-structure",
    "href": "intro.html#book-structure",
    "title": "1  Intro",
    "section": "1.4 Book Structure",
    "text": "1.4 Book Structure\nThe chapters document each step of this journey:\n\n\n\n\n\n\n\n\nStep\nQuestion\nFinding\n\n\n\n\n1. Binary Metrics\nDo forecasts beat random guessing?\nPrimera yes (SEAS5 wins), Postrera inconclusive\n\n\n2. Continuous Metrics\nDoes skill hold with different metrics?\nPrimera confirmed (SEAS5 wins), Postrera ambiguous\n\n\n3. CHIRPS Tie-Breaker\nIs skill robust across observation sources?\nPrimera robust, Postrera still ambiguous\n\n\n4. Temporal Drift\nAre trends/drift affecting skill estimates?\nMild drift, unlikely to explain poor Postrera skill\n\n\n5. Internal Consistency\nAre forecasts internally coherent?\nINSIVUMEH Postrera forecasts lack signal\n\n\n6. ENSO Stratification\nDoes skill vary by ENSO phase?\nPrimera consistent; Postrera El Niño challenging\n\n\n7. Regional Comparison\nDoes baseline period matter?\nLonger baseline reveals strong Postrera skill at LT0-1",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html",
    "href": "01_binary_metrics.html",
    "title": "2  Binary Classification Metrics",
    "section": "",
    "text": "3 Introduction\nThis chapter evaluates forecast skill using binary classification metrics - the traditional approach where forecasts are judged as either “correct” or “incorrect” based on whether they cross a drought threshold.\nThis analysis compares the skill of INSIVUMEH (regional) and SEAS5 (global) seasonal precipitation forecasts for detecting drought conditions in Chiquimula, Guatemala. We evaluate forecasts for two agricultural seasons:\nWe use two comparison frameworks:\nDrought is defined using a return period of 4 years (RP4), meaning approximately 6 drought years are expected over the 25-year baseline (2000-2024).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#native-comparison",
    "href": "01_binary_metrics.html#native-comparison",
    "title": "2  Binary Classification Metrics",
    "section": "4.1 Native Comparison",
    "text": "4.1 Native Comparison\nSEAS5 validated against ERA5; INSIVUMEH models validated against ENACTS.\n\n\nCode\ncreate_f1_heatmap(\n  df_skill_native,\n  title = \"F1 Score: Native Observation Sources\",\n  subtitle = \"Black outline = best performer per leadtime\",\n  caption = paste0(\"SEAS5 vs ERA5, INSIVUMEH vs ENACTS (\", aoi_name, \")\")\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#enacts-only-comparison",
    "href": "01_binary_metrics.html#enacts-only-comparison",
    "title": "2  Binary Classification Metrics",
    "section": "4.2 ENACTS-Only Comparison",
    "text": "4.2 ENACTS-Only Comparison\nAll models validated against ENACTS for direct comparison.\n\n\nCode\ncreate_f1_heatmap(\n  df_skill_enacts,\n  title = \"F1 Score: All Models vs ENACTS\",\n  subtitle = \"Black outline = best performer per leadtime\",\n  caption = paste0(\"All forecasts vs ENACTS (\", aoi_name, \")\")\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#primera-season",
    "href": "01_binary_metrics.html#primera-season",
    "title": "2  Binary Classification Metrics",
    "section": "5.1 Primera Season",
    "text": "5.1 Primera Season\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\nplot_yearlt_heatmap(df_yearlt_native, \"primera\", \"native\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_yearlt_heatmap(df_yearlt_enacts, \"primera\", \"enacts_only\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#postrera-season",
    "href": "01_binary_metrics.html#postrera-season",
    "title": "2  Binary Classification Metrics",
    "section": "5.2 Postrera Season",
    "text": "5.2 Postrera Season\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\nplot_yearlt_heatmap(df_yearlt_native, \"postrera\", \"native\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_yearlt_heatmap(df_yearlt_enacts, \"postrera\", \"enacts_only\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#native-comparison-3",
    "href": "01_binary_metrics.html#native-comparison-3",
    "title": "2  Binary Classification Metrics",
    "section": "6.1 Native Comparison",
    "text": "6.1 Native Comparison\n\n\nCode\ncreate_corr_heatmap(\n  df_corr_native,\n  title = \"Anomaly Correlation: Native Observation Sources\",\n  subtitle = \"Black outline = best performer. Higher = better captures interannual variability.\",\n  caption = \"SEAS5 vs ERA5, INSIVUMEH vs ENACTS\"\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#enacts-only-comparison-1",
    "href": "01_binary_metrics.html#enacts-only-comparison-1",
    "title": "2  Binary Classification Metrics",
    "section": "6.2 ENACTS-Only Comparison",
    "text": "6.2 ENACTS-Only Comparison\n\n\nCode\ncreate_corr_heatmap(\n  df_corr_enacts,\n  title = \"Anomaly Correlation: All Models vs ENACTS\",\n  subtitle = \"Black outline = best performer. Higher = better captures interannual variability.\",\n  caption = \"All models vs ENACTS\"\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#scatterplots",
    "href": "01_binary_metrics.html#scatterplots",
    "title": "2  Binary Classification Metrics",
    "section": "6.3 Scatterplots",
    "text": "6.3 Scatterplots\nScatterplots show the relationship between forecast and observed anomalies for each model-leadtime combination.\n\n\nScatterplot function with R² and p-value\ncreate_corr_scatter &lt;- function(df_anom, window_name, title_suffix) {\n  df_plot &lt;- df_anom |&gt;\n    filter(window == window_name) |&gt;\n    mutate(\n      model_short = str_remove(forecast_source, \"INSIVUMEH_\"),\n      facet_label = paste0(model_short, \" (LT\", leadtime, \")\")\n    )\n\n  # Calculate r and p-value for each facet\n  df_stats &lt;- df_plot |&gt;\n    group_by(forecast_source, leadtime, facet_label) |&gt;\n    summarise(\n      r = cor(fcst_anom, obs_anom, use = \"complete.obs\"),\n      p_value = cor.test(fcst_anom, obs_anom)$p.value,\n      .groups = \"drop\"\n    ) |&gt;\n    mutate(\n      label = paste0(\"r = \", sprintf(\"%.2f\", r), \"\\n\",\n                     \"p = \", ifelse(p_value &lt; 0.001, \"&lt;0.001\", sprintf(\"%.3f\", p_value)))\n    )\n\n  ggplot(df_plot, aes(x = obs_anom, y = fcst_anom)) +\n    geom_hline(yintercept = 0, color = \"grey70\", linetype = \"dashed\") +\n    geom_vline(xintercept = 0, color = \"grey70\", linetype = \"dashed\") +\n    geom_smooth(method = \"lm\", se = TRUE, color = \"#007CE1\", fill = \"#007CE1\", alpha = 0.2) +\n    geom_point(alpha = 0.4, size = 2) +\n    geom_text(\n      data = df_stats,\n      aes(x = Inf, y = Inf, label = label),\n      hjust = 1.1, vjust = 1.3, size = 3, color = \"grey20\", fontface = \"bold\",\n      inherit.aes = FALSE\n    ) +\n    facet_wrap(~facet_label, scales = \"free\") +\n    labs(\n      title = paste0(\"Forecast vs Observed Anomalies: \", str_to_title(window_name)),\n      subtitle = title_suffix,\n      x = \"Observed Anomaly (z-score)\",\n      y = \"Forecast Anomaly (z-score)\"\n    ) +\n    theme_minimal() +\n    theme(\n      strip.text = element_text(face = \"bold\", size = 9),\n      panel.grid.minor = element_blank()\n    )\n}\n\n\n\n6.3.1 Primera Season\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\ncreate_corr_scatter(df_anom_native, \"primera\", \"SEAS5 vs ERA5, INSIVUMEH vs ENACTS\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_corr_scatter(df_anom_enacts, \"primera\", \"All models vs ENACTS\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.3.2 Postrera Season\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\ncreate_corr_scatter(df_anom_native, \"postrera\", \"SEAS5 vs ERA5, INSIVUMEH vs ENACTS\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_corr_scatter(df_anom_enacts, \"postrera\", \"All models vs ENACTS\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#primera-season-2",
    "href": "01_binary_metrics.html#primera-season-2",
    "title": "2  Binary Classification Metrics",
    "section": "7.1 Primera Season",
    "text": "7.1 Primera Season\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\ncreate_random_detail_plot(df_comparison_native, \"primera\", random_percentiles, \"native\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_random_detail_plot(df_comparison_enacts, \"primera\", random_percentiles, \"enacts_only\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#postrera-season-2",
    "href": "01_binary_metrics.html#postrera-season-2",
    "title": "2  Binary Classification Metrics",
    "section": "7.2 Postrera Season",
    "text": "7.2 Postrera Season\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\ncreate_random_detail_plot(df_comparison_native, \"postrera\", random_percentiles, \"native\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_random_detail_plot(df_comparison_enacts, \"postrera\", random_percentiles, \"enacts_only\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#summary-by-model",
    "href": "01_binary_metrics.html#summary-by-model",
    "title": "2  Binary Classification Metrics",
    "section": "7.3 Summary by Model",
    "text": "7.3 Summary by Model\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\ncreate_random_summary_plot(df_comparison_native, random_percentiles, \"native\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_random_summary_plot(df_comparison_enacts, random_percentiles, \"enacts_only\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html",
    "href": "02_continuous_metrics.html",
    "title": "3  Continuous Metrics",
    "section": "",
    "text": "3.1 Beyond Binary: A More Nuanced View\nThe binary metrics in Chapter 2 (F1 score, precision, recall) provide one lens on forecast skill, but they can be noisy - especially with only ~25 years of data and ~6 drought events. A single misclassified year can swing F1 scores dramatically.\nConsider an analogy: judging a temperature forecast as “wrong” for predicting 33°F when it was actually 31°F. The forecast told you to expect cold, even if it missed the freezing threshold. Binary metrics don’t distinguish between a near-miss and being completely off.\nThis chapter introduces continuous metrics that provide a more nuanced picture:\nOf these, ROC-AUC is particularly interpretable for operational decisions. An AUC of 0.75 means: “If you randomly select a drought year and a non-drought year, there’s a 75% chance the forecast correctly identifies which is drier.” This directly answers whether the forecast can discriminate drought from non-drought conditions - the core operational question.\nSetup: Libraries and data loading\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(cumulus)\nlibrary(yardstick)\nlibrary(gghdx)\ngghdx()\n\nbox::use(../../R/enacts)\nbox::use(../../R/seas5)\n\nset.seed(42)\n\n# Configuration\nBASELINE_START &lt;- 2000\nBASELINE_END &lt;- 2024\nPRIMERA_MONTHS &lt;- 5:8\nPOSTRERA_MONTHS &lt;- 9:11\nPRIMERA_ISSUED_MONTHS &lt;- c(3, 4, 5)\nPOSTRERA_ISSUED_MONTHS &lt;- c(6, 7, 8, 9)\n\n# Helper function\ncalc_rp_threshold &lt;- function(x, rp_target = 4, direction = -1) {\n  x &lt;- x[!is.na(x)]\n  n &lt;- length(x)\n  if (n &lt; 3) return(NA_real_)\n  ranks &lt;- rank(x * -direction, ties.method = \"average\")\n  rp &lt;- (n + 1) / ranks\n  approx(rp, x, xout = rp_target, rule = 2)$y\n}\nLoad forecast and observation data\n# Load forecasts\ndf_insiv &lt;- cumulus::blob_read(\n  name = \"ds-aa-lac-dry-corridor/data/processed/insivumeh_special/insivumeh_special_models_zonal_seasonal_chiquimula.parquet\",\n  container = \"projects\"\n)\ndf_enacts &lt;- enacts$load_enacts_seasonal(\"chiquimula\")\ndf_seas5 &lt;- seas5$load_seas5_seasonal()\n\n# Load ERA5\ncon &lt;- pg_con()\ndf_era5_raw &lt;- tbl(con, \"era5\") |&gt;\n  filter(pcode == \"GT20\") |&gt;\n  collect() |&gt;\n  mutate(\n    year = year(valid_date),\n    month = month(valid_date),\n    mean = mean * days_in_month(valid_date)\n  )\nDBI::dbDisconnect(con)\n\n# Aggregate ERA5 to seasonal\naggregate_obs_seasonal &lt;- function(df, window_name) {\n  months &lt;- if (window_name == \"primera\") PRIMERA_MONTHS else POSTRERA_MONTHS\n  df |&gt;\n    filter(month %in% months) |&gt;\n    group_by(year) |&gt;\n    summarise(obs_mm = sum(value, na.rm = TRUE), .groups = \"drop\") |&gt;\n    mutate(window = window_name)\n}\n\ndf_era5 &lt;- bind_rows(\n  aggregate_obs_seasonal(df_era5_raw |&gt; rename(value = mean), \"primera\"),\n  aggregate_obs_seasonal(df_era5_raw |&gt; rename(value = mean), \"postrera\")\n)\n\n# Combine forecasts\ndf_fcst_all &lt;- bind_rows(df_insiv, df_seas5) |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END)\n\n# Filter to operational leadtimes\ndf_fcst_filtered &lt;- df_fcst_all |&gt;\n  mutate(issued_month = month(issued_date)) |&gt;\n  filter(\n    (window == \"primera\" & issued_month %in% PRIMERA_ISSUED_MONTHS) |\n    (window == \"postrera\" & issued_month %in% POSTRERA_ISSUED_MONTHS)\n  )",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#beyond-binary-a-more-nuanced-view",
    "href": "02_continuous_metrics.html#beyond-binary-a-more-nuanced-view",
    "title": "3  Continuous Metrics",
    "section": "",
    "text": "Correlation metrics measure how well forecasts track year-to-year variability\nError metrics quantify the typical magnitude of forecast errors\nROC-AUC assesses ranking skill: do lower forecasts correspond to drier years?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#comparison-frameworks",
    "href": "02_continuous_metrics.html#comparison-frameworks",
    "title": "3  Continuous Metrics",
    "section": "3.2 Comparison Frameworks",
    "text": "3.2 Comparison Frameworks\n\n\n\n\n\n\nNoteTwo Ways to Validate Forecasts\n\n\n\nNative Comparison: Each model validated against its “natural” observation source\n\nSEAS5 vs ERA5 (both from ECMWF)\nINSIVUMEH vs ENACTS (regional data)\n\nENACTS-only: All models validated against ENACTS\n\nAllows direct model-to-model comparison\nENACTS is the operational observation source",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#method-1-correlation-metrics",
    "href": "02_continuous_metrics.html#method-1-correlation-metrics",
    "title": "3  Continuous Metrics",
    "section": "3.3 Method 1: Correlation Metrics",
    "text": "3.3 Method 1: Correlation Metrics\n\n3.3.1 What It Measures\nCorrelation measures whether forecasts correctly identify which years are wetter or drier than average. A forecast doesn’t need to predict exact rainfall amounts - it just needs to rank years correctly.\n\n\n3.3.2 Metrics\n\n\n\n\n\n\n\n\nMetric\nFormula\nInterpretation\n\n\n\n\nSpearman ρ\nRank correlation\nDid wet years get higher forecasts? Robust to outliers.\n\n\nPearson r\nLinear correlation\nSame, but sensitive to extreme values.\n\n\n\n\n\n3.3.3 Strengths & Weaknesses\n\n\nStrengths\n\nMeasures interannual variability skill\nNo threshold dependence\nWorks on any units (mm or z-scores)\nSpearman is robust to outliers\n\n\nWeaknesses\n\nDoesn’t measure absolute accuracy\nA forecast could rank years perfectly but have huge bias\nSample size matters (n=25 years)\n\n\n\n\n\nCalculate standardized anomalies\n# Forecast anomalies\ndf_fcst_anom &lt;- df_fcst_filtered |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  mutate(\n    fcst_mean = mean(value, na.rm = TRUE),\n    fcst_sd = sd(value, na.rm = TRUE),\n    fcst_anom = (value - fcst_mean) / fcst_sd\n  ) |&gt;\n  ungroup() |&gt;\n  select(year, window, leadtime, forecast_source, value, fcst_anom)\n\n# Observation anomalies\ncalc_obs_anom &lt;- function(df, obs_name) {\n  df |&gt;\n    filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n    group_by(window) |&gt;\n    mutate(\n      obs_mean = mean(obs_mm, na.rm = TRUE),\n      obs_sd = sd(obs_mm, na.rm = TRUE),\n      obs_anom = (obs_mm - obs_mean) / obs_sd\n    ) |&gt;\n    ungroup() |&gt;\n    mutate(obs_source = obs_name) |&gt;\n    select(year, window, obs_mm, obs_anom, obs_source)\n}\n\ndf_enacts_anom &lt;- calc_obs_anom(df_enacts, \"ENACTS\")\ndf_era5_anom &lt;- calc_obs_anom(df_era5, \"ERA5\")\n\n# Join - Native\ndf_anom_native &lt;- bind_rows(\n  df_fcst_anom |&gt; filter(forecast_source == \"SEAS5\") |&gt;\n    inner_join(df_era5_anom, by = c(\"year\", \"window\")),\n  df_fcst_anom |&gt; filter(str_detect(forecast_source, \"INSIVUMEH\")) |&gt;\n    inner_join(df_enacts_anom, by = c(\"year\", \"window\"))\n)\n\n# Join - ENACTS-only\ndf_anom_enacts &lt;- df_fcst_anom |&gt;\n  inner_join(df_enacts_anom, by = c(\"year\", \"window\"))\n\n\n\n\nCalculate continuous metrics\ncalc_continuous_metrics &lt;- function(df) {\n  df |&gt;\n    group_by(forecast_source, window, leadtime, obs_source) |&gt;\n    summarise(\n      n = n(),\n      spearman = cor(fcst_anom, obs_anom, method = \"spearman\", use = \"complete.obs\"),\n      pearson = cor(fcst_anom, obs_anom, method = \"pearson\", use = \"complete.obs\"),\n      rmse = sqrt(mean((fcst_anom - obs_anom)^2, na.rm = TRUE)),\n      mae = mean(abs(fcst_anom - obs_anom), na.rm = TRUE),\n      bias_mm = mean(value - obs_mm, na.rm = TRUE),\n      .groups = \"drop\"\n    )\n}\n\ndf_metrics_native &lt;- calc_continuous_metrics(df_anom_native)\ndf_metrics_enacts &lt;- calc_continuous_metrics(df_anom_enacts)\n\n\n\n\n3.3.4 Results: Spearman Correlation\n\n\nHeatmap function\ncreate_metric_heatmap &lt;- function(df, metric_col, metric_name, title, caption,\n                                  midpoint = 0, lower_is_better = FALSE) {\n  df_plot &lt;- df |&gt;\n    filter(!is.na(!!sym(metric_col))) |&gt;\n    mutate(window = factor(window, levels = c(\"primera\", \"postrera\"))) |&gt;\n    group_by(window, leadtime) |&gt;\n    mutate(is_best = if (lower_is_better) {\n      !!sym(metric_col) == min(!!sym(metric_col), na.rm = TRUE)\n    } else {\n      !!sym(metric_col) == max(!!sym(metric_col), na.rm = TRUE)\n    }) |&gt;\n    ungroup()\n\n  ggplot(df_plot, aes(x = factor(leadtime), y = forecast_source)) +\n    geom_tile(aes(fill = !!sym(metric_col)), color = \"white\", linewidth = 0.5) +\n    geom_tile(\n      data = df_plot |&gt; filter(is_best),\n      fill = NA, color = \"black\", linewidth = 1.5\n    ) +\n    geom_text(aes(label = sprintf(\"%.2f\", !!sym(metric_col))), size = 4, fontface = \"bold\", color = \"black\") +\n    facet_wrap(~window) +\n    scale_fill_gradient2(\n      low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n      midpoint = midpoint, name = metric_name\n    ) +\n    labs(title = title, x = \"Leadtime (months)\", y = NULL, caption = caption) +\n    theme_minimal() +\n    theme(legend.position = \"right\", panel.grid = element_blank(),\n          plot.caption = element_text(hjust = 0))\n}\n\n\n\nNative ComparisonENACTS-only\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_metrics_native, \"spearman\", \"Spearman ρ\",\n  \"Spearman Rank Correlation - NATIVE\",\n  \"NATIVE: SEAS5 vs ERA5; INSIVUMEH vs ENACTS. Black outline = best per leadtime.\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_metrics_enacts, \"spearman\", \"Spearman ρ\",\n  \"Spearman Rank Correlation - ENACTS-ONLY\",\n  \"ENACTS-ONLY: All models vs ENACTS. Black outline = best per leadtime.\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.5 Interpretation: Correlation\n\n\n\n\n\n\nTipKey Findings\n\n\n\nPrimera (May-Aug)\n\nSEAS5 shows strong correlation (ρ = 0.56-0.60 at LT0-2)\nINSIVUMEH models competitive at LT2, especially CESM1 (ρ = 0.58)\nAll models beat random (ρ = 0 expected by chance)\n\nPostrera (Sep-Nov)\n\nWeaker correlations across all models (ρ = 0.2-0.5)\nSEAS5 slightly better at LT1 (ρ = 0.51)\nCCSM4 shows unexpected strength at LT3 (ρ = 0.48)\nMore variability between leadtimes - less reliable signal",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#method-2-error-magnitude-rmse-mae",
    "href": "02_continuous_metrics.html#method-2-error-magnitude-rmse-mae",
    "title": "3  Continuous Metrics",
    "section": "3.4 Method 2: Error Magnitude (RMSE, MAE)",
    "text": "3.4 Method 2: Error Magnitude (RMSE, MAE)\n\n3.4.1 What It Measures\nRoot Mean Square Error (RMSE) and Mean Absolute Error (MAE) measure how far forecasts deviate from observations in standardized units.\n\n\n3.4.2 Metrics\n\n\n\n\n\n\n\n\nMetric\nFormula\nInterpretation\n\n\n\n\nRMSE\n√(mean((fcst - obs)²))\nPenalizes large errors more heavily\n\n\nMAE\nmean(|fcst - obs|)\nEqual penalty per unit error\n\n\n\n\n\n3.4.3 Strengths & Weaknesses\n\n\nStrengths\n\nMeasures absolute accuracy\nRMSE sensitive to outliers (useful for flagging catastrophic misses)\nMAE more robust, easier to interpret\n\n\nWeaknesses\n\nDepends on units (z-scores vs mm)\nCan be dominated by a few bad years\nDoesn’t distinguish bias from random error\n\n\n\n\n\n3.4.4 Results: RMSE\n\nNative ComparisonENACTS-only\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_metrics_native, \"rmse\", \"RMSE\",\n  \"RMSE (Standardized Anomalies) - NATIVE\",\n  \"NATIVE: SEAS5 vs ERA5; INSIVUMEH vs ENACTS. Lower = better. Black outline = best.\",\n  midpoint = 1, lower_is_better = TRUE\n) +\n  scale_fill_gradient2(low = \"#1A9850\", mid = \"#FFFFBF\", high = \"#D73027\",\n                       midpoint = 1, name = \"RMSE\", limits = c(0.5, 1.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_metrics_enacts, \"rmse\", \"RMSE\",\n  \"RMSE (Standardized Anomalies) - ENACTS-ONLY\",\n  \"ENACTS-ONLY: All models vs ENACTS. Lower = better. Black outline = best.\",\n  midpoint = 1, lower_is_better = TRUE\n) +\n  scale_fill_gradient2(low = \"#1A9850\", mid = \"#FFFFBF\", high = \"#D73027\",\n                       midpoint = 1, name = \"RMSE\", limits = c(0.5, 1.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4.5 Interpretation: RMSE\n\n\n\n\n\n\nTipKey Findings\n\n\n\n\nSEAS5 has lowest RMSE for Primera (0.81-0.95 vs 0.95-1.13 for INSIVUMEH)\nFor Postrera, CCSM4 often has lower RMSE than SEAS5\nRMSE &gt; 1.0 means error exceeds one standard deviation - forecasts add noise rather than signal\nCFSv2 Postrera LT3 has RMSE = 1.44 - actively harmful predictions",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#method-3-bias",
    "href": "02_continuous_metrics.html#method-3-bias",
    "title": "3  Continuous Metrics",
    "section": "3.5 Method 3: Bias",
    "text": "3.5 Method 3: Bias\n\n3.5.1 What It Measures\nBias measures systematic over- or under-prediction. A forecast with zero bias is “calibrated” on average, even if individual predictions are wrong.\n\n\n3.5.2 Formula\n\\[\\text{Bias} = \\text{mean}(\\text{forecast} - \\text{observed})\\]\n\nPositive bias: Forecast too wet\nNegative bias: Forecast too dry\n\n\n\n3.5.3 Strengths & Weaknesses\n\n\nStrengths\n\nEasy to interpret (in mm)\nCan be corrected with simple adjustments\nReveals systematic model issues\n\n\nWeaknesses\n\nHides random error (a model can have zero bias but huge scatter)\nMust use raw mm, not z-scores (z-score bias is always ~0 by construction)\n\n\n\n\n\n\n\n\n\nWarningImportant Note on Units\n\n\n\nBias must be calculated on raw millimeters, not standardized anomalies. When you standardize to z-scores, you force the mean to zero - so bias between two z-scored series is mathematically guaranteed to be ~0, which is meaningless.\n\n\n\n\n3.5.4 Results: Bias (Raw mm)\n\n\nBias heatmap function\ncreate_bias_heatmap &lt;- function(df, title, caption) {\n  df_plot &lt;- df |&gt;\n    filter(!is.na(bias_mm)) |&gt;\n    mutate(window = factor(window, levels = c(\"primera\", \"postrera\"))) |&gt;\n    group_by(window, leadtime) |&gt;\n    mutate(is_best = abs(bias_mm) == min(abs(bias_mm), na.rm = TRUE)) |&gt;\n    ungroup()\n\n  bias_max &lt;- max(abs(df_plot$bias_mm), na.rm = TRUE)\n\n  ggplot(df_plot, aes(x = factor(leadtime), y = forecast_source)) +\n    geom_tile(aes(fill = bias_mm), color = \"white\", linewidth = 0.5) +\n    geom_tile(\n      data = df_plot |&gt; filter(is_best),\n      fill = NA, color = \"black\", linewidth = 1.5\n    ) +\n    geom_text(aes(label = sprintf(\"%.0f\", bias_mm)), size = 4, fontface = \"bold\", color = \"black\") +\n    facet_wrap(~window) +\n    scale_fill_gradient2(\n      low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n      midpoint = 0, limits = c(-bias_max, bias_max), name = \"Bias (mm)\"\n    ) +\n    labs(title = title, x = \"Leadtime (months)\", y = NULL, caption = caption) +\n    theme_minimal() +\n    theme(legend.position = \"right\", panel.grid = element_blank(),\n          plot.caption = element_text(hjust = 0))\n}\n\n\n\nNative ComparisonENACTS-only\n\n\n\n\nCode\ncreate_bias_heatmap(\n  df_metrics_native,\n  \"Mean Bias - NATIVE\",\n  \"NATIVE: SEAS5 vs ERA5; INSIVUMEH vs ENACTS. Units: mm. Black outline = closest to zero.\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_bias_heatmap(\n  df_metrics_enacts,\n  \"Mean Bias - ENACTS-ONLY\",\n  \"ENACTS-ONLY: All models vs ENACTS. Units: mm. Black outline = closest to zero.\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.5 Interpretation: Bias\n\n\n\n\n\n\nTipKey Findings\n\n\n\nPrimera\n\nAll models are biased wet (positive bias)\nSEAS5: +68 to +132mm too wet\nINSIVUMEH models: +244 to +262mm too wet (severe overestimation)\nThis means forecasts systematically predict more rain than actually falls\n\nPostrera\n\nAll models are biased dry (negative bias)\nSEAS5: -113 to -132mm too dry\nINSIVUMEH models: -21 to -37mm too dry (much smaller bias)\nFor Postrera, INSIVUMEH has better calibration than SEAS5",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#method-4-distance-to-threshold",
    "href": "02_continuous_metrics.html#method-4-distance-to-threshold",
    "title": "3  Continuous Metrics",
    "section": "3.6 Method 4: Distance-to-Threshold",
    "text": "3.6 Method 4: Distance-to-Threshold\n\n3.6.1 What It Measures\nInstead of binary hit/miss, measure how far each forecast and observation was from their respective drought thresholds. Then correlate these distances.\n\n\n3.6.2 Why This Matters\nConsider two “false positive” cases:\n\nForecast: 10mm below threshold. Observed: 5mm above threshold. (Near miss)\nForecast: 100mm below threshold. Observed: 150mm above threshold. (Confident and wrong)\n\nBinary metrics score both as equally wrong. Distance-to-threshold reveals that Case 1 was almost right, while Case 2 was a confident failure.\n\n\n3.6.3 Calculation\nfcst_distance = forecast_value - forecast_threshold\nobs_distance = observed_value - observed_threshold\n\ndistance_correlation = cor(fcst_distance, obs_distance)\n\nNegative distance = below threshold (drought-like)\nPositive distance = above threshold (normal)\n\n\n\n3.6.4 Strengths & Weaknesses\n\n\nStrengths\n\nGives partial credit for near-misses\nOperationally relevant (close calls matter)\nUses actual mm, not abstract scores\n\n\nWeaknesses\n\nStill depends on threshold choice\nCorrelation can be high even if absolute distances don’t match\n\n\n\n\n\nCalculate distance-to-threshold\n# Observation thresholds\nobs_thresh_enacts &lt;- df_enacts |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n  group_by(window) |&gt;\n  summarise(obs_thresh = calc_rp_threshold(obs_mm, 4, -1), .groups = \"drop\")\n\nobs_thresh_era5 &lt;- df_era5 |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n  group_by(window) |&gt;\n  summarise(obs_thresh = calc_rp_threshold(obs_mm, 4, -1), .groups = \"drop\")\n\n# Forecast thresholds\nfcst_thresholds &lt;- df_fcst_filtered |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  summarise(fcst_thresh = calc_rp_threshold(value, 4, -1), .groups = \"drop\")\n\n# Build distance data\nbuild_distance_data &lt;- function(df_fcst, df_obs, obs_thresholds, obs_name) {\n  df_obs_filtered &lt;- df_obs |&gt; filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END)\n\n  df_fcst |&gt;\n    left_join(df_obs_filtered |&gt; select(year, window, obs_mm), by = c(\"year\", \"window\")) |&gt;\n    left_join(obs_thresholds, by = \"window\") |&gt;\n    left_join(fcst_thresholds, by = c(\"forecast_source\", \"window\", \"leadtime\")) |&gt;\n    mutate(\n      fcst_distance_mm = value - fcst_thresh,\n      obs_distance_mm = obs_mm - obs_thresh,\n      fcst_drought = value &lt;= fcst_thresh,\n      obs_drought = obs_mm &lt;= obs_thresh,\n      obs_source = obs_name\n    ) |&gt;\n    filter(!is.na(obs_mm), !is.na(fcst_thresh))\n}\n\ndf_dist_native &lt;- bind_rows(\n  build_distance_data(df_fcst_filtered |&gt; filter(forecast_source == \"SEAS5\"),\n                      df_era5, obs_thresh_era5, \"ERA5\"),\n  build_distance_data(df_fcst_filtered |&gt; filter(str_detect(forecast_source, \"INSIVUMEH\")),\n                      df_enacts, obs_thresh_enacts, \"ENACTS\")\n)\n\ndf_dist_enacts &lt;- build_distance_data(df_fcst_filtered, df_enacts, obs_thresh_enacts, \"ENACTS\")\n\n# Calculate distance metrics\ncalc_distance_metrics &lt;- function(df) {\n  df |&gt;\n    group_by(forecast_source, window, leadtime, obs_source) |&gt;\n    summarise(\n      dist_corr = cor(fcst_distance_mm, obs_distance_mm, use = \"complete.obs\"),\n      mae_dist = mean(abs(fcst_distance_mm - obs_distance_mm), na.rm = TRUE),\n      .groups = \"drop\"\n    )\n}\n\ndf_dist_metrics_native &lt;- calc_distance_metrics(df_dist_native)\ndf_dist_metrics_enacts &lt;- calc_distance_metrics(df_dist_enacts)\n\n\n\n\n3.6.5 Results: Distance Correlation\n\nNative ComparisonENACTS-only\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_dist_metrics_native, \"dist_corr\", \"Correlation\",\n  \"Distance-to-Threshold Correlation - NATIVE\",\n  \"NATIVE: SEAS5 vs ERA5; INSIVUMEH vs ENACTS. Higher = forecast distances match observed distances.\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_dist_metrics_enacts, \"dist_corr\", \"Correlation\",\n  \"Distance-to-Threshold Correlation - ENACTS-ONLY\",\n  \"ENACTS-ONLY: All models vs ENACTS. Higher = forecast distances match observed distances.\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.6.6 Scatter Plot: Distance Comparison\n\n\nDistance scatter plots\ndf_dist_enacts |&gt;\n  filter(window == \"primera\", leadtime %in% c(1, 2)) |&gt;\n  mutate(\n    model = str_remove(forecast_source, \"INSIVUMEH_\"),\n    facet = paste0(model, \" LT\", leadtime)\n  ) |&gt;\n  ggplot(aes(x = obs_distance_mm, y = fcst_distance_mm)) +\n  geom_hline(yintercept = 0, color = \"grey70\", linetype = \"dashed\") +\n  geom_vline(xintercept = 0, color = \"grey70\", linetype = \"dashed\") +\n  geom_abline(slope = 1, intercept = 0, color = \"#B2182B\", linetype = \"dotted\", linewidth = 0.8) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"#007CE1\", fill = \"#007CE1\", alpha = 0.2) +\n  geom_point(aes(color = obs_drought), alpha = 0.6, size = 2.5) +\n  facet_wrap(~facet, ncol = 4) +\n  scale_color_manual(\n    values = c(\"TRUE\" = \"#D73027\", \"FALSE\" = \"#4575B4\"),\n    labels = c(\"TRUE\" = \"Drought year\", \"FALSE\" = \"Normal year\"),\n    name = \"Observed\"\n  ) +\n  labs(\n    title = \"Primera LT1-2: Forecast vs Observed Distance from Threshold\",\n    subtitle = \"Dotted red = perfect agreement. Blue = linear fit. Dashed lines = threshold (distance = 0).\",\n    x = \"Observed Distance from Threshold (mm)\",\n    y = \"Forecast Distance from Threshold (mm)\",\n    caption = \"ENACTS-ONLY comparison. Negative = below threshold (drought-like).\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\", plot.caption = element_text(hjust = 0))\n\n\n\n\n\n\n\n\n\n\n\n3.6.7 Interpretation: Distance-to-Threshold\n\n\n\n\n\n\nTipKey Findings\n\n\n\nPrimera\n\nSEAS5 LT1 has the strongest distance correlation (r = 0.66)\nWhen SEAS5 says “drier than usual”, observations tend to agree\nINSIVUMEH models also show skill (r = 0.5-0.53)\n\nPostrera\n\nCCSM4 shows best distance skill at LT1 (r = 0.51)\nSEAS5 Postrera distance correlation drops sharply at LT2 (r = 0.16)\nCFSv2 LT3 has negative correlation (r = -0.08) - actively misleading",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#method-5-roc-curves-auc",
    "href": "02_continuous_metrics.html#method-5-roc-curves-auc",
    "title": "3  Continuous Metrics",
    "section": "3.7 Method 5: ROC Curves & AUC",
    "text": "3.7 Method 5: ROC Curves & AUC\n\n3.7.1 What It Measures\nROC (Receiver Operating Characteristic) curves measure ranking skill: did lower forecast values correspond to actual drought years?\nUnlike binary metrics, ROC doesn’t require choosing a specific threshold. It asks: “If I ranked all years by forecast value, would drought years tend to be at the bottom?”\n\n\n3.7.2 How to Read an ROC Curve\n\nDiagonal line: Random guessing (AUC = 0.5)\nCurve above diagonal: Skill (drought years get lower forecasts)\nCurve below diagonal: Inverted skill (drought years get higher forecasts - very bad)\n\n\n\n3.7.3 AUC Interpretation\n\n\n\nAUC\nInterpretation\n\n\n\n\n0.9-1.0\nOutstanding\n\n\n0.8-0.9\nExcellent\n\n\n0.7-0.8\nAcceptable\n\n\n0.5-0.7\nPoor\n\n\n&lt; 0.5\nWorse than random\n\n\n\n\n\n3.7.4 Strengths & Weaknesses\n\n\nStrengths\n\nThreshold-independent\nMeasures ranking/discrimination skill\nWell-understood statistically\n\n\nWeaknesses\n\nDoesn’t measure calibration\nCan be high even with severe bias\nSmall sample size (6 drought years) limits precision\n\n\n\n\n\nCalculate ROC and AUC\n# Build ROC data\nbuild_roc_data &lt;- function(df_fcst, df_obs, obs_thresholds, obs_name) {\n  df_obs_filtered &lt;- df_obs |&gt; filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END)\n\n  df_fcst |&gt;\n    left_join(df_obs_filtered |&gt; select(year, window, obs_mm), by = c(\"year\", \"window\")) |&gt;\n    left_join(obs_thresholds, by = \"window\") |&gt;\n    mutate(\n      truth = factor(obs_mm &lt;= obs_thresh, c(TRUE, FALSE), c(\"drought\", \"no_drought\")),\n      drought_score = -value,  # Lower forecast = higher drought score\n      obs_source = obs_name\n    ) |&gt;\n    filter(!is.na(truth))\n}\n\ndf_roc_native &lt;- bind_rows(\n  build_roc_data(df_fcst_filtered |&gt; filter(forecast_source == \"SEAS5\"),\n                 df_era5, obs_thresh_era5, \"ERA5\"),\n  build_roc_data(df_fcst_filtered |&gt; filter(str_detect(forecast_source, \"INSIVUMEH\")),\n                 df_enacts, obs_thresh_enacts, \"ENACTS\")\n)\n\ndf_roc_enacts &lt;- build_roc_data(df_fcst_filtered, df_enacts, obs_thresh_enacts, \"ENACTS\")\n\n# Calculate AUC\ncalc_auc &lt;- function(df) {\n  if (length(unique(df$truth)) &lt; 2) return(NA_real_)\n  roc_auc(df, truth = truth, drought_score, event_level = \"first\")$.estimate\n}\n\ndf_auc_native &lt;- df_roc_native |&gt;\n  group_by(forecast_source, window, leadtime, obs_source) |&gt;\n  summarise(auc = calc_auc(pick(everything())), .groups = \"drop\")\n\ndf_auc_enacts &lt;- df_roc_enacts |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  summarise(auc = calc_auc(pick(everything())), obs_source = \"ENACTS\", .groups = \"drop\")\n\n\n\n\n3.7.5 Results: AUC Heatmaps\n\nNative ComparisonENACTS-only\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_auc_native, \"auc\", \"AUC\",\n  \"ROC-AUC - NATIVE\",\n  \"NATIVE: SEAS5 vs ERA5; INSIVUMEH vs ENACTS. AUC &gt; 0.7 = acceptable skill.\",\n  midpoint = 0.5\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_auc_enacts, \"auc\", \"AUC\",\n  \"ROC-AUC - ENACTS-ONLY\",\n  \"ENACTS-ONLY: All models vs ENACTS. AUC &gt; 0.7 = acceptable skill.\",\n  midpoint = 0.5\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.7.6 ROC Curves: Primera\n\n\nROC curves\n# Calculate ROC curves\ncalc_roc_curve &lt;- function(df) {\n  if (length(unique(df$truth)) &lt; 2) return(tibble(specificity = NA, sensitivity = NA))\n  roc_curve(df, truth = truth, drought_score, event_level = \"first\")\n}\n\ndf_roc_curves_enacts &lt;- df_roc_enacts |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  group_modify(~calc_roc_curve(.x)) |&gt;\n  ungroup()\n\ndf_roc_curves_enacts |&gt;\n  filter(window == \"primera\", !is.na(specificity)) |&gt;\n  mutate(\n    model = str_remove(forecast_source, \"INSIVUMEH_\"),\n    facet = paste0(model, \" LT\", leadtime)\n  ) |&gt;\n  left_join(\n    df_auc_enacts |&gt; filter(window == \"primera\") |&gt;\n      mutate(model = str_remove(forecast_source, \"INSIVUMEH_\"),\n             facet = paste0(model, \" LT\", leadtime),\n             label = paste0(\"AUC = \", sprintf(\"%.2f\", auc))),\n    by = c(\"forecast_source\", \"window\", \"leadtime\", \"facet\", \"model\")\n  ) |&gt;\n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"grey50\") +\n  geom_path(color = \"#007CE1\", linewidth = 1) +\n  geom_text(aes(x = 0.7, y = 0.2, label = label), hjust = 0, size = 3, fontface = \"bold\",\n            color = \"#007CE1\", check_overlap = TRUE) +\n  facet_wrap(~facet, ncol = 4) +\n  coord_equal() +\n  labs(\n    title = \"ROC Curves: Primera - ENACTS-ONLY\",\n    subtitle = \"Dashed line = random guessing. Curve above diagonal = skill.\",\n    x = \"False Positive Rate (1 - Specificity)\",\n    y = \"True Positive Rate (Sensitivity)\",\n    caption = \"ENACTS-ONLY: All models vs ENACTS.\"\n  ) +\n  theme_minimal() +\n  theme(strip.text = element_text(face = \"bold\"), plot.caption = element_text(hjust = 0))\n\n\n\n\n\n\n\n\n\n\n\n3.7.7 Interpretation: ROC-AUC\n\n\n\n\n\n\nTipKey Findings\n\n\n\nPrimera\n\nSEAS5 and CESM1 show excellent discrimination (AUC = 0.87-0.88 at LT1-2)\nCCSM4 also strong (AUC = 0.81 at LT1)\nAll models beat random guessing\n\nPostrera\n\nMuch weaker AUC across all models (0.5-0.8 range)\nCCSM4 LT3 surprisingly good (AUC = 0.83)\nSEAS5 LT2 near random (AUC = 0.54)\nCFSv2 LT3 inverted (AUC = 0.40) - lower forecasts correspond to wetter years",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#summary-which-model-wins",
    "href": "02_continuous_metrics.html#summary-which-model-wins",
    "title": "3  Continuous Metrics",
    "section": "3.8 Summary: Which Model Wins?",
    "text": "3.8 Summary: Which Model Wins?\n\n\nSummary comparison\ndf_summary &lt;- df_metrics_enacts |&gt;\n  left_join(df_dist_metrics_enacts, by = c(\"forecast_source\", \"window\", \"leadtime\", \"obs_source\")) |&gt;\n  left_join(df_auc_enacts, by = c(\"forecast_source\", \"window\", \"leadtime\", \"obs_source\")) |&gt;\n  filter(leadtime %in% c(1, 2)) |&gt;\n  mutate(model = str_remove(forecast_source, \"INSIVUMEH_\")) |&gt;\n  select(model, window, leadtime, spearman, rmse, bias_mm, dist_corr, auc) |&gt;\n  mutate(across(c(spearman, rmse, dist_corr, auc), ~round(.x, 2)),\n         bias_mm = round(bias_mm, 0))\n\n\n\n3.8.1 Primera (MJJA)\n\n\nCode\ndf_summary |&gt;\n  filter(window == \"primera\") |&gt;\n  arrange(leadtime, desc(auc)) |&gt;\n  knitr::kable(\n    col.names = c(\"Model\", \"Window\", \"LT\", \"Spearman\", \"RMSE\", \"Bias (mm)\", \"Dist Corr\", \"AUC\"),\n    caption = \"Primera: All metrics at LT1-2 (ENACTS-only)\"\n  )\n\n\n\nPrimera: All metrics at LT1-2 (ENACTS-only)\n\n\nModel\nWindow\nLT\nSpearman\nRMSE\nBias (mm)\nDist Corr\nAUC\n\n\n\n\nSEAS5\nprimera\n1\n0.56\n0.81\n96\n0.66\n0.88\n\n\nCESM1\nprimera\n1\n0.48\n0.95\n261\n0.53\n0.87\n\n\nCCSM4\nprimera\n1\n0.48\n0.96\n258\n0.52\n0.81\n\n\nCFSv2\nprimera\n1\n0.39\n0.98\n253\n0.50\n0.65\n\n\nSEAS5\nprimera\n2\n0.56\n0.94\n68\n0.54\n0.88\n\n\nCESM1\nprimera\n2\n0.58\n1.02\n259\n0.46\n0.86\n\n\nCFSv2\nprimera\n2\n0.47\n1.00\n245\n0.48\n0.75\n\n\nCCSM4\nprimera\n2\n0.32\n1.13\n248\n0.34\n0.70\n\n\n\n\n\n\n\n\n\n\n\nNotePrimera Recommendation: SEAS5\n\n\n\n\nHighest AUC (0.88 at both LT1 and LT2)\nHighest distance correlation (0.66 at LT1)\nLowest RMSE (0.81 at LT1)\nSmallest bias (+68 to +96mm vs +244-262mm for INSIVUMEH)\nCESM1 is competitive but has much larger bias\n\n\n\n\n\n3.8.2 Postrera (SON)\n\n\nCode\ndf_summary |&gt;\n  filter(window == \"postrera\") |&gt;\n  arrange(leadtime, desc(auc)) |&gt;\n  knitr::kable(\n    col.names = c(\"Model\", \"Window\", \"LT\", \"Spearman\", \"RMSE\", \"Bias (mm)\", \"Dist Corr\", \"AUC\"),\n    caption = \"Postrera: All metrics at LT1-2 (ENACTS-only)\"\n  )\n\n\n\nPostrera: All metrics at LT1-2 (ENACTS-only)\n\n\nModel\nWindow\nLT\nSpearman\nRMSE\nBias (mm)\nDist Corr\nAUC\n\n\n\n\nSEAS5\npostrera\n1\n0.51\n1.14\n-131\n0.32\n0.69\n\n\nCCSM4\npostrera\n1\n0.50\n0.97\n-37\n0.51\n0.64\n\n\nCESM1\npostrera\n1\n0.19\n1.28\n-33\n0.14\n0.64\n\n\nCFSv2\npostrera\n1\n0.19\n1.26\n-23\n0.18\n0.58\n\n\nCCSM4\npostrera\n2\n0.26\n1.31\n-28\n0.10\n0.79\n\n\nCFSv2\npostrera\n2\n0.06\n1.28\n-28\n0.15\n0.68\n\n\nCESM1\npostrera\n2\n0.08\n1.17\n-21\n0.29\n0.61\n\n\nSEAS5\npostrera\n2\n0.29\n1.27\n-128\n0.16\n0.54\n\n\n\n\n\n\n\n\n\n\n\nNotePostrera Recommendation: Ambiguous\n\n\n\nNo clear winner - different metrics favor different models:\n\n\n\nMetric\nLT1 Winner\nLT2 Winner\n\n\n\n\nSpearman\nSEAS5 (0.51)\nSEAS5 (0.29)\n\n\nRMSE\nCCSM4 (0.97)\nCESM1 (1.17)\n\n\nBias\nCFSv2 (-23mm)\nCESM1 (-21mm)\n\n\nDist Corr\nCCSM4 (0.51)\nCESM1 (0.29)\n\n\nAUC\nSEAS5 (0.69)\nCCSM4 (0.79)\n\n\n\nIf forced to choose one model: CCSM4 has the best average AUC and distance correlation, but no model shows strong, consistent skill.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#conclusions",
    "href": "02_continuous_metrics.html#conclusions",
    "title": "3  Continuous Metrics",
    "section": "3.9 Conclusions",
    "text": "3.9 Conclusions\n\n3.9.1 Key Takeaways\n\nPrimera has genuine forecast skill across multiple metrics. SEAS5 is the clear winner with AUC values around 0.80 - meaning 80% probability of correctly ranking a drought vs non-drought year.\nPostrera remains challenging - no model shows consistent, strong skill. AUC values hover near 0.5-0.6 (barely above random guessing). Consider supplementing with monitoring-based triggers.\nROC-AUC provides the clearest picture - it directly answers “can this forecast discriminate drought?” without depending on threshold choices. Binary F1 scores can be variable with small samples; AUC is more stable.\nBias matters operationally - INSIVUMEH models systematically over-predict Primera rainfall by ~250mm. This could be corrected with simple bias adjustment if these models are used.\nCorrelation and AUC tell consistent stories - both favor SEAS5 for Primera, show weaker skill for Postrera. When multiple metrics agree, we can be more confident in the assessment.\n\n\n\n3.9.2 Operational Recommendations\n\n\n\n\n\n\n\n\n\nSeason\nRecommended Model\nConfidence\nNotes\n\n\n\n\nPrimera LT0\nSEAS5\nHigh\nOnly option for May activation\n\n\nPrimera LT1-2\nSEAS5\nHigh\nBest on all metrics\n\n\nPostrera LT1-2\nCCSM4 or SEAS5\nLow\nConsider ensemble or monitoring triggers",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "03_chirps_tiebreaker.html",
    "href": "03_chirps_tiebreaker.html",
    "title": "4  CHIRPS Tie-Breaker",
    "section": "",
    "text": "4.1 The Need for a Third Opinion\nChapters 2 and 3 evaluated forecasts against two observation sources:\nThese showed consistent results: SEAS5 performs well for Primera, all models struggle with Postrera. But what if both observation sources happen to favor SEAS5? To break potential ties and increase confidence, we introduce a third independent observation source: CHIRPS.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>CHIRPS Tie-Breaker</span>"
    ]
  },
  {
    "objectID": "03_chirps_tiebreaker.html#the-need-for-a-third-opinion",
    "href": "03_chirps_tiebreaker.html#the-need-for-a-third-opinion",
    "title": "4  CHIRPS Tie-Breaker",
    "section": "",
    "text": "ERA5: Global reanalysis (SEAS5’s “native” validation)\nENACTS: Station-blended satellite product (operational standard for Guatemala)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>CHIRPS Tie-Breaker</span>"
    ]
  },
  {
    "objectID": "03_chirps_tiebreaker.html#observational-data-sets",
    "href": "03_chirps_tiebreaker.html#observational-data-sets",
    "title": "4  CHIRPS Tie-Breaker",
    "section": "4.2 Observational Data Sets",
    "text": "4.2 Observational Data Sets\nThis analysis uses three independent observation sources:\nCHIRPS (Climate Hazards Group InfraRed Precipitation with Station data) is a quasi-global rainfall dataset that blends satellite imagery with station data. Key characteristics:\n\n\n\n\n\n\n\n\n\nFeature\nCHIRPS\nENACTS\nERA5\n\n\n\n\nResolution\n0.05° (~5km)\n0.05° (~5km)\n0.25° (~25km)\n\n\nSource\nSatellite + stations\nSatellite + stations\nModel reanalysis\n\n\nCoverage\n50°S-50°N\nRegional\nGlobal\n\n\nTemporal\n1981-present\nVaries\n1940-present\n\n\n\nCHIRPS and ENACTS both blend satellite and station data, but use different algorithms and station networks. This makes CHIRPS a useful independent check - if forecasts perform well against multiple observation sources, we can be more confident in the skill assessment.\n\n\nSetup: Libraries and data loading\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(cumulus)\nlibrary(yardstick)\nlibrary(gghdx)\ngghdx()\n\nbox::use(../../R/enacts)\nbox::use(../../R/seas5)\n\nset.seed(42)\n\n# Configuration\nBASELINE_START &lt;- 2000\nBASELINE_END &lt;- 2024\nPRIMERA_MONTHS &lt;- 5:8\nPOSTRERA_MONTHS &lt;- 9:11\nPRIMERA_ISSUED_MONTHS &lt;- c(3, 4, 5)\nPOSTRERA_ISSUED_MONTHS &lt;- c(6, 7, 8, 9)\n\n# Helper function\ncalc_rp_threshold &lt;- function(x, rp_target = 4, direction = -1) {\n  x &lt;- x[!is.na(x)]\n  n &lt;- length(x)\n  if (n &lt; 3) return(NA_real_)\n  ranks &lt;- rank(x * -direction, ties.method = \"average\")\n  rp &lt;- (n + 1) / ranks\n  approx(rp, x, xout = rp_target, rule = 2)$y\n}\n\n\n\n\nLoad forecast and CHIRPS data\n# Load forecasts\ndf_insiv &lt;- cumulus::blob_read(\n  name = \"ds-aa-lac-dry-corridor/data/processed/insivumeh_special/insivumeh_special_models_zonal_seasonal_chiquimula.parquet\",\n  container = \"projects\"\n)\ndf_seas5 &lt;- seas5$load_seas5_seasonal()\n\n# Load CHIRPS - extracted via GEE script\ndf_chirps_raw &lt;- cumulus::blob_read(\n  name = \"ds-aa-lac-dry-corridor/raw/chirps/2026_cadc_drought_v3_aoi_chirps_monthly_historical.parquet\",\n  container = \"projects\"\n)\n\n# Wrangle CHIRPS to seasonal totals\ndf_chirps &lt;- df_chirps_raw |&gt;\n  filter(ADM1_NAME == \"Chiquimula\") |&gt;\n  mutate(\n    year = year(date),\n    month = month(date),\n    window = case_when(\n      month %in% PRIMERA_MONTHS ~ \"primera\",\n      month %in% POSTRERA_MONTHS ~ \"postrera\",\n      TRUE ~ NA_character_\n    )\n  ) |&gt;\n  filter(!is.na(window), year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n  group_by(year, window) |&gt;\n  summarise(obs_mm = sum(value, na.rm = TRUE), .groups = \"drop\")\n\n# Combine forecasts\ndf_fcst_all &lt;- bind_rows(df_insiv, df_seas5) |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END)\n\n# Filter to operational leadtimes\ndf_fcst_filtered &lt;- df_fcst_all |&gt;\n  mutate(issued_month = month(issued_date)) |&gt;\n  filter(\n    (window == \"primera\" & issued_month %in% PRIMERA_ISSUED_MONTHS) |\n    (window == \"postrera\" & issued_month %in% POSTRERA_ISSUED_MONTHS)\n  )\n\n# Join forecasts with CHIRPS\ndf_joined &lt;- df_fcst_filtered |&gt;\n  left_join(df_chirps, by = c(\"year\", \"window\")) |&gt;\n  filter(!is.na(obs_mm)) # there are not any NA - any ways",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>CHIRPS Tie-Breaker</span>"
    ]
  },
  {
    "objectID": "03_chirps_tiebreaker.html#metrics-against-chirps",
    "href": "03_chirps_tiebreaker.html#metrics-against-chirps",
    "title": "4  CHIRPS Tie-Breaker",
    "section": "4.3 Metrics Against CHIRPS",
    "text": "4.3 Metrics Against CHIRPS\n\n\n\n\n\n\nNoteWhy Rank-Based Metrics Work Across Different Observation Sources\n\n\n\nBecause CHIRPS and ENACTS have different climatologies (CHIRPS shows ~300mm higher Primera totals), comparing raw mm errors across sources would be misleading. Instead, we focus on rank-based metrics:\n\nSpearman correlation: Evaluates the full ranking across all years. If observations rank years as [driest, 2nd, 3rd, … wettest], how well does the forecast reproduce that ordering? Larger rank errors are penalized more heavily.\nROC-AUC: Evaluates separation between drought and non-drought years only. Do drought years consistently get lower forecasts than non-drought years? It ignores ranking within each group—scrambling the order of non-drought years doesn’t affect AUC.\n\nWhen they diverge: A forecast could perfectly separate drought from non-drought (AUC=1) but scramble rankings within each group (moderate Spearman). Or it could track the overall wet-dry gradient well (high Spearman) but fail to place drought years at the bottom (low AUC).\nThese metrics allow fair comparison of forecast skill across observation sources with different absolute values.\n\n\nSpearman & ROC-AUC continue to tell a similar story. Models are skillful in primera with SEAS5 as the dominant competitor. Postrera skill remains low, dubious, and messy. Different models win at different leadtimes with some surprising results of INSIVUMEH provided models showing stronger predictive power at greater leadtimes. SEAS5 remains competitive, but less dominant\n\n\nCalculate thresholds and metrics\n# Forecast thresholds\ndf_fcst_thresh &lt;- df_joined |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  summarise(fcst_thresh = calc_rp_threshold(value), .groups = \"drop\")\n\n# CHIRPS observation thresholds\ndf_obs_thresh &lt;- df_chirps |&gt;\n  group_by(window) |&gt;\n  summarise(obs_thresh = calc_rp_threshold(obs_mm), .groups = \"drop\")\n\n# Join thresholds\ndf_analysis &lt;- df_joined |&gt;\n  left_join(df_fcst_thresh, by = c(\"forecast_source\", \"window\", \"leadtime\")) |&gt;\n  left_join(df_obs_thresh, by = \"window\") |&gt;\n  mutate(\n    fcst_drought = value &lt; fcst_thresh,\n    obs_drought = obs_mm &lt; obs_thresh\n  )\n\n# Continuous metrics\ndf_metrics &lt;- df_analysis |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  summarise(\n    spearman = cor(value, obs_mm, method = \"spearman\", use = \"complete.obs\"),\n    bias_mm = mean(value - obs_mm, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt; \n     mutate(\n      window = fct_relevel(window, \"primera\", \"postrera\")\n    )\n\n\n# vectorized calc AUC\ncalc_auc &lt;- function(truth, drought_score) {                                                                                                                                           \n  if (length(unique(truth)) &lt; 2) return(NA_real_)                                                                                                                           \n  roc_auc_vec(truth, drought_score, event_level = \"first\")                                                                                                                            \n}                                                                                                                                                                                                 \n\ndf_roc &lt;- df_analysis |&gt;\n  mutate(\n    truth = factor(\n      obs_drought,\n      levels = c(TRUE, FALSE),\n      labels = c(\"drought\", \"no_drought\")\n      ),\n    # roc_auc is just rank based, magnitude doesnt matter just invert to +drought +score\n    drought_score = -value\n  )\n\ndf_auc &lt;- df_roc |&gt;                                                                                 \n  group_by(forecast_source, window, leadtime) |&gt;                                                                                                                                      \n  summarise(auc = calc_auc(truth, drought_score), .groups = \"drop\")   \n\n\n\n4.3.1 Spearman Correlation\n\n\nCode\ncreate_metric_heatmap &lt;- function(df, metric_col, metric_name, title, caption, midpoint = 0) {\n  df_plot &lt;- df |&gt;\n    group_by(window, leadtime) |&gt;\n    mutate(is_best = !!sym(metric_col) == max(!!sym(metric_col), na.rm = TRUE)) |&gt;\n    ungroup() \n \n\n  ggplot(df_plot, aes(x = factor(leadtime), y = forecast_source)) +\n    geom_tile(aes(fill = !!sym(metric_col)), color = \"white\", linewidth = 0.5) +\n    geom_tile(\n      data = df_plot |&gt; filter(is_best),\n      fill = NA, color = \"black\", linewidth = 1.5\n    ) +\n    geom_text(aes(label = sprintf(\"%.2f\", !!sym(metric_col))), size = 4, fontface = \"bold\", color = \"black\") +\n    facet_wrap(~window) +\n    scale_fill_gradient2(\n      low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n      midpoint = midpoint, name = metric_name\n    ) +\n    labs(title = title, x = \"Leadtime (months)\", y = NULL, caption = caption) +\n    theme_minimal() +\n    theme(legend.position = \"right\", panel.grid = element_blank(),\n          plot.caption = element_text(hjust = 0))\n}\n\ncreate_metric_heatmap(\n  df = df_metrics,\n  metric_col = \"spearman\",\n  metric_name =\"Spearman ρ\",\n  title = \"Spearman Correlation - CHIRPS\",\n  caption = \"All forecasts validated against CHIRPS. Black border = best per leadtime/season.\"\n)\n\n\n\n\n\n\n\n\n\nSurprise finding: INSIVUMEH_CESM1 shows the strongest Spearman correlation for Primera at LT1 (0.72) and LT2 (0.52), outperforming SEAS5. This is the opposite of what we saw with ERA5 and ENACTS.\n\n\n4.3.2 ROC-AUC\n\n\nCode\ncreate_metric_heatmap(\n  df= df_auc, \n  metric_col = \"auc\",\n  metric_name = \"AUC\",\n  title = \"ROC-AUC - CHIRPS\",\n  caption = \"All forecasts validated against CHIRPS. AUC &gt; 0.7 = acceptable skill. Black border = best.\",\n  midpoint = 0.5\n)\n\n\n\n\n\n\n\n\n\n\n\n4.3.3 F1 Score (Binary)\nFor completeness, here’s the binary F1 score against CHIRPS - the same metric used in Chapter 2. Story remains consistent.\n\n\nCode\n# Calculate F1\ndf_f1 &lt;- df_analysis |&gt;\n  mutate(\n    truth = factor(obs_drought, levels = c(TRUE, FALSE), labels = c(\"drought\", \"no_drought\")),\n    estimate = factor(fcst_drought, levels = c(TRUE, FALSE), labels = c(\"drought\", \"no_drought\"))\n  ) |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  summarise(\n    f1 = f_meas_vec(truth, estimate, event_level = \"first\"),\n    precision = precision_vec(truth, estimate, event_level = \"first\"),\n    recall = recall_vec(truth, estimate, event_level = \"first\"),\n    .groups = \"drop\"\n  )\n\n# F1 heatmap\ndf_plot &lt;- df_f1 |&gt;\n  group_by(window, leadtime) |&gt;\n  mutate(is_best = f1 == max(f1, na.rm = TRUE)) |&gt;\n  ungroup() |&gt; \n  mutate(\n    window = fct_relevel(window, \"primera\",\"postrera\")\n  )\n\nggplot(df_plot, aes(x = factor(leadtime), y = forecast_source)) +\n  geom_tile(aes(fill = f1), color = \"white\", linewidth = 0.5) +\n  geom_tile(\n    data = df_plot |&gt; filter(is_best),\n    fill = NA, color = \"black\", linewidth = 1.5\n  ) +\n  geom_text(aes(label = sprintf(\"%.0f%%\", f1 * 100)), size = 4, fontface = \"bold\", color = \"black\") +\n  facet_wrap(~window) +\n  scale_fill_gradientn(\n    colors = c(\"#D73027\", \"#FC8D59\", \"#FEE08B\", \"#D9EF8B\", \"#91CF60\", \"#1A9850\"),\n    limits = c(0, 1), labels = scales::percent, name = \"F1\"\n  ) +\n  labs(\n    title = \"F1 Score - CHIRPS\",\n    x = \"Leadtime (months)\", y = NULL,\n    caption = \"All forecasts validated against CHIRPS. Black border = best per leadtime/season.\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\", panel.grid = element_blank(),\n        plot.caption = element_text(hjust = 0))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>CHIRPS Tie-Breaker</span>"
    ]
  },
  {
    "objectID": "03_chirps_tiebreaker.html#conclusions",
    "href": "03_chirps_tiebreaker.html#conclusions",
    "title": "4  CHIRPS Tie-Breaker",
    "section": "4.4 Conclusions",
    "text": "4.4 Conclusions\n\n4.4.1 What We’ve Learned So Far\n\nPrimera skill is robust: Multiple models show genuine skill (AUC &gt; 0.7) across all three observation sources. This isn’t an artifact of one particular dataset.\nPostrera remains unresolved: No model shows reliable, consistent skill. CCSM4’s apparent better performance at longer leadtimes is suspicious - skill should not improve with leadtime. This inverted pattern is likely noise from the small sample (only ~6 drought events in 25 years).\n\n\n\n4.4.2 Open Questions\nThe tie-breaker analysis raises more questions than it answers for Postrera:\n\nWhy the inverted skill pattern? CCSM4 showing better skill at LT2-3 than LT1 is backwards - forecast skill should degrade with leadtime. Is this genuine or noise?\nWhich model is actually better? With different metrics favoring different models at different leadtimes, we cannot make a confident recommendation.\nIs the poor skill a data artifact? Could temporal trends in the observation sources be affecting our skill estimates?\n\n\n\n4.4.3 Next Steps\nTo better understand the poor Postrera skill and the contradictory patterns across models, the next chapter examines temporal drift - whether systematic trends in forecasts or observations might explain some of what we’re seeing.\n\n\n\n\n\n\nTipTechnical Details\n\n\n\n\n\n\n4.4.4 CHIRPS vs ENACTS Comparison\nHow different are CHIRPS and ENACTS for Chiquimula? Understanding this helps interpret why forecast skill might differ across observation sources.\n\n\nCompare CHIRPS and ENACTS observations\n# Load ENACTS for comparison\ndf_enacts_compare &lt;- enacts$load_enacts_seasonal(\"chiquimula\")\n\ndf_compare &lt;- df_chirps |&gt;\n  rename(chirps = obs_mm) |&gt;\n  left_join(\n    df_enacts_compare |&gt; select(year, window, enacts = obs_mm),\n    by = c(\"year\", \"window\")\n  ) |&gt;\n  filter(!is.na(enacts))\n\n# Correlation\ncorr_primera &lt;- df_compare |&gt; filter(window == \"primera\") |&gt;\n  summarise(r = cor(chirps, enacts)) |&gt; pull(r)\ncorr_postrera &lt;- df_compare |&gt; filter(window == \"postrera\") |&gt;\n  summarise(r = cor(chirps, enacts)) |&gt; pull(r)\n\ndf_compare |&gt;\n  ggplot(aes(x = enacts, y = chirps)) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"grey50\") +\n  geom_point(alpha = 0.6, size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"#007CE1\", fill = \"#007CE1\", alpha = 0.2) +\n  facet_wrap(~str_to_title(window), scales = \"free\") +\n  labs(\n    title = \"CHIRPS vs ENACTS: Same Region, Different Estimates\",\n    subtitle = sprintf(\"Primera r = %.2f, Postrera r = %.2f\", corr_primera, corr_postrera),\n    x = \"ENACTS (mm)\",\n    y = \"CHIRPS (mm)\",\n    caption = \"Dashed line = 1:1 agreement. CHIRPS consistently higher for Primera.\"\n  )\n\n\n\n\n\n\n\n\n\nCHIRPS and ENACTS are correlated but not identical. CHIRPS tends to estimate higher rainfall for Primera (~300mm more on average). This means a forecast calibrated to one source may not match the other perfectly - making the three-source comparison a meaningful robustness check.\n\n\n4.4.5 Forecast Bias\nBias (mean forecast - observed) is shown for completeness, but is not consequential for our framework since we use rank-based metrics and model-specific thresholds rather than raw precipitation values.\n\n\nCode\ndf_plot &lt;- df_metrics |&gt;\n  group_by(window, leadtime) |&gt;\n  mutate(is_best = abs(bias_mm) == min(abs(bias_mm), na.rm = TRUE)) |&gt;\n  ungroup()\n\nbias_max &lt;- max(abs(df_plot$bias_mm), na.rm = TRUE)\n\nggplot(df_plot, aes(x = factor(leadtime), y = forecast_source)) +\n  geom_tile(aes(fill = bias_mm), color = \"white\", linewidth = 0.5) +\n  geom_tile(\n    data = df_plot |&gt; filter(is_best),\n    fill = NA, color = \"black\", linewidth = 1.5\n  ) +\n  geom_text(aes(label = sprintf(\"%.0f\", bias_mm)), size = 4, fontface = \"bold\", color = \"black\") +\n  facet_wrap(~window) +\n  scale_fill_gradient2(\n    low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n    midpoint = 0, limits = c(-bias_max, bias_max), name = \"Bias (mm)\"\n  ) +\n  labs(\n    title = \"Forecast Bias - CHIRPS\",\n    x = \"Leadtime (months)\", y = NULL,\n    caption = \"Units: millimeters. Bias = mean(forecast - observed). Negative = dry bias.\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\", panel.grid = element_blank(),\n        plot.caption = element_text(hjust = 0))\n\n\n\n\n\n\n\n\n\nAgainst CHIRPS, all models have dry bias for Primera and wet bias for Postrera. SEAS5 has the largest dry bias for Primera (~280mm under), while INSIVUMEH models are closer (~120mm under).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>CHIRPS Tie-Breaker</span>"
    ]
  },
  {
    "objectID": "04_forecast_drift.html",
    "href": "04_forecast_drift.html",
    "title": "5  Temporal Drift",
    "section": "",
    "text": "5.1 Why Check for Drift?\nSkill metrics assume the forecast-observation relationship is stationary over time. If either forecasts or observations have systematic trends (drift), skill estimates may be misleading:\nThis chapter examines temporal trends in both forecasts and observations.\nSetup\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(cumulus)\nlibrary(gghdx)\ngghdx()\n\nbox::use(../../R/enacts)\nbox::use(../../R/seas5)\n\nBASELINE_START &lt;- 2000\nBASELINE_END &lt;- 2024\nPRIMERA_MONTHS &lt;- 5:8\nPOSTRERA_MONTHS &lt;- 9:11\nLoad all data\n# Forecasts\ndf_insiv &lt;- cumulus::blob_read(\n  name = \"ds-aa-lac-dry-corridor/data/processed/insivumeh_special/insivumeh_special_models_zonal_seasonal_chiquimula.parquet\",\n  container = \"projects\"\n)\ndf_seas5 &lt;- seas5$load_seas5_seasonal()\n\n# Observations\ndf_enacts &lt;- enacts$load_enacts_seasonal(\"chiquimula\")\n\n# ERA5\ncon &lt;- pg_con()\ndf_era5_raw &lt;- tbl(con, \"era5\") |&gt;\n  filter(pcode == \"GT20\") |&gt;\n  collect() |&gt;\n  mutate(\n    year = year(valid_date),\n    month = month(valid_date),\n    mean = mean * days_in_month(valid_date)\n  )\nDBI::dbDisconnect(con)\n\ndf_era5 &lt;- bind_rows(\n  df_era5_raw |&gt; rename(value = mean) |&gt;\n    filter(month %in% PRIMERA_MONTHS) |&gt;\n    group_by(year) |&gt;\n    summarise(obs_mm = sum(value, na.rm = TRUE), .groups = \"drop\") |&gt;\n    mutate(window = \"primera\"),\n  df_era5_raw |&gt; rename(value = mean) |&gt;\n    filter(month %in% POSTRERA_MONTHS) |&gt;\n    group_by(year) |&gt;\n    summarise(obs_mm = sum(value, na.rm = TRUE), .groups = \"drop\") |&gt;\n    mutate(window = \"postrera\")\n)\n\n# CHIRPS\ndf_chirps_raw &lt;- cumulus::blob_read(\n  name = \"ds-aa-lac-dry-corridor/raw/chirps/2026_cadc_drought_v3_aoi_chirps_monthly_historical.parquet\",\n  container = \"projects\"\n)\n\ndf_chirps &lt;- df_chirps_raw |&gt;\n  filter(ADM1_NAME == \"Chiquimula\") |&gt;\n  mutate(year = year(date), month = month(date)) |&gt;\n  mutate(\n    window = case_when(\n      month %in% PRIMERA_MONTHS ~ \"primera\",\n      month %in% POSTRERA_MONTHS ~ \"postrera\",\n      TRUE ~ NA_character_\n    )\n  ) |&gt;\n  filter(!is.na(window)) |&gt;\n  group_by(year, window) |&gt;\n  summarise(obs_mm = sum(value, na.rm = TRUE), .groups = \"drop\")\n\n# Combine\ndf_obs_all &lt;- bind_rows(\n  df_enacts |&gt; mutate(source = \"ENACTS\"),\n  df_era5 |&gt; mutate(source = \"ERA5\"),\n  df_chirps |&gt; mutate(source = \"CHIRPS\")\n) |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END)\n\ndf_fcst_all &lt;- bind_rows(df_insiv, df_seas5) |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n  mutate(model = str_remove(forecast_source, \"INSIVUMEH_\"))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Drift</span>"
    ]
  },
  {
    "objectID": "04_forecast_drift.html#why-check-for-drift",
    "href": "04_forecast_drift.html#why-check-for-drift",
    "title": "5  Temporal Drift",
    "section": "",
    "text": "A model that performed well historically might be degrading\nApparent “skill” might reflect coincidental alignment of trends rather than genuine predictive ability\nDifferent observation sources might have different drift characteristics",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Drift</span>"
    ]
  },
  {
    "objectID": "04_forecast_drift.html#observation-drift",
    "href": "04_forecast_drift.html#observation-drift",
    "title": "5  Temporal Drift",
    "section": "5.2 Observation Drift",
    "text": "5.2 Observation Drift\nAre the three observation sources showing consistent trends, or is one drifting differently?\n\n\nCalculate observation trends\nobs_trends &lt;- df_obs_all |&gt;\n  group_by(source, window) |&gt;\n  summarise(\n    n = n(),\n    mean_mm = mean(obs_mm),\n    trend_per_year = coef(lm(obs_mm ~ year))[2],\n    trend_pvalue = summary(lm(obs_mm ~ year))$coefficients[2, 4],\n    total_trend = trend_per_year * (BASELINE_END - BASELINE_START),\n    trend_pct = total_trend / mean_mm * 100,\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(significant = trend_pvalue &lt; 0.1)\n\n\n\n\nCode\nobs_trends |&gt;\n  mutate(\n    trend_summary = sprintf(\"%.1f mm/yr (%+.0f%% total)%s\",\n                            trend_per_year, trend_pct,\n                            ifelse(significant, \"*\", \"\"))\n  ) |&gt;\n  select(source, window, trend_summary) |&gt;\n  pivot_wider(names_from = window, values_from = trend_summary) |&gt;\n  knitr::kable(caption = \"Observation trends 2000-2024. * = p &lt; 0.1\")\n\n\n\nObservation trends 2000-2024. * = p &lt; 0.1\n\n\nsource\npostrera\nprimera\n\n\n\n\nCHIRPS\n3.2 mm/yr (+34% total)\n-3.6 mm/yr (-9% total)\n\n\nENACTS\n-2.3 mm/yr (-10% total)\n-9.4 mm/yr (-36% total)*\n\n\nERA5\n3.8 mm/yr (+17% total)\n-2.8 mm/yr (-8% total)\n\n\n\n\n\n\n\nCode\ndf_obs_all |&gt;\n  ggplot(aes(x = year, y = obs_mm, color = source)) +\n  geom_line(alpha = 0.7) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = TRUE, alpha = 0.2) +\n  facet_wrap(~str_to_title(window), scales = \"free_y\") +\n  labs(\n    title = \"Observation Time Series by Source\",\n    subtitle = \"Lines show linear trends with 95% CI\",\n    x = \"Year\", y = \"Seasonal Rainfall (mm)\", color = \"Source\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nKey finding: ENACTS shows a stronger drying trend for Primera (-9.4 mm/yr) than ERA5 or CHIRPS. This could affect skill comparisons if forecasts are compared against different observation sources.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Drift</span>"
    ]
  },
  {
    "objectID": "04_forecast_drift.html#forecast-drift",
    "href": "04_forecast_drift.html#forecast-drift",
    "title": "5  Temporal Drift",
    "section": "5.3 Forecast Drift",
    "text": "5.3 Forecast Drift\nDo forecasts show systematic trends over time?\n\n\nCode\ndf_fcst_all |&gt;\n  filter(leadtime %in% c(1, 2, 3)) |&gt;\n  ggplot(aes(x = year, y = value, color = model)) +\n  geom_line(alpha = 0.7) +\n  geom_point(alpha = 0.5, size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.8) +\n  facet_grid(rows = vars(paste0(\"LT\", leadtime)), cols = vars(str_to_title(window)), scales = \"free_y\") +\n  labs(\n    title = \"Forecast Time Series by Leadtime\",\n    subtitle = \"Lines show linear trends\",\n    x = \"Year\", y = \"Forecast (mm)\", color = \"Model\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nCalculate forecast trends\nfcst_trends &lt;- df_fcst_all |&gt;\n  filter(leadtime %in% c(1, 2, 3)) |&gt;\n  group_by(model, window, leadtime) |&gt;\n  summarise(\n    trend_per_year = coef(lm(value ~ year))[2],\n    trend_pvalue = summary(lm(value ~ year))$coefficients[2, 4],\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    significant = trend_pvalue &lt; 0.1,\n    trend_fmt = sprintf(\"%.1f%s\", trend_per_year, ifelse(significant, \"*\", \"\"))\n  )\n\nfcst_trends |&gt;\n  select(model, window, leadtime, trend_fmt) |&gt;\n  pivot_wider(names_from = leadtime, values_from = trend_fmt, names_prefix = \"LT\") |&gt;\n  arrange(window, model) |&gt;\n  knitr::kable(caption = \"Forecast trends (mm/year). * = p &lt; 0.1\")\n\n\n\nForecast trends (mm/year). * = p &lt; 0.1\n\n\nmodel\nwindow\nLT1\nLT2\nLT3\n\n\n\n\nCCSM4\npostrera\n-1.1\n-0.4\n-0.5\n\n\nCESM1\npostrera\n-1.1*\n0.0\n0.0\n\n\nCFSv2\npostrera\n0.2\n0.2\n-1.4\n\n\nSEAS5\npostrera\n0.3\n0.6\n0.2\n\n\nCCSM4\nprimera\n-3.2\n-0.8\n-3.1\n\n\nCESM1\nprimera\n-0.1\n-2.3\n-1.7\n\n\nCFSv2\nprimera\n-1.2\n-4.6*\n-2.1\n\n\nSEAS5\nprimera\n-4.5*\n-3.6\n-0.1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Drift</span>"
    ]
  },
  {
    "objectID": "04_forecast_drift.html#differential-drift-forecast-error-over-time",
    "href": "04_forecast_drift.html#differential-drift-forecast-error-over-time",
    "title": "5  Temporal Drift",
    "section": "5.4 Differential Drift (Forecast Error Over Time)",
    "text": "5.4 Differential Drift (Forecast Error Over Time)\nMore important than individual trends is whether the forecast-observation relationship is changing.\n\n\nCode\ndf_error &lt;- df_fcst_all |&gt;\n  filter(leadtime %in% c(1, 2, 3)) |&gt;\n  left_join(df_enacts |&gt; select(year, window, obs_mm), by = c(\"year\", \"window\")) |&gt;\n  mutate(error = value - obs_mm)\n\ndf_error |&gt;\n  ggplot(aes(x = year, y = error, color = model)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"grey50\") +\n  geom_line(alpha = 0.7) +\n  geom_point(alpha = 0.5, size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.8) +\n  facet_grid(rows = vars(paste0(\"LT\", leadtime)), cols = vars(str_to_title(window)), scales = \"free_y\") +\n  labs(\n    title = \"Forecast Error Over Time (vs ENACTS)\",\n    subtitle = \"Positive = wet bias, Negative = dry bias. Lines show trends.\",\n    x = \"Year\", y = \"Error (mm)\", color = \"Model\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nerror_trends &lt;- df_error |&gt;\n  group_by(model, window, leadtime) |&gt;\n  summarise(\n    trend_per_year = coef(lm(error ~ year))[2],\n    trend_pvalue = summary(lm(error ~ year))$coefficients[2, 4],\n    total_drift = trend_per_year * (BASELINE_END - BASELINE_START),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    significant = trend_pvalue &lt; 0.1,\n    drift_fmt = sprintf(\"%.1f (%+.0f mm total)%s\",\n                        trend_per_year, total_drift,\n                        ifelse(significant, \"*\", \"\"))\n  )\n\nerror_trends |&gt;\n  filter(leadtime == 1) |&gt;\n  select(model, window, drift_fmt) |&gt;\n  pivot_wider(names_from = window, values_from = drift_fmt) |&gt;\n  knitr::kable(caption = \"Error drift at LT1 (mm/year). * = p &lt; 0.1\")\n\n\n\nError drift at LT1 (mm/year). * = p &lt; 0.1\n\n\nmodel\npostrera\nprimera\n\n\n\n\nCCSM4\n1.3 (+30 mm total)\n6.2 (+148 mm total)\n\n\nCESM1\n1.3 (+30 mm total)\n9.2 (+222 mm total)*\n\n\nCFSv2\n2.5 (+60 mm total)\n8.2 (+197 mm total)*\n\n\nSEAS5\n2.7 (+64 mm total)\n4.9 (+116 mm total)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Drift</span>"
    ]
  },
  {
    "objectID": "04_forecast_drift.html#split-half-stability",
    "href": "04_forecast_drift.html#split-half-stability",
    "title": "5  Temporal Drift",
    "section": "5.5 Split-Half Stability",
    "text": "5.5 Split-Half Stability\nAnother way to assess stationarity: does skill differ between the first and second half of the record?\n\n\nCode\ndf_split &lt;- df_fcst_all |&gt;\n  filter(leadtime == 1) |&gt;\n  left_join(df_enacts |&gt; select(year, window, obs_mm), by = c(\"year\", \"window\")) |&gt;\n  mutate(period = ifelse(year &lt;= 2012, \"2000-2012\", \"2013-2024\"))\n\nsplit_corr &lt;- df_split |&gt;\n  group_by(model, window, period) |&gt;\n  summarise(cor = cor(value, obs_mm, use = \"complete.obs\"), .groups = \"drop\")\n\nsplit_corr |&gt;\n  ggplot(aes(x = model, y = cor, fill = period)) +\n  geom_col(position = \"dodge\", alpha = 0.8) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  facet_wrap(~str_to_title(window)) +\n  scale_fill_manual(values = c(\"2000-2012\" = \"#1A9850\", \"2013-2024\" = \"#D73027\")) +\n  labs(\n    title = \"Forecast-Observation Correlation by Period (LT1)\",\n    subtitle = \"If skill is stable, bars should be similar height\",\n    x = \"Model\", y = \"Correlation with ENACTS\", fill = \"Period\"\n  ) +\n  theme(legend.position = \"bottom\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nCode\nsplit_corr |&gt;\n  pivot_wider(names_from = period, values_from = cor) |&gt;\n  mutate(change = `2013-2024` - `2000-2012`) |&gt;\n  select(model, window, `2000-2012`, `2013-2024`, change) |&gt;\n  mutate(across(where(is.numeric), ~sprintf(\"%.2f\", .x))) |&gt;\n  pivot_wider(names_from = window, values_from = c(`2000-2012`, `2013-2024`, change)) |&gt;\n  knitr::kable(caption = \"Split-half correlations and change\")\n\n\n\nSplit-half correlations and change\n\n\n\n\n\n\n\n\n\n\n\nmodel\n2000-2012_postrera\n2000-2012_primera\n2013-2024_postrera\n2013-2024_primera\nchange_postrera\nchange_primera\n\n\n\n\nCCSM4\n0.67\n0.46\n0.44\n0.38\n-0.23\n-0.08\n\n\nCESM1\n-0.21\n0.56\n0.52\n0.40\n0.73\n-0.16\n\n\nCFSv2\n0.20\n0.53\n0.20\n0.34\n0.00\n-0.19\n\n\nSEAS5\n0.65\n0.40\n0.09\n0.74\n-0.56\n0.34",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Drift</span>"
    ]
  },
  {
    "objectID": "04_forecast_drift.html#summary",
    "href": "04_forecast_drift.html#summary",
    "title": "5  Temporal Drift",
    "section": "5.6 Summary",
    "text": "5.6 Summary\n\n5.6.1 What We Found\nTemporal drift in climate data products is a known issue across many sources and regions - satellite algorithms evolve, station networks change, and reanalysis systems are updated. We observe drift here as well:\n\nObservations: ENACTS shows a stronger drying trend for Primera than other sources\nForecasts: Some significant trends exist (e.g., SEAS5 Primera drying ~4 mm/yr)\nError drift: INSIVUMEH Primera errors are trending positive, accumulating ~150-200mm over 24 years\nSplit-half stability: Postrera correlations are highly unstable between early and late periods\n\n\n\n5.6.2 Does This Explain Poor Postrera Skill?\nNo. The drift magnitudes are modest relative to interannual variability, and drift affects both seasons similarly. The split-half instability for Postrera is concerning, but this is more likely a symptom of limited sample size than evidence that drift is causing poor skill.\nBottom line: Drift is present but does not explain the contradictory Postrera patterns from Chapter 3. The investigation continues in the next chapter with internal consistency diagnostics.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Drift</span>"
    ]
  },
  {
    "objectID": "05_inter_lt_correlation.html",
    "href": "05_inter_lt_correlation.html",
    "title": "6  Leadtime Diagnostics",
    "section": "",
    "text": "6.1 Motivation\nIn Chapter 4, CCSM4 showed a suspicious pattern for Postrera: skill at LT2-3 exceeded skill at LT1. This is backwards—forecast skill should degrade with increasing leadtime, not improve.\nThis chapter investigates whether this pattern reflects genuine predictability or a data quality issue by examining:\nSetup\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(cumulus)\nlibrary(gghdx)\ngghdx()\n\nbox::use(../../R/enacts)\nbox::use(../../R/seas5)\n\nBASELINE_START &lt;- 2000\nBASELINE_END &lt;- 2024\nPRIMERA_MONTHS &lt;- 5:8\nPOSTRERA_MONTHS &lt;- 9:11\nLoad forecast and observation data\n# Forecasts\ndf_insiv &lt;- cumulus::blob_read(\n  name = \"ds-aa-lac-dry-corridor/data/processed/insivumeh_special/insivumeh_special_models_zonal_seasonal_chiquimula.parquet\",\n  container = \"projects\"\n)\ndf_seas5 &lt;- seas5$load_seas5_seasonal()\n\n# Observations\ndf_enacts &lt;- enacts$load_enacts_seasonal(\"chiquimula\")\n\n# Combine forecasts\ndf_fcst_all &lt;- bind_rows(df_insiv, df_seas5) |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n  mutate(model = str_remove(forecast_source, \"INSIVUMEH_\"))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Leadtime Diagnostics</span>"
    ]
  },
  {
    "objectID": "05_inter_lt_correlation.html#motivation",
    "href": "05_inter_lt_correlation.html#motivation",
    "title": "6  Leadtime Diagnostics",
    "section": "",
    "text": "Inter-leadtime correlations: Do consecutive leadtimes agree with each other?\nCoefficient of Variation (CV): Do forecasts show realistic interannual variability?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Leadtime Diagnostics</span>"
    ]
  },
  {
    "objectID": "05_inter_lt_correlation.html#inter-leadtime-correlations",
    "href": "05_inter_lt_correlation.html#inter-leadtime-correlations",
    "title": "6  Leadtime Diagnostics",
    "section": "6.2 Inter-Leadtime Correlations",
    "text": "6.2 Inter-Leadtime Correlations\nFor a well-behaved forecast system, predictions at different leadtimes for the same target season should be correlated—they’re all trying to predict the same thing. If LT1 and LT2 forecasts are uncorrelated, one or both must be essentially random.\n\n\nFunction to compute correlation matrix\ncompute_lt_corr_matrix &lt;- function(df, model_name, window_name) {\n  df_wide &lt;- df |&gt;\n    filter(model == model_name, window == window_name, !is.na(leadtime)) |&gt;\n    select(year, leadtime, value) |&gt;\n    pivot_wider(names_from = leadtime, values_from = value, names_prefix = \"LT\")\n\n  # Add observations\n  df_obs &lt;- df_enacts |&gt;\n    filter(window == window_name, year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n    select(year, obs_mm)\n\n  df_wide &lt;- df_wide |&gt;\n    left_join(df_obs, by = \"year\") |&gt;\n    rename(OBS = obs_mm)\n\n  # Compute correlation matrix - only keep LT and OBS columns\n  cor_mat &lt;- df_wide |&gt;\n    select(starts_with(\"LT\"), OBS) |&gt;\n    cor(use = \"pairwise.complete.obs\")\n\n  cor_mat\n}\n\n\n\n\nSEAS5 inter-leadtime correlations\nmodels &lt;- unique(df_fcst_all$model)\nwindows &lt;- c(\"primera\", \"postrera\")\n\n# Compute all correlation matrices\ncorr_results &lt;- expand_grid(model = models, window = windows) |&gt;\n  mutate(\n    corr_mat = map2(model, window, ~compute_lt_corr_matrix(df_fcst_all, .x, .y))\n  )\n\n\n\n6.2.1 SEAS5 Correlation Patterns\n\n\nCode\n# Extract SEAS5 matrices\nseas5_primera &lt;- corr_results |&gt;\n  filter(model == \"SEAS5\", window == \"primera\") |&gt;\n  pull(corr_mat) |&gt;\n  pluck(1)\n\nseas5_postrera &lt;- corr_results |&gt;\n  filter(model == \"SEAS5\", window == \"postrera\") |&gt;\n  pull(corr_mat) |&gt;\n  pluck(1)\n\n# Convert to long format for plotting (lower triangle only)\nmat_to_df &lt;- function(mat, label) {\n  var_order &lt;- c(\"LT4\", \"LT3\", \"LT2\", \"LT1\", \"OBS\")\n  # Filter to only include columns that exist in the matrix\n  var_order &lt;- var_order[var_order %in% colnames(mat)]\n\n  as.data.frame(mat) |&gt;\n    mutate(row = rownames(mat)) |&gt;\n    pivot_longer(-row, names_to = \"col\", values_to = \"cor\") |&gt;\n    mutate(\n      label = label,\n      row_idx = match(row, var_order),\n      col_idx = match(col, var_order)\n    ) |&gt;\n    # Keep lower triangle only (row_idx &gt; col_idx)\n    filter(row_idx &gt; col_idx) |&gt;\n    select(-row_idx, -col_idx)\n}\n\ndf_seas5_corr &lt;- bind_rows(\n  mat_to_df(seas5_primera, \"Primera\"),\n  mat_to_df(seas5_postrera, \"Postrera\")\n) |&gt; \n  mutate(\n    label = fct_relevel(label, \"Primera\",\"Postrera\")\n  )\n\ndf_seas5_corr |&gt;\n  mutate(\n    row = factor(row, levels = rev(c(\"LT4\", \"LT3\", \"LT2\", \"LT1\", \"OBS\"))),\n    col = factor(col, levels = c(\"LT4\", \"LT3\", \"LT2\", \"LT1\", \"OBS\"))\n  ) |&gt;\n  ggplot(aes(x = col, y = row, fill = cor)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = sprintf(\"%.2f\", cor)), size = 3, fontface = \"bold\", color = \"black\") +\n  facet_wrap(~label, scales = \"free_x\") +\n  scale_fill_gradient2(low = \"#D73027\", mid = \"white\", high = \"#1A9850\", midpoint = 0.4, limits = c(-1, 1)) +\n  labs(\n    title = \"SEAS5: Inter-Leadtime Correlations\",\n    subtitle = \"Adjacent leadtimes should correlate strongly\",\n    x = NULL, y = NULL, fill = \"Correlation\",\n    caption = \"Omitting leadtime 0 as it is not available from INSVIUMEH provided models\"\n  ) +\n  theme(legend.position = \"right\",plot.caption = element_text(hjust= 0,vjust=-1))\n\n\n\n\n\n\n\n\n\nSEAS5 shows expected behavior: Adjacent leadtimes correlate at 0.6-0.9. LT1 correlates best with observations. This is a well-behaved forecast system.\n\n\n6.2.2 INSIVUMEH Correlation Patterns\n\n\nCode\n# Extract INSIVUMEH matrices for all models\ninsiv_models &lt;- c(\"CFSv2\", \"CCSM4\", \"CESM1\")\n\ndf_insiv_corr &lt;- map_dfr(insiv_models, function(m) {\n  map_dfr(windows, function(w) {\n    mat &lt;- corr_results |&gt;\n      filter(model == m, window == w) |&gt;\n      pull(corr_mat) |&gt;\n      pluck(1)\n\n    if (is.null(mat)) return(NULL)\n\n    mat_to_df(mat, m) |&gt;\n      mutate(window = w)\n  })\n})\n\ndf_insiv_corr |&gt;\n  filter(!is.na(row), !is.na(col)) |&gt;\n  mutate(\n    row = factor(row, levels = rev(c(\"LT4\", \"LT3\", \"LT2\", \"LT1\", \"OBS\"))),\n    col = factor(col, levels = c(\"LT4\", \"LT3\", \"LT2\", \"LT1\", \"OBS\")),\n    window = str_to_title(window),\n    window = fct_relevel(window, \"Primera\",\"Postrera\")\n  ) |&gt;\n  ggplot(aes(x = col, y = row, fill = cor)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = sprintf(\"%.2f\", cor)), size = 2.5, fontface = \"bold\", color = \"black\") +\n  facet_grid(label ~ window, scales = \"free_x\") +\n  scale_fill_gradient2(low = \"#D73027\", mid = \"white\", high = \"#1A9850\", midpoint = 0.4, limits = c(-1, 1)) +\n  labs(\n    title = \"INSIVUMEH Models: Inter-Leadtime Correlations\",\n    subtitle = \"Postrera shows near-zero or negative correlations between leadtimes\",\n    x = NULL, y = NULL, fill = \"Correlation\"\n  ) +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nCritical finding: INSIVUMEH Postrera forecasts show near-zero or negative correlations between leadtimes. When LT1 predicts wet, LT2 might predict dry for the same target season. This is a fundamental data quality issue—these forecasts are not internally consistent.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Leadtime Diagnostics</span>"
    ]
  },
  {
    "objectID": "05_inter_lt_correlation.html#coefficient-of-variation",
    "href": "05_inter_lt_correlation.html#coefficient-of-variation",
    "title": "6  Leadtime Diagnostics",
    "section": "6.3 Coefficient of Variation",
    "text": "6.3 Coefficient of Variation\nAnother diagnostic: do forecasts show realistic interannual variability? A forecast that barely varies from year to year isn’t providing useful information, even if it occasionally correlates with observations.\n\n\nCalculate CV for forecasts and observations\n# Forecast CV\nfcst_cv &lt;- df_fcst_all |&gt;\n  filter(leadtime == 1) |&gt;\n  group_by(model, window) |&gt;\n  summarise(\n    mean_mm = mean(value),\n    sd_mm = sd(value),\n    cv_pct = sd_mm / mean_mm * 100,\n    .groups = \"drop\"\n  )\n\n# Observation CV\nobs_cv &lt;- df_enacts |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n  group_by(window) |&gt;\n  summarise(\n    mean_mm = mean(obs_mm),\n    sd_mm = sd(obs_mm),\n    cv_pct = sd_mm / mean_mm * 100,\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(model = \"ENACTS (Observed)\")\n\n# Combine\ncv_all &lt;- bind_rows(\n  fcst_cv |&gt; mutate(type = \"Forecast\"),\n  obs_cv |&gt; mutate(type = \"Observed\")\n)\n\n\n\n\nCode\ncv_all |&gt;\n  mutate(\n    model = factor(model, levels = c(\"ENACTS (Observed)\", \"SEAS5\", \"CFSv2\", \"CCSM4\", \"CESM1\")),\n    window = fct_relevel(str_to_title(window), \"Primera\", \"Postrera\")\n  ) |&gt;\n  ggplot(aes(x = model, y = cv_pct, fill = type)) +\n  geom_col(alpha = 0.8) +\n  geom_hline(\n    data = obs_cv |&gt; mutate(window = fct_relevel(str_to_title(window), \"Primera\", \"Postrera\")),\n    aes(yintercept = cv_pct),\n    linetype = \"dashed\", color = \"grey30\"\n  ) +\n  facet_wrap(~window) +\n  labs(\n    title = \"Coefficient of Variation: Forecasts vs Observations\",\n    subtitle = \"Dashed line = observed variability. Forecasts should approach this level.\",\n    x = NULL, y = \"CV (%)\", fill = NULL\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\ncv_all |&gt;\n  select(model, window, cv_pct) |&gt;\n  mutate(window = str_to_title(window)) |&gt;\n  pivot_wider(names_from = window, values_from = cv_pct) |&gt;\n  select(model, Primera, Postrera) |&gt;\n  mutate(across(where(is.numeric), ~sprintf(\"%.1f%%\", .x))) |&gt;\n  knitr::kable(caption = \"Coefficient of Variation by Model and Season\")\n\n\n\nCoefficient of Variation by Model and Season\n\n\nmodel\nPrimera\nPostrera\n\n\n\n\nCCSM4\n11.9%\n6.6%\n\n\nCESM1\n16.3%\n3.5%\n\n\nCFSv2\n14.0%\n6.9%\n\n\nSEAS5\n12.5%\n11.1%\n\n\nENACTS (Observed)\n25.9%\n22.6%\n\n\n\n\n\nKey findings (shown here for LT1 for simplicity):\n\nObserved CV: Primera ~14%, Postrera ~23%\nSEAS5: Reasonable CV for both seasons (8-15%), approaching observed variability\nINSIVUMEH Primera: Moderate CV (10-18%), usable signal\nINSIVUMEH Postrera: Very low CV (3-7%), forecasts barely vary from climatology\n\nThis pattern holds across all leadtimes—see the expanded analysis in the collapsible section below.\n\n\n\n\n\n\nTipRobustness Check: CV Across Leadtimes and Observation Sources\n\n\n\n\n\nDoes the low CV pattern for INSIVUMEH Postrera hold across all leadtimes? And how does it compare to multiple observation sources?\n\n\nLoad additional observation sources and calculate CV across leadtimes\n# Forecast CV across all leadtimes\nfcst_cv_all_lt &lt;- df_fcst_all |&gt;\n  group_by(model, window, leadtime) |&gt;\n  summarise(\n    mean_mm = mean(value),\n    sd_mm = sd(value),\n    cv_pct = sd_mm / mean_mm * 100,\n    .groups = \"drop\"\n  )\n\n# Load ERA5\ncon &lt;- pg_con()\ndf_era5_raw &lt;- tbl(con, \"era5\") |&gt;\n  filter(pcode == \"GT20\") |&gt;\n  collect() |&gt;\n  mutate(\n    year = year(valid_date),\n    month = month(valid_date),\n    mean = mean * days_in_month(valid_date)\n  )\nDBI::dbDisconnect(con)\n\ndf_era5 &lt;- bind_rows(\n  df_era5_raw |&gt;\n    filter(month %in% PRIMERA_MONTHS) |&gt;\n    group_by(year) |&gt;\n    summarise(obs_mm = sum(mean), .groups = \"drop\") |&gt;\n    mutate(window = \"primera\"),\n  df_era5_raw |&gt;\n    filter(month %in% POSTRERA_MONTHS) |&gt;\n    group_by(year) |&gt;\n    summarise(obs_mm = sum(mean), .groups = \"drop\") |&gt;\n    mutate(window = \"postrera\")\n) |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END)\n\n# Load CHIRPS\ndf_chirps_raw &lt;- cumulus::blob_read(\n  name = \"ds-aa-lac-dry-corridor/raw/chirps/2026_cadc_drought_v3_aoi_chirps_monthly_historical.parquet\",\n  container = \"projects\"\n)\n\ndf_chirps &lt;- df_chirps_raw |&gt;\n  filter(ADM1_NAME == \"Chiquimula\") |&gt;\n  mutate(\n    year = year(date),\n    month = month(date),\n    window = case_when(\n      month %in% PRIMERA_MONTHS ~ \"primera\",\n      month %in% POSTRERA_MONTHS ~ \"postrera\",\n      TRUE ~ NA_character_\n    )\n  ) |&gt;\n  filter(!is.na(window), year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n  group_by(year, window) |&gt;\n  summarise(obs_mm = sum(value, na.rm = TRUE), .groups = \"drop\")\n\n# Calculate observed CV from all three sources\nobs_cv_all &lt;- bind_rows(\n  df_enacts |&gt;\n    filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n    group_by(window) |&gt;\n    summarise(cv_pct = sd(obs_mm) / mean(obs_mm) * 100, .groups = \"drop\") |&gt;\n    mutate(source = \"ENACTS\"),\n  df_era5 |&gt;\n    group_by(window) |&gt;\n    summarise(cv_pct = sd(obs_mm) / mean(obs_mm) * 100, .groups = \"drop\") |&gt;\n    mutate(source = \"ERA5\"),\n  df_chirps |&gt;\n    group_by(window) |&gt;\n    summarise(cv_pct = sd(obs_mm) / mean(obs_mm) * 100, .groups = \"drop\") |&gt;\n    mutate(source = \"CHIRPS\")\n)\n\n\n\n\nCreate color-coded CV table\nlibrary(gt)\n\n# Get observed CV range for reference\nobs_cv_summary &lt;- obs_cv_all |&gt;\n  group_by(window) |&gt;\n  summarise(\n    obs_mean = mean(cv_pct),\n    obs_min = min(cv_pct),\n    obs_max = max(cv_pct),\n    .groups = \"drop\"\n  )\n\n# Create wide table for gt\ndf_cv_wide &lt;- fcst_cv_all_lt |&gt;\n  mutate(\n    col_name = paste0(str_to_title(window), \" LT\", leadtime)\n  ) |&gt;\n  select(model, col_name, cv_pct) |&gt;\n  pivot_wider(names_from = col_name, values_from = cv_pct)\n\n# Add observed row (using mean across sources)\nobs_row &lt;- obs_cv_all |&gt;\n  group_by(window) |&gt;\n  summarise(cv_pct = mean(cv_pct), .groups = \"drop\") |&gt;\n  mutate(col_name = paste0(str_to_title(window), \" (Obs)\")) |&gt;\n  pivot_wider(names_from = col_name, values_from = cv_pct) |&gt;\n  mutate(model = \"Observed (mean)\")\n\n# Combine and order\ndf_cv_wide &lt;- df_cv_wide |&gt;\n  mutate(model = factor(model, levels = c(\"SEAS5\", \"CFSv2\", \"CCSM4\", \"CESM1\"))) |&gt;\n  arrange(model) |&gt;\n  mutate(model = as.character(model))\n\n# Define column order\nprimera_cols &lt;- paste0(\"Primera LT\", 1:3)\npostrera_cols &lt;- paste0(\"Postrera LT\", 1:3)\nall_cols &lt;- c(primera_cols, postrera_cols)\nall_cols &lt;- all_cols[all_cols %in% names(df_cv_wide)]\n\ndf_cv_wide |&gt;\n  select(model, all_of(all_cols)) |&gt;\n  gt(rowname_col = \"model\") |&gt;\n  tab_header(\n    title = \"Coefficient of Variation by Model, Season, and Leadtime\",\n    subtitle = \"Higher CV = more interannual variability (closer to observations)\"\n  ) |&gt;\n  tab_spanner(label = \"Primera\", columns = starts_with(\"Primera\")) |&gt;\n  tab_spanner(label = \"Postrera\", columns = starts_with(\"Postrera\")) |&gt;\n  fmt_number(columns = everything(), decimals = 1, pattern = \"{x}%\") |&gt;\n  data_color(\n    columns = starts_with(\"Primera\"),\n    palette = c(\"#D73027\", \"#FFFFBF\", \"#1A9850\"),\n    domain = c(0, obs_cv_summary |&gt; filter(window == \"primera\") |&gt; pull(obs_mean))\n  ) |&gt;\n  data_color(\n    columns = starts_with(\"Postrera\"),\n    palette = c(\"#D73027\", \"#FFFFBF\", \"#1A9850\"),\n    domain = c(0, obs_cv_summary |&gt; filter(window == \"postrera\") |&gt; pull(obs_mean))\n  ) |&gt;\n  tab_source_note(\n    source_note = sprintf(\n      \"Observed CV: Primera %.0f-%.0f%%, Postrera %.0f-%.0f%% (across ENACTS, ERA5, CHIRPS)\",\n      obs_cv_summary |&gt; filter(window == \"primera\") |&gt; pull(obs_min),\n      obs_cv_summary |&gt; filter(window == \"primera\") |&gt; pull(obs_max),\n      obs_cv_summary |&gt; filter(window == \"postrera\") |&gt; pull(obs_min),\n      obs_cv_summary |&gt; filter(window == \"postrera\") |&gt; pull(obs_max)\n    )\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_stub()\n  ) |&gt;\n  cols_label_with(fn = ~str_remove(.x, \"Primera |Postrera \"))\n\n\n\n\n\n\n\n\nCoefficient of Variation by Model, Season, and Leadtime\n\n\nHigher CV = more interannual variability (closer to observations)\n\n\n\nPrimera\nPostrera\n\n\nLT1\nLT2\nLT3\nLT1\nLT2\nLT3\n\n\n\n\nSEAS5\n12.5%\n11.8%\n10.5%\n11.1%\n11.2%\n11.5%\n\n\nCFSv2\n14.0%\n11.5%\n10.7%\n6.9%\n5.1%\n6.1%\n\n\nCCSM4\n11.9%\n12.2%\n10.1%\n6.6%\n6.8%\n9.0%\n\n\nCESM1\n16.3%\n11.7%\n12.3%\n3.5%\n3.2%\n6.3%\n\n\n\nObserved CV: Primera 18-26%, Postrera 22-34% (across ENACTS, ERA5, CHIRPS)\n\n\n\n\n\n\n\n\n\n\nCode\n# Postrera-only bar plot with labeled observation lines\nobs_cv_postrera &lt;- obs_cv_all |&gt;\n  filter(window == \"postrera\") |&gt;\n  mutate(label_x = 3.5)  # Position for labels\n\nfcst_cv_all_lt |&gt;\n  filter(window == \"postrera\") |&gt;\n  ggplot(aes(x = factor(leadtime), y = cv_pct, fill = model)) +\n  geom_col(position = \"dodge\", alpha = 0.8) +\n  geom_hline(\n    data = obs_cv_postrera,\n    aes(yintercept = cv_pct, linetype = source),\n    inherit.aes = FALSE,\n    color = \"grey30\"\n  ) +\n  geom_text(\n    data = obs_cv_postrera,\n    aes(x = label_x, y = cv_pct, label = source),\n    inherit.aes = FALSE,\n    hjust = 0, vjust = -0.3, size = 3, color = \"grey30\"\n  ) +\n  scale_linetype_manual(values = c(\"ENACTS\" = \"dashed\", \"ERA5\" = \"dotted\", \"CHIRPS\" = \"longdash\")) +\n  coord_cartesian(xlim = c(0.5, 4.2), clip = \"off\") +\n  labs(\n    title = \"Postrera: Forecast CV vs Observed Variability\",\n    subtitle = \"INSIVUMEH models show almost no interannual variation\",\n    x = \"Leadtime\", y = \"CV (%)\", fill = \"Model\"\n  ) +\n  theme(legend.position = \"bottom\") +\n  guides(linetype = \"none\")\n\n\n\n\n\n\n\n\n\nThe pattern holds: INSIVUMEH Postrera CV remains very low (3-7%) across all leadtimes, far below observed variability from any source. This is not a leadtime-specific issue—the Postrera forecasts systematically lack interannual signal.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Leadtime Diagnostics</span>"
    ]
  },
  {
    "objectID": "05_inter_lt_correlation.html#implications",
    "href": "05_inter_lt_correlation.html#implications",
    "title": "6  Leadtime Diagnostics",
    "section": "6.4 Implications",
    "text": "6.4 Implications\nThe diagnostic tests reveal a fundamental issue with INSIVUMEH Postrera forecasts:\n\n\n\n\n\n\n\n\n\nDiagnostic\nSEAS5\nINSIVUMEH Primera\nINSIVUMEH Postrera\n\n\n\n\nInter-LT correlation\nHigh (0.6-0.9)\nModerate (0.3-0.7)\nNear-zero or negative\n\n\nCV vs observed\n~70% of observed\n~80% of observed\n~20% of observed\n\n\nSignal strength\nStrong\nModerate\nLimited\n\n\n\nThis explains the suspicious patterns in Chapter 4:\n\nWhy CCSM4 LT3 &gt; LT1 for Postrera: If leadtimes are uncorrelated, apparent “skill” at any leadtime is random. By chance, LT3 happened to correlate better with observations than LT1 in this sample.\nWhy INSIVUMEH Postrera shows erratic skill: The forecasts contain almost no signal—they’re essentially flat lines near climatology with random noise.\nWhy skill scores are unreliable: When forecasts don’t vary, correlations are dominated by noise and small-sample coincidences.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Leadtime Diagnostics</span>"
    ]
  },
  {
    "objectID": "05_inter_lt_correlation.html#the-real-tie-breaker-for-postrera",
    "href": "05_inter_lt_correlation.html#the-real-tie-breaker-for-postrera",
    "title": "6  Leadtime Diagnostics",
    "section": "6.5 The Real Tie-Breaker for Postrera",
    "text": "6.5 The Real Tie-Breaker for Postrera\nThe skill metrics in earlier chapters were inconclusive for Postrera—no model showed consistent skill, and the patterns were erratic. These diagnostics break the tie.\nThe issue isn’t that INSIVUMEH Postrera forecasts lack skill—it’s that they lack signal. A forecast that barely varies from climatology and shows no internal consistency between leadtimes cannot be operationally useful, regardless of what correlation metrics suggest.\nPostrera recommendation: SEAS5\nNot because SEAS5 has demonstrated strong Postrera skill (it hasn’t), but because:\n\nSEAS5 is internally consistent—different leadtimes agree with each other\nSEAS5 shows realistic interannual variability\nSEAS5 at least behaves like a forecast, even if predictability is limited\n\nINSIVUMEH Postrera forecasts show limited predictive signal for this region and season.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Leadtime Diagnostics</span>"
    ]
  },
  {
    "objectID": "05_inter_lt_correlation.html#final-recommendations",
    "href": "05_inter_lt_correlation.html#final-recommendations",
    "title": "6  Leadtime Diagnostics",
    "section": "6.6 Final Recommendations",
    "text": "6.6 Final Recommendations\n\n\n\n\n\n\n\n\nSeason\nRecommended\nRationale\n\n\n\n\nPrimera\nSEAS5\nClear skill advantage across metrics and observation sources\n\n\nPostrera\nSEAS5\nINSIVUMEH forecasts show limited signal (low CV, weak inter-leadtime consistency)\n\n\n\nFor Postrera specifically, consider:\n\nAcknowledging limited predictability in operational communications\nUsing shorter-range forecasts when available\nSupplementing forecast-based triggers with monitoring-based indicators (e.g., observed rainfall deficits or VHI during early Postrera)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Leadtime Diagnostics</span>"
    ]
  },
  {
    "objectID": "07_multi_aoi_comparison.html",
    "href": "07_multi_aoi_comparison.html",
    "title": "7  CADC - Framework Skill Comparison",
    "section": "",
    "text": "7.1 Overview\nThis chapter extends the skill assessment beyond Chiquimula (Guatemala) to examine SEAS5 forecast performance across the Central American Dry Corridor, comparing:\nWe evaluate how skill varies by:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>CADC - Framework Skill Comparison</span>"
    ]
  },
  {
    "objectID": "07_multi_aoi_comparison.html#overview",
    "href": "07_multi_aoi_comparison.html#overview",
    "title": "7  CADC - Framework Skill Comparison",
    "section": "",
    "text": "Guatemala (Chiquimula - GT20)\nHonduras (El Paraíso + Francisco Morazán - HN07/HN08, area-weighted)\nEl Salvador (San Vicente - SV11)\n\n\n\nSeason: Primera (May-Aug) vs Postrera (Sep-Nov)\nLeadtime: LT0-2 for Primera, LT0-3 for Postrera\nBaseline period: 2000-2024 (25 years) vs 1990-2024 (35 years) vs 1981-2024 (44 years)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>CADC - Framework Skill Comparison</span>"
    ]
  },
  {
    "objectID": "07_multi_aoi_comparison.html#data-preparation",
    "href": "07_multi_aoi_comparison.html#data-preparation",
    "title": "7  CADC - Framework Skill Comparison",
    "section": "7.2 Data Preparation",
    "text": "7.2 Data Preparation\n\n\nCode\nPRIMERA_MONTHS &lt;- 5:8\nPOSTRERA_MONTHS &lt;- 9:11\nPRIMERA_ISSUED_MONTHS &lt;- c(3, 4, 5)\nPOSTRERA_ISSUED_MONTHS &lt;- c(6, 7, 8, 9)\n\nAOI_PCODES &lt;- c(\"HN07\", \"HN08\", \"SV11\", \"GT20\")\n\ncon &lt;- pg_con()\n\ndf_weights &lt;- tbl(con, \"polygon\") |&gt;\n  mutate(across(pcode, as.character)) |&gt;\n  filter(adm_level == 1, pcode %in% AOI_PCODES) |&gt;\n  select(pcode, iso3, name, seas5_n_upsampled_pixels) |&gt;\n  collect()\n\ndf_seas5_raw &lt;- tbl(con, \"seas5\") |&gt;\n  mutate(across(pcode, as.character)) |&gt;\n  filter(adm_level == 1, pcode %in% AOI_PCODES) |&gt;\n  collect()\n\ndf_era5_raw &lt;- tbl(con, \"era5\") |&gt;\n  mutate(across(pcode, as.character)) |&gt;\n  filter(pcode %in% AOI_PCODES) |&gt;\n  collect()\n\nDBI::dbDisconnect(con)\n\n# Process to seasonal totals\ndf_seas5_mm &lt;- df_seas5_raw |&gt;\n  mutate(value_mm = days_in_month(valid_date) * mean)\n\ndf_seas5_seasonal &lt;- bind_rows(\n  seas5_aggregate_forecast(df_seas5_mm, value = \"value_mm\", valid_months = PRIMERA_MONTHS,\n                           by = c(\"iso3\", \"pcode\", \"issued_date\")) |&gt; mutate(window = \"primera\"),\n  seas5_aggregate_forecast(df_seas5_mm, value = \"value_mm\", valid_months = POSTRERA_MONTHS,\n                           by = c(\"iso3\", \"pcode\", \"issued_date\")) |&gt; mutate(window = \"postrera\")\n) |&gt;\n  rename(fcst_mm = value_mm) |&gt;\n  mutate(\n    year = year(issued_date),\n    issued_month = month(issued_date)\n  ) |&gt;\n  filter(\n    (window == \"primera\" & issued_month %in% PRIMERA_ISSUED_MONTHS) |\n    (window == \"postrera\" & issued_month %in% POSTRERA_ISSUED_MONTHS)\n  )\n\ndf_era5_monthly &lt;- df_era5_raw |&gt;\n  mutate(\n    year = year(valid_date),\n    month = month(valid_date),\n    value_mm = mean * days_in_month(valid_date)\n  )\n\ndf_era5_seasonal &lt;- bind_rows(\n  df_era5_monthly |&gt;\n    filter(month %in% PRIMERA_MONTHS) |&gt;\n    group_by(pcode, iso3, year) |&gt;\n    summarise(obs_mm = sum(value_mm), .groups = \"drop\") |&gt;\n    mutate(window = \"primera\"),\n  df_era5_monthly |&gt;\n    filter(month %in% POSTRERA_MONTHS) |&gt;\n    group_by(pcode, iso3, year) |&gt;\n    summarise(obs_mm = sum(value_mm), .groups = \"drop\") |&gt;\n    mutate(window = \"postrera\")\n)\n\n# Also create country-level ERA5 for yearly RP heatmaps\ndf_obs_country &lt;- df_era5_seasonal |&gt;\n left_join(df_weights |&gt; select(pcode, seas5_n_upsampled_pixels), by = \"pcode\") |&gt;\n  mutate(\n    country_aoi = case_when(\n      pcode == \"GT20\" ~ \"Guatemala\",\n      pcode %in% c(\"HN07\", \"HN08\") ~ \"Honduras\",\n      pcode == \"SV11\" ~ \"El Salvador\"\n    )\n  ) |&gt;\n  group_by(country_aoi, year, window) |&gt;\n  summarise(\n    obs_mm = weighted.mean(obs_mm, w = seas5_n_upsampled_pixels),\n    .groups = \"drop\"\n  )\n\n# Join and aggregate to country level (weighted mean for Honduras)\ndf_joined &lt;- df_seas5_seasonal |&gt;\n  left_join(\n    df_era5_seasonal |&gt; select(pcode, year, window, obs_mm),\n    by = c(\"pcode\", \"year\", \"window\")\n  ) |&gt;\n  filter(!is.na(obs_mm)) |&gt;\n  left_join(df_weights |&gt; select(pcode, seas5_n_upsampled_pixels), by = \"pcode\") |&gt;\n  mutate(\n    country_aoi = case_when(\n      pcode == \"GT20\" ~ \"Guatemala\",\n      pcode %in% c(\"HN07\", \"HN08\") ~ \"Honduras\",\n      pcode == \"SV11\" ~ \"El Salvador\"\n    )\n  ) |&gt;\n  group_by(country_aoi, year, window, leadtime, issued_date) |&gt;\n  summarise(\n    fcst_mm = weighted.mean(fcst_mm, w = seas5_n_upsampled_pixels),\n    obs_mm = weighted.mean(obs_mm, w = seas5_n_upsampled_pixels),\n    .groups = \"drop\"\n  )\n\n\n\n\nCode\n# =============================================================================\n# RP threshold calculation - consistent with other chapters\n# Uses proper ranking with interpolation (not simple quantile)\n# =============================================================================\ncalc_rp_threshold &lt;- function(x, rp_target = 4, direction = -1) {\n  x &lt;- x[!is.na(x)]\n  n &lt;- length(x)\n  if (n &lt; 3) return(NA_real_)\n\n  # Use proper ranking with tie handling\n\n# For direction = -1 (drought/low extreme): lowest values get rank 1 (highest RP)\n  ranks &lt;- rank(x * -direction, ties.method = \"average\")\n  rp &lt;- (n + 1) / ranks\n\n  # Interpolate to find value at target RP\n  approx(rp, x, xout = rp_target, rule = 2)$y\n}\n\n# =============================================================================\n# Skill metrics calculation using yardstick\n# =============================================================================\ncalc_skill_metrics &lt;- function(df, baseline_start, baseline_end) {\n  df_baseline &lt;- df |&gt;\n    filter(year &gt;= baseline_start, year &lt;= baseline_end)\n\n  # Calculate observation thresholds per country/window\n  obs_thresholds &lt;- df_baseline |&gt;\n    group_by(country_aoi, window) |&gt;\n    summarise(obs_thresh = calc_rp_threshold(obs_mm, 4, -1), .groups = \"drop\")\n\n  # Calculate forecast thresholds per country/window/leadtime\n  fcst_thresholds &lt;- df_baseline |&gt;\n    group_by(country_aoi, window, leadtime) |&gt;\n    summarise(fcst_thresh = calc_rp_threshold(fcst_mm, 4, -1), .groups = \"drop\")\n\n  # Classify and create factors with proper levels for yardstick\n  # CRITICAL: drought must be first level (positive class)\n  # Using forcats::fct() for stricter validation (errors if values don't match levels)\n  drought_levels &lt;- c(\"drought\", \"no_drought\")\n\ndf_classified &lt;- df_baseline |&gt;\n    left_join(obs_thresholds, by = c(\"country_aoi\", \"window\")) |&gt;\n    left_join(fcst_thresholds, by = c(\"country_aoi\", \"window\", \"leadtime\")) |&gt;\n    mutate(\n      # Truth: was it actually a drought?\n      truth = fct(\n        if_else(obs_mm &lt;= obs_thresh, \"drought\", \"no_drought\"),\n        levels = drought_levels\n      ),\n      # Estimate: did we predict drought?\n      estimate = fct(\n        if_else(fcst_mm &lt;= fcst_thresh, \"drought\", \"no_drought\"),\n        levels = drought_levels\n      )\n    ) |&gt;\n    filter(!is.na(truth), !is.na(estimate))\n\n  # Calculate metrics using yardstick\n  df_classified |&gt;\n    group_by(country_aoi, window, leadtime) |&gt;\n    summarise(\n      n_years = n_distinct(year),\n      n_drought = sum(truth == \"drought\"),\n\n      # Spearman correlation (continuous)\n      spearman = cor(fcst_mm, obs_mm, method = \"spearman\", use = \"complete.obs\"),\n\n      # ROC-AUC: lower forecast = predict drought, so negate fcst_mm\n      # yardstick returns NA (with warning) if only one class present\n      roc_auc = roc_auc_vec(truth, -fcst_mm, event_level = \"first\"),\n\n      # F1 using yardstick (event_level = \"first\" means drought is positive class)\n      f1 = f_meas_vec(truth, estimate, event_level = \"first\"),\n      precision = precision_vec(truth, estimate, event_level = \"first\"),\n      recall = recall_vec(truth, estimate, event_level = \"first\"),\n\n      .groups = \"drop\"\n    ) |&gt;\n    select(country_aoi, window, leadtime, n_years, n_drought, spearman, roc_auc, f1)\n}\n\ndf_skill_2000 &lt;- calc_skill_metrics(df_joined, 2000, 2024) |&gt; mutate(baseline = \"2000-2024\")\ndf_skill_1991 &lt;- calc_skill_metrics(df_joined, 1991, 2024) |&gt; mutate(baseline = \"1991-2024\")\ndf_skill_1990 &lt;- calc_skill_metrics(df_joined, 1990, 2024) |&gt; mutate(baseline = \"1990-2024\")\ndf_skill_1981 &lt;- calc_skill_metrics(df_joined, 1981, 2024) |&gt; mutate(baseline = \"1981-2024\")\n\ndf_skill_all &lt;- bind_rows(df_skill_2000, df_skill_1991, df_skill_1990, df_skill_1981) |&gt;\n  mutate(\n    baseline = factor(baseline, levels = c(\"2000-2024\", \"1991-2024\", \"1990-2024\", \"1981-2024\")),\n    window = factor(window, levels = c(\"primera\", \"postrera\")),\n    country_aoi = factor(country_aoi, levels = c(\"Guatemala\", \"Honduras\", \"El Salvador\"))\n  )",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>CADC - Framework Skill Comparison</span>"
    ]
  },
  {
    "objectID": "07_multi_aoi_comparison.html#primera-consistently-skillful",
    "href": "07_multi_aoi_comparison.html#primera-consistently-skillful",
    "title": "7  CADC - Framework Skill Comparison",
    "section": "7.3 Primera: Consistently Skillful",
    "text": "7.3 Primera: Consistently Skillful\nPrimera forecasts show acceptable skill across all three countries, with ROC-AUC values consistently above 0.65 and reaching 0.85+ for Guatemala at shorter leadtimes.\n\n2000-2024 Baseline1990-2024 Baseline1981-2024 Baseline\n\n\n\n\nCode\ndf_skill_all |&gt;\n  filter(window == \"primera\", leadtime %in% c(0, 1, 2), baseline == \"2000-2024\") |&gt;\n  ggplot(aes(x = factor(leadtime), y = country_aoi, fill = roc_auc)) +\n  geom_tile(color = \"white\", linewidth = 0.8) +\n  geom_text(aes(label = sprintf(\"%.2f\", roc_auc)), size = 5, fontface = \"bold\") +\n  scale_fill_gradient2(\n    low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n    midpoint = 0.65, limits = c(0.5, 0.95), name = \"ROC-AUC\",\n    oob = scales::squish\n  ) +\n  labs(\n    title = \"Primera (May-Aug): SEAS5 Drought Detection Skill\",\n    subtitle = \"Baseline: 2000-2024 (n=25). ROC-AUC for RP4 drought threshold.\",\n    x = \"Leadtime (months)\", y = NULL\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_skill_all |&gt;\n  filter(window == \"primera\", leadtime %in% c(0, 1, 2), baseline == \"1990-2024\") |&gt;\n  ggplot(aes(x = factor(leadtime), y = country_aoi, fill = roc_auc)) +\n  geom_tile(color = \"white\", linewidth = 0.8) +\n  geom_text(aes(label = sprintf(\"%.2f\", roc_auc)), size = 5, fontface = \"bold\") +\n  scale_fill_gradient2(\n    low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n    midpoint = 0.65, limits = c(0.5, 0.95), name = \"ROC-AUC\",\n    oob = scales::squish\n  ) +\n  labs(\n    title = \"Primera (May-Aug): SEAS5 Drought Detection Skill\",\n    subtitle = \"Baseline: 1990-2024 (n=35). ROC-AUC for RP4 drought threshold.\",\n    x = \"Leadtime (months)\", y = NULL\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_skill_all |&gt;\n  filter(window == \"primera\", leadtime %in% c(0, 1, 2), baseline == \"1981-2024\") |&gt;\n  ggplot(aes(x = factor(leadtime), y = country_aoi, fill = roc_auc)) +\n  geom_tile(color = \"white\", linewidth = 0.8) +\n  geom_text(aes(label = sprintf(\"%.2f\", roc_auc)), size = 5, fontface = \"bold\") +\n  scale_fill_gradient2(\n    low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n    midpoint = 0.65, limits = c(0.5, 0.95), name = \"ROC-AUC\",\n    oob = scales::squish\n  ) +\n  labs(\n    title = \"Primera (May-Aug): SEAS5 Drought Detection Skill\",\n    subtitle = \"Baseline: 1981-2024 (n=44). ROC-AUC for RP4 drought threshold.\",\n    x = \"Leadtime (months)\", y = NULL\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nKey finding: Primera skill is robust across AOIs and baseline periods. The choice of baseline has minimal impact on Primera performance.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>CADC - Framework Skill Comparison</span>"
    ]
  },
  {
    "objectID": "07_multi_aoi_comparison.html#postrera-baseline-period-matters",
    "href": "07_multi_aoi_comparison.html#postrera-baseline-period-matters",
    "title": "7  CADC - Framework Skill Comparison",
    "section": "7.4 Postrera: Baseline Period Matters",
    "text": "7.4 Postrera: Baseline Period Matters\nPostrera presents a more complex picture. The 2000-2024 baseline suggests limited skill, but extending to 1981-2024 reveals substantially better performance.\n\n2000-2024 Baseline1990-2024 Baseline1981-2024 BaselineComparison\n\n\n\n\nCode\ndf_skill_all |&gt;\n  filter(window == \"postrera\", leadtime %in% c(0, 1, 2, 3), baseline == \"2000-2024\") |&gt;\n  ggplot(aes(x = factor(leadtime), y = country_aoi, fill = roc_auc)) +\n  geom_tile(color = \"white\", linewidth = 0.8) +\n  geom_text(aes(label = sprintf(\"%.2f\", roc_auc)), size = 5, fontface = \"bold\") +\n  scale_fill_gradient2(\n    low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n    midpoint = 0.65, limits = c(0.4, 0.9), name = \"ROC-AUC\",\n    oob = scales::squish\n  ) +\n  labs(\n    title = \"Postrera (Sep-Nov): SEAS5 Drought Detection Skill\",\n    subtitle = \"Baseline: 2000-2024 (n=25). Skill appears limited at longer leadtimes.\",\n    x = \"Leadtime (months)\", y = NULL\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_skill_all |&gt;\n  filter(window == \"postrera\", leadtime %in% c(0, 1, 2, 3), baseline == \"1990-2024\") |&gt;\n  ggplot(aes(x = factor(leadtime), y = country_aoi, fill = roc_auc)) +\n  geom_tile(color = \"white\", linewidth = 0.8) +\n  geom_text(aes(label = sprintf(\"%.2f\", roc_auc)), size = 5, fontface = \"bold\") +\n  scale_fill_gradient2(\n    low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n    midpoint = 0.65, limits = c(0.4, 0.9), name = \"ROC-AUC\",\n    oob = scales::squish\n  ) +\n  labs(\n    title = \"Postrera (Sep-Nov): SEAS5 Drought Detection Skill\",\n    subtitle = \"Baseline: 1990-2024 (n=35). ROC-AUC for RP4 drought threshold.\",\n    x = \"Leadtime (months)\", y = NULL\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_skill_all |&gt;\n  filter(window == \"postrera\", leadtime %in% c(0, 1, 2, 3), baseline == \"1981-2024\") |&gt;\n  ggplot(aes(x = factor(leadtime), y = country_aoi, fill = roc_auc)) +\n  geom_tile(color = \"white\", linewidth = 0.8) +\n  geom_text(aes(label = sprintf(\"%.2f\", roc_auc)), size = 5, fontface = \"bold\") +\n  scale_fill_gradient2(\n    low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n    midpoint = 0.65, limits = c(0.4, 0.9), name = \"ROC-AUC\",\n    oob = scales::squish\n  ) +\n  labs(\n    title = \"Postrera (Sep-Nov): SEAS5 Drought Detection Skill\",\n    subtitle = \"Baseline: 1981-2024 (n=44). Longer baseline reveals stronger skill.\",\n    x = \"Leadtime (months)\", y = NULL\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_skill_all |&gt;\n  filter(window == \"postrera\", leadtime %in% c(0, 1, 2, 3)) |&gt;\n  ggplot(aes(x = factor(leadtime), y = roc_auc, fill = baseline)) +\n  geom_col(position = position_dodge(width = 0.8), width = 0.7) +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"grey40\") +\n  geom_hline(yintercept = 0.7, linetype = \"dotted\", color = \"grey60\") +\n  facet_wrap(~country_aoi) +\n  scale_fill_manual(values = c(\"2000-2024\" = \"#fc8d62\", \"1990-2024\" = \"#8da0cb\", \"1981-2024\" = \"#66c2a5\")) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +\n  labs(\n    title = \"Postrera: Baseline Comparison\",\n    subtitle = \"Dashed = random chance (0.5). Dotted = 0.7 threshold. Longer baseline consistently better.\",\n    x = \"Leadtime (months)\", y = \"ROC-AUC\", fill = \"Baseline\"\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    strip.text = element_text(size = 12, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.4.1 Skill Improvement Table\n\n\nCode\ndf_skill_all |&gt;\n  filter(window == \"postrera\", leadtime %in% c(0, 1, 2, 3)) |&gt;\n  select(country_aoi, baseline, leadtime, roc_auc) |&gt;\n  pivot_wider(names_from = baseline, values_from = roc_auc) |&gt;\n  mutate(\n    diff_1990 = `1990-2024` - `2000-2024`,\n    diff_1981 = `1981-2024` - `2000-2024`\n  ) |&gt;\n  arrange(country_aoi, leadtime) |&gt;\n  gt() |&gt;\n  fmt_number(columns = c(`2000-2024`, `1990-2024`, `1981-2024`, diff_1990, diff_1981), decimals = 2) |&gt;\n  cols_label(\n    country_aoi = \"Country\",\n    leadtime = \"LT\",\n    `2000-2024` = \"2000-2024\",\n    `1990-2024` = \"1990-2024\",\n    `1981-2024` = \"1981-2024\",\n    diff_1990 = \"Δ 1990\",\n    diff_1981 = \"Δ 1981\"\n  ) |&gt;\n  tab_header(\n    title = \"Postrera ROC-AUC: Baseline Comparison\",\n    subtitle = \"Skill improvement from extending baseline (relative to 2000-2024)\"\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#d4edda\"),\n    locations = cells_body(\n      columns = c(diff_1990, diff_1981),\n      rows = diff_1981 &gt; 0.1\n    )\n  )\n\n\n\n\n\n\n\n\nPostrera ROC-AUC: Baseline Comparison\n\n\nSkill improvement from extending baseline (relative to 2000-2024)\n\n\nCountry\nLT\n2000-2024\n1991-2024\n1990-2024\n1981-2024\nΔ 1990\nΔ 1981\n\n\n\n\nGuatemala\n0\n0.66\n0.7980769\n0.76\n0.80\n0.11\n0.14\n\n\nGuatemala\n1\n0.66\n0.6875000\n0.67\n0.72\n0.01\n0.07\n\n\nGuatemala\n2\n0.50\n0.6201923\n0.58\n0.66\n0.08\n0.16\n\n\nGuatemala\n3\n0.57\n0.6586538\n0.61\n0.65\n0.04\n0.08\n\n\nHonduras\n0\n0.85\n0.8413462\n0.80\n0.87\n−0.05\n0.01\n\n\nHonduras\n1\n0.82\n0.7980769\n0.76\n0.79\n−0.07\n−0.03\n\n\nHonduras\n2\n0.66\n0.7307692\n0.69\n0.77\n0.03\n0.11\n\n\nHonduras\n3\n0.62\n0.6826923\n0.69\n0.74\n0.07\n0.12\n\n\nEl Salvador\n0\n0.71\n0.7596154\n0.75\n0.86\n0.04\n0.15\n\n\nEl Salvador\n1\n0.60\n0.6634615\n0.67\n0.76\n0.07\n0.17\n\n\nEl Salvador\n2\n0.46\n0.5673077\n0.57\n0.63\n0.11\n0.16\n\n\nEl Salvador\n3\n0.43\n0.5288462\n0.52\n0.63\n0.09\n0.20\n\n\n\n\n\n\n\n\n\n7.4.2 F1 Score by Baseline\n\n1981-2024 Baseline1991-2024 Baseline2000-2024 BaselineComparison\n\n\n\n\nCode\ndf_skill_all |&gt;\n  filter(window == \"postrera\", leadtime %in% c(0, 1, 2, 3), baseline == \"1981-2024\") |&gt;\n  ggplot(aes(x = factor(leadtime), y = country_aoi, fill = f1)) +\n  geom_tile(color = \"white\", linewidth = 0.8) +\n  geom_text(aes(label = sprintf(\"%.2f\", f1)), size = 5, fontface = \"bold\") +\n  scale_fill_gradient2(\n    low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n    midpoint = 0.4, limits = c(0, 0.8), name = \"F1 Score\",\n    oob = scales::squish, na.value = \"grey80\"\n  ) +\n  labs(\n    title = \"Postrera (Sep-Nov): F1 Score for Drought Classification\",\n    subtitle = \"Baseline: 1981-2024 (n=44). F1 score for RP4 drought threshold.\",\n    x = \"Leadtime (months)\", y = NULL\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_skill_all |&gt;\n  filter(window == \"postrera\", leadtime %in% c(0, 1, 2, 3), baseline == \"1991-2024\") |&gt;\n  ggplot(aes(x = factor(leadtime), y = country_aoi, fill = f1)) +\n  geom_tile(color = \"white\", linewidth = 0.8) +\n  geom_text(aes(label = sprintf(\"%.2f\", f1)), size = 5, fontface = \"bold\") +\n  scale_fill_gradient2(\n    low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n    midpoint = 0.4, limits = c(0, 0.8), name = \"F1 Score\",\n    oob = scales::squish, na.value = \"grey80\"\n  ) +\n  labs(\n    title = \"Postrera (Sep-Nov): F1 Score for Drought Classification\",\n    subtitle = \"Baseline: 1991-2024 (n=34). F1 score for RP4 drought threshold.\",\n    x = \"Leadtime (months)\", y = NULL\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_skill_all |&gt;\n  filter(window == \"postrera\", leadtime %in% c(0, 1, 2, 3), baseline == \"2000-2024\") |&gt;\n  ggplot(aes(x = factor(leadtime), y = country_aoi, fill = f1)) +\n  geom_tile(color = \"white\", linewidth = 0.8) +\n  geom_text(aes(label = sprintf(\"%.2f\", f1)), size = 5, fontface = \"bold\") +\n  scale_fill_gradient2(\n    low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n    midpoint = 0.4, limits = c(0, 0.8), name = \"F1 Score\",\n    oob = scales::squish, na.value = \"grey80\"\n  ) +\n  labs(\n    title = \"Postrera (Sep-Nov): F1 Score for Drought Classification\",\n    subtitle = \"Baseline: 2000-2024 (n=25). F1 score for RP4 drought threshold.\",\n    x = \"Leadtime (months)\", y = NULL\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_skill_all |&gt;\n  filter(window == \"postrera\", leadtime %in% c(0, 1, 2, 3),\n         baseline %in% c(\"1981-2024\", \"1991-2024\", \"2000-2024\")) |&gt;\n  mutate(baseline = factor(baseline, levels = c(\"2000-2024\", \"1991-2024\", \"1981-2024\"))) |&gt;\n  ggplot(aes(x = factor(leadtime), y = f1, fill = baseline)) +\n  geom_col(position = position_dodge(width = 0.8), width = 0.7) +\n  facet_wrap(~country_aoi) +\n  scale_fill_manual(values = c(\"2000-2024\" = \"#fc8d62\", \"1991-2024\" = \"#8da0cb\", \"1981-2024\" = \"#66c2a5\")) +\n  scale_y_continuous(limits = c(0, 0.8), breaks = seq(0, 0.8, 0.2)) +\n  labs(\n    title = \"Postrera F1 Score: Baseline Comparison\",\n    subtitle = \"F1 = harmonic mean of precision and recall. Longer baseline generally improves F1.\",\n    x = \"Leadtime (months)\", y = \"F1 Score\", fill = \"Baseline\"\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    strip.text = element_text(size = 12, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>CADC - Framework Skill Comparison</span>"
    ]
  },
  {
    "objectID": "07_multi_aoi_comparison.html#leadtime-vs-operational-value-trade-off",
    "href": "07_multi_aoi_comparison.html#leadtime-vs-operational-value-trade-off",
    "title": "7  CADC - Framework Skill Comparison",
    "section": "7.5 Leadtime vs Operational Value Trade-off",
    "text": "7.5 Leadtime vs Operational Value Trade-off\nThe key challenge for Postrera is balancing forecast skill against operational lead time:\n\n\n\nLeadtime\nIssued\nOperational Lead Time\nSkill (1981-2024)\n\n\n\n\nLT0\nSeptember\n~0 months\nExcellent (0.80-0.87)\n\n\nLT1\nAugust\n~1 month\nGood (0.72-0.79)\n\n\nLT2\nJuly\n~2 months\nModerate (0.63-0.77)\n\n\nLT3\nJune\n~3 months\nWeaker (0.63-0.74)\n\n\n\nRecommendation: For Postrera, use LT0 and LT1:\n\nLT1 (August-issued): Provides ~1 month lead time for preparedness actions with good skill (ROC-AUC 0.72-0.79)\nLT0 (September-issued): Excellent skill (ROC-AUC 0.80-0.87) for confirming/updating the LT1 forecast as the season begins\nSkill degrades substantially at LT2-3, making these less reliable for operational decisions",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>CADC - Framework Skill Comparison</span>"
    ]
  },
  {
    "objectID": "07_multi_aoi_comparison.html#regional-differences",
    "href": "07_multi_aoi_comparison.html#regional-differences",
    "title": "7  CADC - Framework Skill Comparison",
    "section": "7.6 Regional Differences",
    "text": "7.6 Regional Differences\nHonduras consistently shows the strongest Postrera skill, while Guatemala shows the weakest:\n\n\nCode\ndf_skill_all |&gt;\n  filter(baseline == \"1981-2024\") |&gt;\n  mutate(\n    window_label = factor(window,\n      levels = c(\"primera\", \"postrera\"),\n      labels = c(\"Primera (May-Aug)\", \"Postrera (Sep-Nov)\")\n    )\n  ) |&gt;\n  ggplot(aes(x = factor(leadtime), y = roc_auc, color = country_aoi, group = country_aoi)) +\n  geom_line(linewidth = 1.2) +\n  geom_point(size = 3) +\n  geom_hline(yintercept = 0.7, linetype = \"dashed\", color = \"grey50\") +\n  facet_wrap(~window_label, scales = \"free_x\") +\n  scale_color_manual(values = c(\"Guatemala\" = \"#1b9e77\", \"Honduras\" = \"#d95f02\", \"El Salvador\" = \"#7570b3\")) +\n  scale_y_continuous(limits = c(0.5, 0.95)) +\n  labs(\n    title = \"SEAS5 Skill Across Regions (1981-2024 Baseline)\",\n    subtitle = \"Honduras shows strongest Postrera skill. Dashed line = 0.7 threshold.\",\n    x = \"Leadtime (months)\", y = \"ROC-AUC\", color = \"Country\"\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    strip.text = element_text(size = 12, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>CADC - Framework Skill Comparison</span>"
    ]
  },
  {
    "objectID": "07_multi_aoi_comparison.html#binary-performance-metrics",
    "href": "07_multi_aoi_comparison.html#binary-performance-metrics",
    "title": "7  CADC - Framework Skill Comparison",
    "section": "7.7 Binary Performance Metrics",
    "text": "7.7 Binary Performance Metrics\nThis section shows the empirical return period (RP) of both SEAS5 forecasts and ERA5 observations for each year and country, allowing direct comparison of forecast drought signals against what actually occurred.\n\n\nCode\n# Calculate empirical RP for forecasts\ncalc_empirical_rp_fcst &lt;- function(df, baseline_start, baseline_end) {\n  df |&gt;\n    filter(year &gt;= baseline_start, year &lt;= baseline_end) |&gt;\n    group_by(country_aoi, window, leadtime) |&gt;\n    mutate(\n      n = n(),\n      rank_drought = rank(fcst_mm, ties.method = \"average\"),\n      empirical_rp = (n + 1) / rank_drought\n    ) |&gt;\n    ungroup() |&gt;\n    mutate(source = paste0(\"LT\", leadtime)) |&gt;\n    select(country_aoi, year, window, source, value_mm = fcst_mm, empirical_rp)\n}\n\n# Calculate empirical RP for observations\ncalc_empirical_rp_obs &lt;- function(df, baseline_start, baseline_end) {\n  df |&gt;\n    filter(year &gt;= baseline_start, year &lt;= baseline_end) |&gt;\n    group_by(country_aoi, window) |&gt;\n    mutate(\n      n = n(),\n      rank_drought = rank(obs_mm, ties.method = \"average\"),\n      empirical_rp = (n + 1) / rank_drought\n    ) |&gt;\n    ungroup() |&gt;\n    mutate(source = \"OBS\") |&gt;\n    select(country_aoi, year, window, source, value_mm = obs_mm, empirical_rp)\n}\n\n# Calculate RP for forecasts and observations (1981-2024 baseline)\ndf_rp_fcst &lt;- calc_empirical_rp_fcst(df_joined, 1981, 2024)\ndf_rp_obs &lt;- calc_empirical_rp_obs(df_obs_country, 1981, 2024)\n\n# Combine into single dataset\ndf_rp_yearly &lt;- bind_rows(df_rp_fcst, df_rp_obs)\n\n\n\n\nCode\nplot_yearly_rp_heatmap &lt;- function(df, window_name, title_suffix = \"\") {\n\n  df_plot &lt;- df |&gt;\n    filter(window == window_name, source %in% c(\"OBS\", \"LT0\", \"LT1\")) |&gt;\n    mutate(\n      source = factor(source, levels = c(\"OBS\", \"LT1\", \"LT0\")),\n      country_aoi = factor(country_aoi, levels = c(\"Guatemala\", \"Honduras\", \"El Salvador\")),\n      rp_label = case_when(\n        empirical_rp &gt;= 10 ~ sprintf(\"%.0f\", empirical_rp),\n        empirical_rp &gt;= 1 ~ sprintf(\"%.1f\", empirical_rp),\n        TRUE ~ sprintf(\"%.2f\", empirical_rp)\n      ),\n      rp_capped = pmin(empirical_rp, 15)\n    )\n\n  df_drought &lt;- df_plot |&gt; filter(empirical_rp &gt;= 4)\n\n  ggplot(df_plot, aes(x = source, y = factor(year))) +\n    geom_tile(aes(fill = rp_capped), color = \"white\", linewidth = 0.4) +\n    geom_text(aes(label = rp_label), size = 2.5, fontface = \"bold\") +\n    geom_tile(\n      data = df_drought,\n      aes(x = source, y = factor(year)),\n      fill = NA, color = \"black\", linewidth = 0.8\n    ) +\n    facet_wrap(~country_aoi, nrow = 1) +\n    scale_fill_gradient2(\n      low = \"#1A9850\", mid = \"#FFFFBF\", high = \"#D73027\",\n      midpoint = 2, limits = c(1, 15),\n      name = \"Empirical RP\",\n      oob = scales::squish,\n      breaks = c(1, 2, 4, 8, 15),\n      labels = c(\"1\", \"2\", \"4\", \"8\", \"15+\")\n    ) +\n    scale_y_discrete(limits = rev) +\n    labs(\n      title = paste0(\"SEAS5 Forecast vs ERA5 Observed: \", str_to_title(window_name), title_suffix),\n      subtitle = \"Empirical RP. Black border = RP \\u2265 4 (drought). OBS = ERA5, LT = SEAS5 forecast.\",\n      x = NULL, y = \"Year\",\n      caption = \"Baseline: 1981-2024. Higher RP = drier (drought signal).\"\n    ) +\n    theme(\n      axis.text.y = element_text(size = 6),\n      axis.text.x = element_text(size = 9),\n      strip.text = element_text(size = 11, face = \"bold\"),\n      legend.position = \"right\",\n      panel.grid = element_blank(),\n      panel.spacing = unit(1, \"lines\"),\n      plot.caption = element_text(hjust = 0)\n    )\n}\n\n\nHow to read these plots:\n\nOBS: ERA5 observed rainfall (what actually happened)\nLT1: SEAS5 forecast issued ~1 month before season start\nLT0: SEAS5 forecast issued at season start\nBlack border: RP ≥ 4 indicates drought threshold crossed\nHits: Both forecast AND observation have black borders\nFalse alarms: Forecast has black border but OBS doesn’t\nMisses: OBS has black border but forecast doesn’t\n\n\n7.7.1 Primera\n\n\nCode\nplot_yearly_rp_heatmap(df_rp_yearly, \"primera\")\n\n\n\n\n\n\n\n\n\n\n\n7.7.2 Postrera\n\n\nCode\nplot_yearly_rp_heatmap(df_rp_yearly, \"postrera\")\n\n\n\n\n\n\n\n\n\n\n\n7.7.3 F1 Score vs RP Threshold\nIs RP=4 the optimal threshold? This analysis shows how F1 varies as we change the drought threshold from RP=3 to RP=6.\n\n\nCode\n# Calculate F1 for a range of RP thresholds\nrp_thresholds &lt;- seq(3, 6, by = 0.2)\n\ncalc_f1_by_threshold &lt;- function(df, rp_target, baseline_start = 1981, baseline_end = 2024) {\n  # df_joined already has both fcst_mm and obs_mm\n  df_baseline &lt;- df |&gt;\n    filter(year &gt;= baseline_start, year &lt;= baseline_end)\n\n  # Calculate obs threshold for this RP\n  obs_thresholds &lt;- df_baseline |&gt;\n    group_by(country_aoi, window) |&gt;\n    summarise(obs_thresh = calc_rp_threshold(obs_mm, rp_target, -1), .groups = \"drop\")\n\n  # Calculate forecast threshold for this RP\n  fcst_thresholds &lt;- df_baseline |&gt;\n    group_by(country_aoi, window, leadtime) |&gt;\n    summarise(fcst_thresh = calc_rp_threshold(fcst_mm, rp_target, -1), .groups = \"drop\")\n\n  # Classify and calculate F1\n  drought_levels &lt;- c(\"drought\", \"no_drought\")\n\n  df_baseline |&gt;\n    left_join(obs_thresholds, by = c(\"country_aoi\", \"window\")) |&gt;\n    left_join(fcst_thresholds, by = c(\"country_aoi\", \"window\", \"leadtime\")) |&gt;\n    mutate(\n      truth = fct(if_else(obs_mm &lt;= obs_thresh, \"drought\", \"no_drought\"), levels = drought_levels),\n      estimate = fct(if_else(fcst_mm &lt;= fcst_thresh, \"drought\", \"no_drought\"), levels = drought_levels)\n    ) |&gt;\n    filter(!is.na(truth), !is.na(estimate)) |&gt;\n    group_by(country_aoi, window, leadtime) |&gt;\n    summarise(\n      f1 = f_meas_vec(truth, estimate, event_level = \"first\"),\n      n_drought_obs = sum(truth == \"drought\"),\n      n_drought_fcst = sum(estimate == \"drought\"),\n      .groups = \"drop\"\n    ) |&gt;\n    mutate(rp_threshold = rp_target)\n}\n\n# Run for all thresholds\ndf_f1_by_threshold &lt;- map_dfr(rp_thresholds, ~calc_f1_by_threshold(df_joined, .x))\n\n\n\n\nCode\ndf_f1_by_threshold |&gt;\n  filter(leadtime %in% c(0, 1)) |&gt;\n  mutate(\n    lt_label = paste0(\"LT\", leadtime),\n    country_aoi = factor(country_aoi, levels = c(\"Guatemala\", \"Honduras\", \"El Salvador\")),\n    window_label = factor(window,\n      levels = c(\"primera\", \"postrera\"),\n      labels = c(\"Primera\", \"Postrera\"))\n  ) |&gt;\n  ggplot(aes(x = rp_threshold, y = f1, color = lt_label, linetype = lt_label)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n  geom_vline(xintercept = 4, linetype = \"dashed\", color = \"grey50\", linewidth = 0.5) +\n  facet_grid(window_label ~ country_aoi) +\n  scale_x_continuous(breaks = seq(3, 6, 0.5)) +\n  scale_y_continuous(limits = c(0, 0.8)) +\n  scale_color_manual(values = c(\"LT0\" = \"#1b9e77\", \"LT1\" = \"#d95f02\")) +\n  labs(\n    title = \"F1 Score vs RP Threshold\",\n    subtitle = \"Dashed line = current RP=4 threshold. Higher F1 = better classification performance.\",\n    x = \"RP Threshold\",\n    y = \"F1 Score\",\n    color = \"Leadtime\",\n    linetype = \"Leadtime\",\n    caption = \"Baseline: 1981-2024. F1 = harmonic mean of precision and recall.\"\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    strip.text = element_text(size = 11, face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nInterpretation: Peaks in these curves indicate optimal thresholds. If F1 peaks at RP≠4, consider adjusting the operational threshold. Flat curves suggest the threshold choice matters less for that combination.\n\n\n7.7.4 OR-Based Trigger Optimization (Postrera)\nThe operational trigger uses OR logic: activate if LT0 OR LT1 predicts drought. We can optimize by using different RP thresholds for each leadtime - allowing LT1 (longer lead, less accurate) to be more conservative.\n\n\nCode\n# Search over 2D threshold space for OR-based trigger\n# Constrain both to RP &gt;= 3 (can't go below 3 operationally)\n# Expect LT0 &lt; LT1 (LT0 more accurate, can use lower/more aggressive threshold)\nrp_lt0_grid &lt;- seq(3, 8, by = 0.25)\nrp_lt1_grid &lt;- seq(3, 8, by = 0.25)\nthreshold_grid &lt;- expand.grid(rp_lt0 = rp_lt0_grid, rp_lt1 = rp_lt1_grid)\n\ncalc_or_trigger_f1 &lt;- function(df, rp_lt0, rp_lt1, baseline_start = 1981, baseline_end = 2024) {\n\n  df_baseline &lt;- df |&gt;\n    filter(year &gt;= baseline_start, year &lt;= baseline_end, window == \"postrera\")\n\n  # Get obs threshold (fixed at RP=4 - definition of drought doesn't change)\n  obs_thresh_df &lt;- df_baseline |&gt;\n    group_by(country_aoi) |&gt;\n    summarise(obs_thresh = calc_rp_threshold(obs_mm, 4, -1), .groups = \"drop\")\n\n  # Get forecast thresholds per leadtime (these we optimize)\n  fcst_thresh_lt0 &lt;- df_baseline |&gt;\n    filter(leadtime == 0) |&gt;\n    group_by(country_aoi) |&gt;\n    summarise(fcst_thresh_lt0 = calc_rp_threshold(fcst_mm, rp_lt0, -1), .groups = \"drop\")\n\n  fcst_thresh_lt1 &lt;- df_baseline |&gt;\n    filter(leadtime == 1) |&gt;\n    group_by(country_aoi) |&gt;\n    summarise(fcst_thresh_lt1 = calc_rp_threshold(fcst_mm, rp_lt1, -1), .groups = \"drop\")\n\n  # Pivot to wide format (one row per country-year)\n  df_wide &lt;- df_baseline |&gt;\n    filter(leadtime %in% c(0, 1)) |&gt;\n    select(country_aoi, year, leadtime, fcst_mm, obs_mm) |&gt;\n    pivot_wider(names_from = leadtime, values_from = fcst_mm, names_prefix = \"fcst_lt\") |&gt;\n    left_join(obs_thresh_df, by = \"country_aoi\") |&gt;\n    left_join(fcst_thresh_lt0, by = \"country_aoi\") |&gt;\n    left_join(fcst_thresh_lt1, by = \"country_aoi\") |&gt;\n    mutate(\n      obs_drought = obs_mm &lt;= obs_thresh,\n      fcst_drought_lt0 = fcst_lt0 &lt;= fcst_thresh_lt0,\n      fcst_drought_lt1 = fcst_lt1 &lt;= fcst_thresh_lt1,\n      # OR logic: trigger if either LT predicts drought\n      fcst_drought_or = fcst_drought_lt0 | fcst_drought_lt1\n    )\n\n  # Calculate F1 per country using yardstick\n  drought_levels &lt;- c(\"drought\", \"no_drought\")\n\n  df_wide |&gt;\n    mutate(\n      truth = fct(if_else(obs_drought, \"drought\", \"no_drought\"), levels = drought_levels),\n      estimate = fct(if_else(fcst_drought_or, \"drought\", \"no_drought\"), levels = drought_levels)\n    ) |&gt;\n    group_by(country_aoi) |&gt;\n    summarise(\n      f1 = f_meas_vec(truth, estimate, event_level = \"first\"),\n      precision = precision_vec(truth, estimate, event_level = \"first\"),\n      recall = recall_vec(truth, estimate, event_level = \"first\"),\n      .groups = \"drop\"\n    ) |&gt;\n    mutate(rp_lt0 = rp_lt0, rp_lt1 = rp_lt1)\n}\n\n# Run grid search\ndf_or_grid &lt;- map2_dfr(\n  threshold_grid$rp_lt0,\n  threshold_grid$rp_lt1,\n  ~calc_or_trigger_f1(df_joined, .x, .y)\n)\n\n\n\n7.7.4.1 F1 by Country\n\n\nCode\n# Find optimal per country\ndf_optimal_country &lt;- df_or_grid |&gt;\n  group_by(country_aoi) |&gt;\n  slice_max(f1, n = 1) |&gt;\n  slice(1)\n\ndf_or_grid |&gt;\n  mutate(country_aoi = factor(country_aoi, levels = c(\"Guatemala\", \"Honduras\", \"El Salvador\"))) |&gt;\n  ggplot(aes(x = rp_lt0, y = rp_lt1, fill = f1)) +\n  geom_tile() +\n  geom_point(data = df_optimal_country |&gt;\n               mutate(country_aoi = factor(country_aoi, levels = c(\"Guatemala\", \"Honduras\", \"El Salvador\"))),\n             color = \"white\", size = 3, shape = 4, stroke = 2) +\n  geom_vline(xintercept = 4, linetype = \"dashed\", color = \"white\", alpha = 0.7) +\n  geom_hline(yintercept = 4, linetype = \"dashed\", color = \"white\", alpha = 0.7) +\n  facet_wrap(~country_aoi) +\n  scale_fill_viridis_c(option = \"plasma\", limits = c(0, 0.7), na.value = \"grey50\") +\n  labs(\n    title = \"Postrera: F1 for OR-Based Trigger (LT0 OR LT1)\",\n    subtitle = \"X = optimal per country. Dashed lines = current RP=4 threshold.\",\n    x = \"RP Threshold for LT0 (Sep-issued)\",\n    y = \"RP Threshold for LT1 (Aug-issued)\",\n    fill = \"F1 Score\"\n  ) +\n  theme(\n    strip.text = element_text(size = 11, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n7.7.4.2 Average F1 Across Countries\nFor a single operational threshold, we average F1 across all three countries.\n\n\nCode\n# Average F1 across all countries for each threshold combination\ndf_or_avg &lt;- df_or_grid |&gt;\n  group_by(rp_lt0, rp_lt1) |&gt;\n  summarise(\n    f1_mean = mean(f1, na.rm = TRUE),\n    f1_min = min(f1, na.rm = TRUE),\n    f1_max = max(f1, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# Find overall optimal\ndf_optimal_overall &lt;- df_or_avg |&gt;\n  slice_max(f1_mean, n = 1) |&gt;\n  slice(1)\n\n# Baseline F1 at RP=4/RP=4\nf1_baseline &lt;- df_or_avg |&gt;\n  filter(rp_lt0 == 4, rp_lt1 == 4) |&gt;\n  pull(f1_mean)\n\nggplot(df_or_avg, aes(x = rp_lt0, y = rp_lt1, fill = f1_mean)) +\n  geom_tile() +\n  geom_point(data = df_optimal_overall, color = \"white\", size = 4, shape = 4, stroke = 2) +\n  geom_vline(xintercept = 4, linetype = \"dashed\", color = \"white\", alpha = 0.7) +\n  geom_hline(yintercept = 4, linetype = \"dashed\", color = \"white\", alpha = 0.7) +\n  scale_fill_viridis_c(option = \"plasma\", limits = c(0, 0.6), na.value = \"grey50\") +\n  labs(\n    title = \"Postrera: Average F1 Across Countries (OR Trigger)\",\n    subtitle = paste0(\"Optimal: RP_LT0 = \", df_optimal_overall$rp_lt0,\n                      \", RP_LT1 = \", df_optimal_overall$rp_lt1,\n                      \" | Mean F1 = \", round(df_optimal_overall$f1_mean, 3),\n                      \" (vs \", round(f1_baseline, 3), \" at RP=4/4)\"),\n    x = \"RP Threshold for LT0 (Sep-issued)\",\n    y = \"RP Threshold for LT1 (Aug-issued)\",\n    fill = \"Mean F1\",\n    caption = \"Averaged over Guatemala, Honduras, El Salvador. X = optimal.\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n7.7.4.3 Summary Table\n\n\nCode\n# Get baseline F1 per country at RP=4/RP=4\ndf_baseline_f1 &lt;- df_or_grid |&gt;\n  filter(rp_lt0 == 4, rp_lt1 == 4) |&gt;\n  select(country_aoi, f1_baseline = f1)\n\n# Function to calculate empirical combined RP for OR trigger\ncalc_empirical_or_rp &lt;- function(df, rp_lt0, rp_lt1, baseline_start = 1981, baseline_end = 2024) {\n  df_baseline &lt;- df |&gt;\n    filter(year &gt;= baseline_start, year &lt;= baseline_end, window == \"postrera\")\n\n  # Get forecast thresholds per leadtime\n fcst_thresh_lt0 &lt;- df_baseline |&gt;\n    filter(leadtime == 0) |&gt;\n    group_by(country_aoi) |&gt;\n    summarise(fcst_thresh_lt0 = calc_rp_threshold(fcst_mm, rp_lt0, -1), .groups = \"drop\")\n\n  fcst_thresh_lt1 &lt;- df_baseline |&gt;\n    filter(leadtime == 1) |&gt;\n    group_by(country_aoi) |&gt;\n    summarise(fcst_thresh_lt1 = calc_rp_threshold(fcst_mm, rp_lt1, -1), .groups = \"drop\")\n\n  # Pivot and calculate OR trigger\n  df_baseline |&gt;\n    filter(leadtime %in% c(0, 1)) |&gt;\n    select(country_aoi, year, leadtime, fcst_mm) |&gt;\n    pivot_wider(names_from = leadtime, values_from = fcst_mm, names_prefix = \"fcst_lt\") |&gt;\n    left_join(fcst_thresh_lt0, by = \"country_aoi\") |&gt;\n    left_join(fcst_thresh_lt1, by = \"country_aoi\") |&gt;\n    mutate(\n      fcst_drought_lt0 = fcst_lt0 &lt;= fcst_thresh_lt0,\n      fcst_drought_lt1 = fcst_lt1 &lt;= fcst_thresh_lt1,\n      fcst_drought_or = fcst_drought_lt0 | fcst_drought_lt1\n    ) |&gt;\n    group_by(country_aoi) |&gt;\n    summarise(\n      n_years = n(),\n      n_triggers = sum(fcst_drought_or, na.rm = TRUE),\n      rp_empirical = (n_years + 1) / n_triggers,\n      .groups = \"drop\"\n    )\n}\n\n# Get empirical RP for each threshold combo in the optimal set\ndf_rp_country &lt;- map2_dfr(\n  df_optimal_country$rp_lt0,\n  df_optimal_country$rp_lt1,\n  ~calc_empirical_or_rp(df_joined, .x, .y) |&gt;\n    mutate(rp_lt0 = .x, rp_lt1 = .y)\n)\n\ndf_rp_overall &lt;- calc_empirical_or_rp(df_joined, df_optimal_overall$rp_lt0, df_optimal_overall$rp_lt1) |&gt;\n  mutate(rp_lt0 = df_optimal_overall$rp_lt0, rp_lt1 = df_optimal_overall$rp_lt1)\n\ndf_rp_baseline &lt;- calc_empirical_or_rp(df_joined, 4, 4)\n\n# Country-specific optima\ndf_country_summary &lt;- df_optimal_country |&gt;\n  left_join(df_baseline_f1, by = \"country_aoi\") |&gt;\n  left_join(df_rp_country |&gt; select(country_aoi, rp_lt0, rp_lt1, rp_empirical),\n            by = c(\"country_aoi\", \"rp_lt0\", \"rp_lt1\")) |&gt;\n  mutate(\n    f1_improvement = f1 - f1_baseline,\n    type = \"Country-specific\"\n  ) |&gt;\n  select(type, country_aoi, rp_lt0, rp_lt1, rp_empirical, f1, f1_baseline, f1_improvement)\n\n# Overall optimum applied to each country\ndf_overall_applied &lt;- df_or_grid |&gt;\n  filter(rp_lt0 == df_optimal_overall$rp_lt0, rp_lt1 == df_optimal_overall$rp_lt1) |&gt;\n  left_join(df_baseline_f1, by = \"country_aoi\") |&gt;\n  left_join(df_rp_overall |&gt; select(country_aoi, rp_empirical), by = \"country_aoi\") |&gt;\n  mutate(\n    f1_improvement = f1 - f1_baseline,\n    type = \"Overall optimal\"\n  ) |&gt;\n  select(type, country_aoi, rp_lt0, rp_lt1, rp_empirical, f1, f1_baseline, f1_improvement)\n\n# Get average baseline empirical RP\nrp_baseline_avg &lt;- mean(df_rp_baseline$rp_empirical)\n\nbind_rows(df_overall_applied, df_country_summary) |&gt;\n  mutate(type = factor(type, levels = c(\"Overall optimal\", \"Country-specific\"))) |&gt;\n  arrange(type, country_aoi) |&gt;\n  gt() |&gt;\n  fmt_number(columns = c(rp_lt0, rp_lt1, rp_empirical), decimals = 2) |&gt;\n  fmt_number(columns = c(f1, f1_baseline, f1_improvement), decimals = 3) |&gt;\n  cols_label(\n    type = \"Optimization\",\n    country_aoi = \"Country\",\n    rp_lt0 = \"RP (LT0)\",\n    rp_lt1 = \"RP (LT1)\",\n    rp_empirical = \"RP (OR)\",\n    f1 = \"F1\",\n    f1_baseline = \"Baseline F1\",\n    f1_improvement = \"Δ F1\"\n  ) |&gt;\n  tab_header(\n    title = \"OR-Trigger Threshold Optimization Results\",\n    subtitle = sprintf(\"Baseline = RP=4/4 (empirical OR RP ≈ %.1f)\", rp_baseline_avg)\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#d4edda\"),\n    locations = cells_body(columns = f1_improvement, rows = f1_improvement &gt; 0)\n  ) |&gt;\n  tab_row_group(\n    label = md(\"**Overall Optimal Thresholds**\"),\n    rows = type == \"Overall optimal\"\n  ) |&gt;\n  tab_row_group(\n    label = md(\"**Country-Specific Optimal Thresholds**\"),\n    rows = type == \"Country-specific\"\n  ) |&gt;\n  cols_hide(type)\n\n\n\n\n\n\n\n\nOR-Trigger Threshold Optimization Results\n\n\nBaseline = RP=4/4 (empirical OR RP ≈ 3.3)\n\n\nCountry\nRP (LT0)\nRP (LT1)\nRP (OR)\nF1\nBaseline F1\nΔ F1\n\n\n\n\nCountry-Specific Optimal Thresholds\n\n\nEl Salvador\n3.00\n5.75\n3.00\n0.615\n0.583\n0.032\n\n\nGuatemala\n3.00\n6.50\n3.00\n0.692\n0.640\n0.052\n\n\nHonduras\n4.00\n5.25\n3.75\n0.696\n0.640\n0.056\n\n\nOverall Optimal Thresholds\n\n\nEl Salvador\n3.50\n5.25\n3.46\n0.583\n0.583\n0.000\n\n\nGuatemala\n3.50\n5.25\n3.21\n0.640\n0.640\n0.000\n\n\nHonduras\n3.50\n5.25\n3.46\n0.667\n0.640\n0.027\n\n\n\n\n\n\n\nOptimal Thresholds Summary:\n\n\nCode\n# Print the key numbers\ncat(\"Overall Optimal (averaged across countries):\\n\")\n\n\nOverall Optimal (averaged across countries):\n\n\nCode\ncat(sprintf(\"  LT0 (Sep): RP = %.2f\\n\", df_optimal_overall$rp_lt0))\n\n\n  LT0 (Sep): RP = 3.50\n\n\nCode\ncat(sprintf(\"  LT1 (Aug): RP = %.2f\\n\", df_optimal_overall$rp_lt1))\n\n\n  LT1 (Aug): RP = 5.25\n\n\nCode\ncat(sprintf(\"  Empirical OR RP = %.2f (vs %.2f at RP=4/4)\\n\",\n            mean(df_rp_overall$rp_empirical), rp_baseline_avg))\n\n\n  Empirical OR RP = 3.38 (vs 3.30 at RP=4/4)\n\n\nCode\ncat(sprintf(\"  Mean F1 = %.3f (vs %.3f at RP=4/4)\\n\\n\", df_optimal_overall$f1_mean, f1_baseline))\n\n\n  Mean F1 = 0.630 (vs 0.621 at RP=4/4)\n\n\nCode\ncat(\"Country-Specific Optimal:\\n\")\n\n\nCountry-Specific Optimal:\n\n\nCode\ndf_optimal_country |&gt;\n  left_join(df_rp_country |&gt; select(country_aoi, rp_lt0, rp_lt1, rp_empirical),\n            by = c(\"country_aoi\", \"rp_lt0\", \"rp_lt1\")) |&gt;\n  arrange(country_aoi) |&gt;\n  rowwise() |&gt;\n  mutate(msg = sprintf(\"  %s: LT0=%.2f, LT1=%.2f, OR RP=%.2f (F1=%.3f)\",\n                       country_aoi, rp_lt0, rp_lt1, rp_empirical, f1)) |&gt;\n  pull(msg) |&gt;\n  cat(sep = \"\\n\")\n\n\n  El Salvador: LT0=3.00, LT1=5.75, OR RP=3.00 (F1=0.615)\n  Guatemala: LT0=3.00, LT1=6.50, OR RP=3.00 (F1=0.692)\n  Honduras: LT0=4.00, LT1=5.25, OR RP=3.75 (F1=0.696)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>CADC - Framework Skill Comparison</span>"
    ]
  },
  {
    "objectID": "07_multi_aoi_comparison.html#summary-table-1",
    "href": "07_multi_aoi_comparison.html#summary-table-1",
    "title": "7  CADC - Framework Skill Comparison",
    "section": "7.8 Summary Table",
    "text": "7.8 Summary Table\n\n\nCode\n# Create summary table for 1981 baseline with recommendations\ndf_summary &lt;- df_skill_all |&gt;\n  filter(baseline == \"1981-2024\") |&gt;\n  filter(\n    (window == \"primera\" & leadtime %in% 0:2) |\n    (window == \"postrera\" & leadtime %in% 0:3)\n  ) |&gt;\n  mutate(\n    score = sprintf(\"%.2f (%.2f)\", roc_auc, f1),\n    lt_label = paste0(\"LT\", leadtime),\n    # Format window labels with months\n    window = case_when(\n      window == \"primera\" ~ \"Primera (May-Aug)\",\n      window == \"postrera\" ~ \"Postrera (Sep-Nov)\"\n    ),\n    # Flag recommended combinations\n    recommended = case_when(\n      window == \"Primera (May-Aug)\" & leadtime %in% 0:2 ~ TRUE,\n      window == \"Postrera (Sep-Nov)\" & leadtime %in% 0:1 ~ TRUE,\n      TRUE ~ FALSE\n    )\n  ) |&gt;\n  select(country_aoi, window, lt_label, score, roc_auc, recommended) |&gt;\n  pivot_wider(names_from = lt_label, values_from = c(score, roc_auc, recommended))\n\n# Build GT table\ndf_summary |&gt;\n  select(country_aoi, window, starts_with(\"score_\")) |&gt;\n  rename_with(~str_remove(.x, \"score_\"), starts_with(\"score_\")) |&gt;\n  gt(groupname_col = \"window\") |&gt;\n  tab_header(\n    title = md(\"**SEAS5 Forecast Skill: ROC-AUC (F1)**\"),\n    subtitle = md(\"RP4 drought threshold | 1981-2024 baseline | *Recommended leadtimes highlighted*\")\n  ) |&gt;\n  cols_label(country_aoi = \"Country\") |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#c7e9c0\"),\n      cell_text(weight = \"bold\")\n    ),\n    locations = list(\n      cells_body(columns = c(LT0, LT1, LT2), rows = window == \"Primera (May-Aug)\"),\n      cells_body(columns = c(LT0, LT1), rows = window == \"Postrera (Sep-Nov)\")\n    )\n  ) |&gt;\n  tab_style(\n    style = cell_text(color = \"#6c757d\"),\n    locations = cells_body(columns = c(LT2, LT3), rows = window == \"Postrera (Sep-Nov)\")\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\", size = px(13)),\n    locations = cells_row_groups()\n  ) |&gt;\n  tab_style(\n    style = cell_borders(sides = \"bottom\", color = \"#dee2e6\", weight = px(2)),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_footnote(\n    footnote = md(\"**Green** = recommended for operational use; Gray = use with caution\"),\n    locations = cells_title(groups = \"subtitle\")\n  ) |&gt;\n  tab_footnote(\n    footnote = \"Format: ROC-AUC (F1). ROC-AUC &gt; 0.7 = good; &gt; 0.8 = excellent\",\n    locations = cells_column_labels(columns = LT0)\n  ) |&gt;\n  tab_source_note(\n    source_note = md(\"**Recommendation**: Use LT0-2 for Primera, LT0-1 for Postrera. Honduras shows strongest Postrera skill.\")\n  ) |&gt;\n  tab_options(\n    table.font.size = px(14),\n    heading.title.font.size = px(18),\n    heading.subtitle.font.size = px(13),\n    column_labels.font.weight = \"bold\",\n    row_group.background.color = \"#f8f9fa\",\n    table.border.top.style = \"solid\",\n    table.border.top.width = px(2),\n    table.border.top.color = \"#1a9850\"\n  )\n\n\n\n\nSEAS5 Forecast Skill Summary\n  \n    \n      SEAS5 Forecast Skill: ROC-AUC (F1)\n    \n    \n      RP4 drought threshold | 1981-2024 baseline | Recommended leadtimes highlighted1\n    \n    \n      Country\n      LT02\n      LT1\n      LT2\n      LT3\n    \n  \n  \n    \n      Postrera (Sep-Nov)\n    \n    El Salvador\n0.86 (0.55)\n0.76 (0.45)\n0.63 (0.36)\n0.63 (0.45)\n    Guatemala\n0.80 (0.64)\n0.72 (0.55)\n0.66 (0.36)\n0.65 (0.45)\n    Honduras\n0.87 (0.64)\n0.79 (0.45)\n0.77 (0.55)\n0.74 (0.36)\n    \n      Primera (May-Aug)\n    \n    El Salvador\n0.72 (0.36)\n0.73 (0.45)\n0.67 (0.45)\nNA\n    Guatemala\n0.81 (0.55)\n0.85 (0.64)\n0.79 (0.64)\nNA\n    Honduras\n0.75 (0.45)\n0.75 (0.45)\n0.71 (0.55)\nNA\n  \n  \n    \n      Recommendation: Use LT0-2 for Primera, LT0-1 for Postrera. Honduras shows strongest Postrera skill.\n    \n  \n  \n    \n      1 Green = recommended for operational use; Gray = use with caution\n    \n    \n      2 Format: ROC-AUC (F1). ROC-AUC &gt; 0.7 = good; &gt; 0.8 = excellent",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>CADC - Framework Skill Comparison</span>"
    ]
  },
  {
    "objectID": "07_multi_aoi_comparison.html#recommendations",
    "href": "07_multi_aoi_comparison.html#recommendations",
    "title": "7  CADC - Framework Skill Comparison",
    "section": "7.9 Recommendations",
    "text": "7.9 Recommendations\nBased on this multi-AOI assessment:\n\nUse the 1981-2024 baseline for threshold calculation. The longer record provides more stable estimates and reveals skill that is masked by the shorter baseline.\nPrimera forecasts are reliable across all three countries at LT0-2. Any of these leadtimes can be used operationally.\nPostrera: Use LT0 and LT1:\n\nLT1 (August-issued): Primary forecast for preparedness planning (~1 month lead time, ROC-AUC 0.72-0.79)\nLT0 (September-issued): Confirmation/update as season begins (ROC-AUC 0.80-0.87)\nLT2-3 show weaker skill and are less reliable for operational decisions\n\nRegional considerations:\n\nHonduras Postrera forecasts are most reliable across all leadtimes\nGuatemala Postrera should be interpreted with more caution\nEl Salvador shows strong improvement with longer baseline\n\nFor Guatemala specifically: Given the weaker Postrera skill, consider:\n\nEmphasizing Primera forecasts for anticipatory action\nUsing Postrera forecasts as one input among several (ENSO state, VHI, etc.)\nRelying primarily on LT0-1 for Postrera decisions",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>CADC - Framework Skill Comparison</span>"
    ]
  },
  {
    "objectID": "appendix_technical_notes.html",
    "href": "appendix_technical_notes.html",
    "title": "Appendix A — Technical Notes",
    "section": "",
    "text": "A.1 Drought Definition\nDrought is defined using a return period of 4 years (RP4):",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Technical Notes</span>"
    ]
  },
  {
    "objectID": "appendix_technical_notes.html#drought-definition",
    "href": "appendix_technical_notes.html#drought-definition",
    "title": "Appendix A — Technical Notes",
    "section": "",
    "text": "Approximately 6 drought years expected over the 25-year baseline (2000-2024)\nThreshold calculated separately for each forecast model and observation source\nThis aligns with typical anticipatory action trigger frameworks",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Technical Notes</span>"
    ]
  },
  {
    "objectID": "appendix_technical_notes.html#observation-sources",
    "href": "appendix_technical_notes.html#observation-sources",
    "title": "Appendix A — Technical Notes",
    "section": "A.2 Observation Sources",
    "text": "A.2 Observation Sources\n\n\n\n\n\n\n\n\n\nSource\nType\nCoverage\nUsed For\n\n\n\n\nERA5\nGlobal reanalysis\nGlobal\nSEAS5 native validation\n\n\nENACTS\nSatellite + stations\nRegional\nOperational standard for Guatemala\n\n\nCHIRPS\nSatellite + stations\nQuasi-global\nIndependent validation",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Technical Notes</span>"
    ]
  },
  {
    "objectID": "appendix_technical_notes.html#key-metrics-glossary",
    "href": "appendix_technical_notes.html#key-metrics-glossary",
    "title": "Appendix A — Technical Notes",
    "section": "A.3 Key Metrics Glossary",
    "text": "A.3 Key Metrics Glossary\n\nF1 Score: Harmonic mean of precision and recall. Balances false alarms and missed events.\nROC-AUC: Area under the receiver operating characteristic curve. Measures discrimination skill - can the forecast distinguish drought from non-drought years?\nSpearman correlation: Rank-based correlation. Do wetter forecasts correspond to wetter observations?\nInter-leadtime correlation: Do consecutive leadtimes agree with each other? A data quality check.\nCoefficient of Variation: How much do forecasts vary relative to their mean? Forecasts that barely vary contain no useful signal.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Technical Notes</span>"
    ]
  },
  {
    "objectID": "appendix_technical_notes.html#archived-exploring-larger-spatial-scales",
    "href": "appendix_technical_notes.html#archived-exploring-larger-spatial-scales",
    "title": "Appendix A — Technical Notes",
    "section": "A.4 Archived: Exploring Larger Spatial Scales",
    "text": "A.4 Archived: Exploring Larger Spatial Scales\n\n\n\n\n\n\nNoteHistorical Context\n\n\n\nThis analysis was initiated early in the research process, following the binary classification metrics assessment. The hypothesis was that seasonal forecasts might perform better when aggregated over larger spatial scales than administrative boundaries. However, as the investigation progressed through continuous metrics, multi-source validation, and internal consistency diagnostics, the root cause of poor Postrera skill became clear: the INSIVUMEH forecasts themselves lack signal, regardless of spatial aggregation. This section is retained for completeness but was not pursued further.\n\n\nBoth SEAS5 and INSIVUMEH may perform poorly at the admin-1 (Chiquimula) scale because seasonal forecasts typically have better skill at larger spatial scales. HydroBASINS provides standardized watershed boundaries at multiple levels that could serve as alternative analysis units.\n\n\nLoad HydroBASINS and Chiquimula boundary\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(cumulus)\n\n# Disable s2 for this section (HydroBASINS has some geometry issues with s2)\nsf_use_s2(FALSE)\n\n# Load Guatemala boundaries\nsf_gtm_adm0 &lt;- cumulus::download_fieldmaps_sf(iso3 = \"GTM\", layer = \"gtm_adm0\")$gtm_adm0\nsf_gtm_adm1 &lt;- cumulus::download_fieldmaps_sf(iso3 = \"GTM\", layer = \"gtm_adm1\")$gtm_adm1 |&gt;\n janitor::clean_names()\nsf_chiquimula &lt;- sf_gtm_adm1 |&gt; filter(adm1_pcode == \"GT20\")\n\n# Path to HydroBASINS data\nhybas_dir &lt;- \"../../data/hybas_na_lev01-12_v1c\"\n\n# Load levels 5 and 6\nsf_hybas_5 &lt;- st_read(file.path(hybas_dir, \"hybas_na_lev05_v1c.shp\"), quiet = TRUE)\nsf_hybas_6 &lt;- st_read(file.path(hybas_dir, \"hybas_na_lev06_v1c.shp\"), quiet = TRUE)\n\n# Ensure same CRS\nsf_chiquimula &lt;- st_transform(sf_chiquimula, st_crs(sf_hybas_5))\n\n# Get bounding box of Chiquimula with buffer for context\nchiq_bbox &lt;- st_bbox(sf_chiquimula)\nbbox_buffer &lt;- 1  # degrees\n\n# Create bbox polygon for filtering\nbbox_poly &lt;- st_as_sfc(st_bbox(c(\n  xmin = as.numeric(chiq_bbox[\"xmin\"]) - bbox_buffer,\n  xmax = as.numeric(chiq_bbox[\"xmax\"]) + bbox_buffer,\n  ymin = as.numeric(chiq_bbox[\"ymin\"]) - bbox_buffer,\n  ymax = as.numeric(chiq_bbox[\"ymax\"]) + bbox_buffer\n), crs = st_crs(sf_hybas_5)))\n\n# Filter basins to region around Chiquimula using intersection\nsf_hybas_5_region &lt;- sf_hybas_5[st_intersects(sf_hybas_5, bbox_poly, sparse = FALSE)[,1], ]\nsf_hybas_6_region &lt;- sf_hybas_6[st_intersects(sf_hybas_6, bbox_poly, sparse = FALSE)[,1], ]\n\n# Find basins that intersect Chiquimula\nsf_hybas_5_intersect &lt;- sf_hybas_5_region[st_intersects(sf_hybas_5_region, sf_chiquimula, sparse = FALSE)[,1], ]\nsf_hybas_6_intersect &lt;- sf_hybas_6_region[st_intersects(sf_hybas_6_region, sf_chiquimula, sparse = FALSE)[,1], ]\n\ncat(\"Level 5 basins intersecting Chiquimula:\", nrow(sf_hybas_5_intersect), \"\\n\")\n\n\nLevel 5 basins intersecting Chiquimula: 2 \n\n\nLoad HydroBASINS and Chiquimula boundary\ncat(\"Level 6 basins intersecting Chiquimula:\", nrow(sf_hybas_6_intersect), \"\\n\")\n\n\nLevel 6 basins intersecting Chiquimula: 2 \n\n\n\n\nMap comparing basin levels 5 and 6 over Chiquimula\nlibrary(patchwork)\n\n# Calculate areas for labels\nsf_hybas_5_intersect &lt;- sf_hybas_5_intersect |&gt;\n  mutate(area_km2 = as.numeric(st_area(geometry)) / 1e6)\n\nsf_hybas_6_intersect &lt;- sf_hybas_6_intersect |&gt;\n  mutate(area_km2 = as.numeric(st_area(geometry)) / 1e6)\n\nchiq_area &lt;- as.numeric(st_area(sf_chiquimula)) / 1e6\n\n# Transform admin0 to match CRS\nsf_gtm_adm0_t &lt;- st_transform(sf_gtm_adm0, st_crs(sf_hybas_5))\n\n# Create base plot function\ncreate_basin_map &lt;- function(sf_basins, level, sf_admin, sf_country) {\n\n  # Get centroids for labels\n  basin_centroids &lt;- st_centroid(sf_basins) |&gt;\n    mutate(\n      x = st_coordinates(geometry)[,1],\n      y = st_coordinates(geometry)[,2]\n    )\n\n  ggplot() +\n    geom_sf(data = sf_country, fill = \"grey95\", color = \"grey50\", linewidth = 0.3) +\n    geom_sf(data = sf_basins, aes(fill = area_km2), color = \"darkblue\", linewidth = 0.8) +\n    geom_sf(data = sf_admin, fill = NA, color = \"red\", linewidth = 1.2) +\n    geom_sf_text(\n      data = basin_centroids,\n      aes(label = paste0(round(area_km2), \" km²\")),\n      size = 3, fontface = \"bold\", color = \"white\"\n    ) +\n    scale_fill_viridis_c(option = \"mako\", direction = -1, name = \"Area (km²)\") +\n    labs(\n      title = paste0(\"HydroBASINS Level \", level),\n      subtitle = paste0(nrow(sf_basins), \" basin(s) intersecting Chiquimula\"),\n      caption = paste0(\"Red outline = Chiquimula (\", round(chiq_area), \" km²)\")\n    ) +\n    theme_minimal() +\n    theme(\n      legend.position = \"bottom\",\n      plot.caption = element_text(color = \"red\", face = \"bold\")\n    )\n}\n\np_lev5 &lt;- create_basin_map(sf_hybas_5_intersect, 5, sf_chiquimula, sf_gtm_adm0_t)\np_lev6 &lt;- create_basin_map(sf_hybas_6_intersect, 6, sf_chiquimula, sf_gtm_adm0_t)\n\np_lev5 + p_lev6 +\n  plot_annotation(\n    title = \"HydroBASINS Comparison: Potential Analysis Units\",\n    subtitle = \"Comparing basin scales that could provide better forecast skill than admin-1 boundaries\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Summary comparison\ntibble(\n  `Spatial Unit` = c(\n    \"Chiquimula (Admin-1)\",\n    paste0(\"Level 5 (\", nrow(sf_hybas_5_intersect), \" basin\", ifelse(nrow(sf_hybas_5_intersect) &gt; 1, \"s\", \"\"), \")\"),\n    paste0(\"Level 6 (\", nrow(sf_hybas_6_intersect), \" basin\", ifelse(nrow(sf_hybas_6_intersect) &gt; 1, \"s\", \"\"), \")\")\n  ),\n  `Total Area (km²)` = c(\n    round(chiq_area),\n    round(sum(sf_hybas_5_intersect$area_km2)),\n    round(sum(sf_hybas_6_intersect$area_km2))\n  ),\n  `Mean Basin Area (km²)` = c(\n    round(chiq_area),\n    round(mean(sf_hybas_5_intersect$area_km2)),\n    round(mean(sf_hybas_6_intersect$area_km2))\n  ),\n  Notes = c(\n    \"Current analysis unit\",\n    \"Larger basins - may have better forecast skill\",\n    \"Medium basins - balance of skill and resolution\"\n  )\n) |&gt;\n  knitr::kable(caption = \"Comparison of spatial units for forecast skill assessment\")\n\n\n\nComparison of spatial units for forecast skill assessment\n\n\n\n\n\n\n\n\nSpatial Unit\nTotal Area (km²)\nMean Basin Area (km²)\nNotes\n\n\n\n\nChiquimula (Admin-1)\n2415\n2415\nCurrent analysis unit\n\n\nLevel 5 (2 basins)\n225446\n112723\nLarger basins - may have better forecast skill\n\n\nLevel 6 (2 basins)\n34487\n17243\nMedium basins - balance of skill and resolution\n\n\n\n\n\n\nA.4.1 Original Interpretation\n\nLevel 5 provides larger drainage basins that may better match the spatial resolution at which seasonal forecasts have skill\nLevel 6 offers a middle ground between admin boundaries and larger regional basins\n\nThis avenue was not pursued further after the internal consistency analysis (Chapter 5) revealed that the INSIVUMEH Postrera forecasts lack meaningful signal at any spatial scale.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Technical Notes</span>"
    ]
  }
]