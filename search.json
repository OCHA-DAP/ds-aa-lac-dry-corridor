[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Seasonal Forecast Skill Assessment",
    "section": "",
    "text": "Key Findings\nThis book evaluates seasonal precipitation forecasts for drought prediction in Chiquimula, Guatemala. We compare SEAS5 (ECMWF global) against INSIVUMEH (Guatemala national service) for two agricultural seasons: Primera (May-Aug) and Postrera (Sep-Nov).",
    "crumbs": [
      "Key Findings"
    ]
  },
  {
    "objectID": "index.html#decision-steps",
    "href": "index.html#decision-steps",
    "title": "Seasonal Forecast Skill Assessment",
    "section": "Decision Steps",
    "text": "Decision Steps\n\n\n\n\n\n\n\n\nStep\nQuestion\nFinding\n\n\n\n\n1. Binary Metrics\nDo forecasts beat random guessing?\nPrimera yes (SEAS5 wins), Postrera inconclusive\n\n\n2. Continuous Metrics\nDoes skill hold with different metrics?\nPrimera confirmed (SEAS5 wins), Postrera ambiguous\n\n\n3. CHIRPS Tie-Breaker\nIs skill robust across observation sources? Does a 3rd data source clear Postrera ambiguity?\nPrimera robust, Postrera still ambiguous\n\n\n4. Temporal Drift\nAre trends/drift affecting skill estimates?\nMild drift, unlikely to be the significant factor causing poor postrera skill\n\n\n5. Internal Consistency\nAre forecasts internally coherent?\nINSIVUMEH Postrera forecasts completely internally incoherent - lack signal",
    "crumbs": [
      "Key Findings"
    ]
  },
  {
    "objectID": "index.html#recommendations",
    "href": "index.html#recommendations",
    "title": "Seasonal Forecast Skill Assessment",
    "section": "Recommendations",
    "text": "Recommendations\n\n\n\n\n\n\n\n\n\nSeason\nForecast\nConfidence\nRationale\n\n\n\n\nPrimera\nSEAS5\nHigh\nClear skill advantage across all metrics and observation sources\n\n\nPostrera\nSEAS5\nLow\nAll sources show low “skill.” However, INSIVUMEH forecasts show no internal consistency and barely vary from climatology\n\n\n\n\n\n\n\n\n\nNoteAbstract\n\n\n\n\n\nThis evaluation informs forecast selection for anticipatory action triggers in Chiquimula, Guatemala (2026). We compared ECMWF SEAS5 and INSIVUMEH (CFSv2, CCSM4, CESM1) seasonal precipitation forecasts using 25 years (2000–2024) of hindcasts validated against three independent observation sources (ERA5, ENACTS, CHIRPS). For Primera (May–August), SEAS5 is recommended based on consistently stronger performance across both binary and continuous skill metrics (F1, ROC-AUC, Spearman correlation), with ROC-AUC values of 0.87–0.88 at 1–2 month lead times that remain robust across all observation sources. For Postrera (September–November), SEAS5 is recommended by elimination: INSIVUMEH forecasts exhibit near-zero inter-leadtime correlations (r = -0.1 to 0.2) and coefficient of variation 3–7% compared to 23% in observations, indicating negligible predictive signal. The counterintuitive pattern of higher apparent skill at longer lead times (LT3 &gt; LT1) for INSIVUMEH Postrera is attributable to sampling noise. No model demonstrates reliable Postrera skill (AUC consistently above 0.7), suggesting monitoring-based triggers may be more appropriate than forecast-only approaches for this season.",
    "crumbs": [
      "Key Findings"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Intro",
    "section": "",
    "text": "1.1 The Decision/Question\nWhich seasonal forecast should Guatemala’s anticipatory action framework use for drought prediction in Chiquimula? Two options are available:\nThis book documents the reserch to answer this question for two agricultural seasons:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro</span>"
    ]
  },
  {
    "objectID": "intro.html#the-decisionquestion",
    "href": "intro.html#the-decisionquestion",
    "title": "1  Intro",
    "section": "",
    "text": "SEAS5: ECMWF’s global seasonal forecasting system\nINSIVUMEH: Regional forecasts from Guatemala’s national meteorological service (CFSv2, CCSM4, and CESM1 models)\n\n\n\n\n\nSeason\nMonths\nDescription\n\n\n\n\nPrimera\nMay-August (MJJA)\nFirst maize planting\n\n\nPostrera\nSeptember-November (SON)\nSecond maize planting",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro</span>"
    ]
  },
  {
    "objectID": "intro.html#research-progression",
    "href": "intro.html#research-progression",
    "title": "1  Intro",
    "section": "1.2 Research Progression",
    "text": "1.2 Research Progression\n\n1.2.1 Step 1: Start with Standard Metrics\nThe analysis began with binary classification metrics (Chapter 1) - the traditional approach where forecasts are scored as hits or misses against a drought threshold (return period of 4 years).\nPrimera results were encouraging: SEAS5 showed F1 scores around 50-60%, significantly better than random guessing. Several model-leadtime combinations beat the 90th percentile of bootstrap random simulations.\nPostrera was troubling: No model showed statistically significant skill. F1 scores hovered near random guessing levels. Different models won at different leadtimes with no clear pattern.\nRemaining question: Are the Postrera results genuinely inconclusive, or are binary metrics too noisy with only ~6 drought events in 25 years?\n\n\n1.2.2 Step 2: Try Continuous Metrics\nChapter 2 introduced continuous metrics that don’t require threshold choices: Spearman correlation, RMSE, and ROC-AUC. These provide partial credit for near-misses and are more stable with small samples.\nPrimera confirmation: SEAS5 dominated across all continuous metrics. AUC values around 0.88 - meaning 88% probability of correctly ranking a drought year below a non-drought year. The signal was real.\nPostrera remained ambiguous: Different metrics favored different models:\n\nSpearman: SEAS5 best at LT1\nRMSE: CCSM4 best\nAUC: SEAS5 at LT1, CCSM4 at LT2\n\nNo model showed AUC consistently above 0.7 (general threshold for “acceptable” discrimination skill).\nRemaining question: Are these results sensitive to which observation dataset we validate against? Would a different “ground truth” give the same answer?\n\n\n1.2.3 Step 3: Bring in a Third Opinion\nChapter 3 introduced CHIRPS as an independent observation source alongside ERA5 and ENACTS. If forecasts perform well against multiple observation sources, the skill assessment is more credible.\nPrimera verdict: Skill was robust across all three observation sources. SEAS5 won most comparisons.\nPostrera raised a red flag: CCSM4 showed better skill at longer leadtimes (LT2-3) than shorter ones (LT1). This is backwards - forecast skill should degrade with increasing leadtime, not improve. The pattern suggested we might be seeing noise rather than genuine skill.\nRemaining question: What’s causing the inverted skill-leadtime relationship for Postrera?\n\n\n1.2.4 Step 4: Check for Drift\nChapter 4 investigated whether temporal trends in forecasts or observations could explain the strange patterns.\nFindings:\n\nENACTS showed a drying trend for Primera, but other sources showed similar patterns\nForecast error drift was modest (~5-8 mm/year)\nSplit-half stability was the key finding: Primera correlations were stable between 2000-2012 and 2013-2024, but Postrera correlations were wildly different between periods\n\nThe drift analysis didn’t explain the inverted skill pattern.\n\n\n1.2.5 Step 5: The Decider - Forecast Internal Consistency\nChapter 5 applied internal consistency diagnostics - asking not “do forecasts predict observations?” but “do forecasts behave like forecasts?”\nInter-leadtime correlations: For a well-behaved forecast, LT1 and LT2 predictions for the same target season should correlate - they’re trying to predict the same thing.\n\nSEAS5: LT1-LT2 correlations of 0.6-0.9 (expected)\nINSIVUMEH Primera: Moderate correlations of 0.3-0.7 (acceptable)\nINSIVUMEH Postrera: Near-zero or negative correlations - when LT1 predicts wet, LT2 might predict dry for the same season\n\nCoefficient of Variation (CV): Forecasts should show interannual variability approaching observed variability.\n\nObserved Postrera CV: ~23%\nSEAS5 Postrera CV: ~15% (reasonable)\nINSIVUMEH Postrera CV: 3-7% - forecasts barely varied from climatology\n\nThis explained everything. INSIVUMEH Postrera forecasts weren’t failing skill tests because Postrera is hard to predict - they were failing because the forecasts show much less interannual variability than the observed rainfall. The apparent skill differences between leadtimes were artifacts of forecasts that vary little from year to year.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro</span>"
    ]
  },
  {
    "objectID": "intro.html#final-recommendations",
    "href": "intro.html#final-recommendations",
    "title": "1  Intro",
    "section": "1.3 Final Recommendations",
    "text": "1.3 Final Recommendations\n\n\n\n\n\n\n\n\n\nSeason\nRecommended\nConfidence\nRationale\n\n\n\n\nPrimera\nSEAS5\nHigh\nClear skill advantage across all metrics and observation sources\n\n\nPostrera\nSEAS5\nLow\nSEAS5 shows better internal consistency, though skill and consistency remains limited for all models\n\n\n\n\n1.3.1 The Postrera Problem\nFor Postrera, the recommendation is SEAS5 by elimination rather than demonstrated skill. INSIVUMEH Postrera forecasts show much lower interannual variability than observed rainfall and weak consistency between leadtimes—suggesting limited predictive signal for this season.\nSEAS5 at least behaves like a forecast: different leadtimes agree with each other, and it shows realistic interannual variability. Whether this translates to operational skill for Postrera remains uncertain.\nOperational implications for Postrera: - Consider monitoring-based triggers (observed rainfall deficits, VHI) rather than relying solely on seasonal forecasts - Acknowledge limited predictability in operational communications - Supplementing forecast-based triggers with monitoring-based indicators (e.g., observed rainfall deficits or VHI during early Postrera)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro</span>"
    ]
  },
  {
    "objectID": "intro.html#book-structure",
    "href": "intro.html#book-structure",
    "title": "1  Intro",
    "section": "1.4 Book Structure",
    "text": "1.4 Book Structure\nThe chapters document each step of this journey:\n\n\n\n\n\n\n\n\nStep\nQuestion\nFinding\n\n\n\n\n1. Binary Metrics\nDo forecasts beat random guessing?\nPrimera yes (SEAS5 wins), Postrera inconclusive\n\n\n2. Continuous Metrics\nDoes skill hold with different metrics?\nPrimera confirmed (SEAS5 wins), Postrera ambiguous\n\n\n3. CHIRPS Tie-Breaker\nIs skill robust across observation sources?\nPrimera robust, Postrera still ambiguous\n\n\n4. Temporal Drift\nAre trends/drift affecting skill estimates?\nMild drift, unlikely to explain poor Postrera skill\n\n\n5. Internal Consistency\nAre forecasts internally coherent?\nINSIVUMEH Postrera forecasts lack signal",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html",
    "href": "01_binary_metrics.html",
    "title": "2  Binary Classification Metrics",
    "section": "",
    "text": "3 Introduction\nThis chapter evaluates forecast skill using binary classification metrics - the traditional approach where forecasts are judged as either “correct” or “incorrect” based on whether they cross a drought threshold.\nThis analysis compares the skill of INSIVUMEH (regional) and SEAS5 (global) seasonal precipitation forecasts for detecting drought conditions in Chiquimula, Guatemala. We evaluate forecasts for two agricultural seasons:\nWe use two comparison frameworks:\nDrought is defined using a return period of 4 years (RP4), meaning approximately 6 drought years are expected over the 25-year baseline (2000-2024).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#native-comparison",
    "href": "01_binary_metrics.html#native-comparison",
    "title": "2  Binary Classification Metrics",
    "section": "4.1 Native Comparison",
    "text": "4.1 Native Comparison\nSEAS5 validated against ERA5; INSIVUMEH models validated against ENACTS.\n\n\nCode\ncreate_f1_heatmap(\n  df_skill_native,\n  title = \"F1 Score: Native Observation Sources\",\n  subtitle = \"Black outline = best performer per leadtime\",\n  caption = paste0(\"SEAS5 vs ERA5, INSIVUMEH vs ENACTS (\", aoi_name, \")\")\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#enacts-only-comparison",
    "href": "01_binary_metrics.html#enacts-only-comparison",
    "title": "2  Binary Classification Metrics",
    "section": "4.2 ENACTS-Only Comparison",
    "text": "4.2 ENACTS-Only Comparison\nAll models validated against ENACTS for direct comparison.\n\n\nCode\ncreate_f1_heatmap(\n  df_skill_enacts,\n  title = \"F1 Score: All Models vs ENACTS\",\n  subtitle = \"Black outline = best performer per leadtime\",\n  caption = paste0(\"All forecasts vs ENACTS (\", aoi_name, \")\")\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#primera-season",
    "href": "01_binary_metrics.html#primera-season",
    "title": "2  Binary Classification Metrics",
    "section": "5.1 Primera Season",
    "text": "5.1 Primera Season\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\nplot_yearlt_heatmap(df_yearlt_native, \"primera\", \"native\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_yearlt_heatmap(df_yearlt_enacts, \"primera\", \"enacts_only\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#postrera-season",
    "href": "01_binary_metrics.html#postrera-season",
    "title": "2  Binary Classification Metrics",
    "section": "5.2 Postrera Season",
    "text": "5.2 Postrera Season\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\nplot_yearlt_heatmap(df_yearlt_native, \"postrera\", \"native\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_yearlt_heatmap(df_yearlt_enacts, \"postrera\", \"enacts_only\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#native-comparison-3",
    "href": "01_binary_metrics.html#native-comparison-3",
    "title": "2  Binary Classification Metrics",
    "section": "6.1 Native Comparison",
    "text": "6.1 Native Comparison\n\n\nCode\ncreate_corr_heatmap(\n  df_corr_native,\n  title = \"Anomaly Correlation: Native Observation Sources\",\n  subtitle = \"Black outline = best performer. Higher = better captures interannual variability.\",\n  caption = \"SEAS5 vs ERA5, INSIVUMEH vs ENACTS\"\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#enacts-only-comparison-1",
    "href": "01_binary_metrics.html#enacts-only-comparison-1",
    "title": "2  Binary Classification Metrics",
    "section": "6.2 ENACTS-Only Comparison",
    "text": "6.2 ENACTS-Only Comparison\n\n\nCode\ncreate_corr_heatmap(\n  df_corr_enacts,\n  title = \"Anomaly Correlation: All Models vs ENACTS\",\n  subtitle = \"Black outline = best performer. Higher = better captures interannual variability.\",\n  caption = \"All models vs ENACTS\"\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#scatterplots",
    "href": "01_binary_metrics.html#scatterplots",
    "title": "2  Binary Classification Metrics",
    "section": "6.3 Scatterplots",
    "text": "6.3 Scatterplots\nScatterplots show the relationship between forecast and observed anomalies for each model-leadtime combination.\n\n\nScatterplot function with R² and p-value\ncreate_corr_scatter &lt;- function(df_anom, window_name, title_suffix) {\n  df_plot &lt;- df_anom |&gt;\n    filter(window == window_name) |&gt;\n    mutate(\n      model_short = str_remove(forecast_source, \"INSIVUMEH_\"),\n      facet_label = paste0(model_short, \" (LT\", leadtime, \")\")\n    )\n\n  # Calculate r and p-value for each facet\n  df_stats &lt;- df_plot |&gt;\n    group_by(forecast_source, leadtime, facet_label) |&gt;\n    summarise(\n      r = cor(fcst_anom, obs_anom, use = \"complete.obs\"),\n      p_value = cor.test(fcst_anom, obs_anom)$p.value,\n      .groups = \"drop\"\n    ) |&gt;\n    mutate(\n      label = paste0(\"r = \", sprintf(\"%.2f\", r), \"\\n\",\n                     \"p = \", ifelse(p_value &lt; 0.001, \"&lt;0.001\", sprintf(\"%.3f\", p_value)))\n    )\n\n  ggplot(df_plot, aes(x = obs_anom, y = fcst_anom)) +\n    geom_hline(yintercept = 0, color = \"grey70\", linetype = \"dashed\") +\n    geom_vline(xintercept = 0, color = \"grey70\", linetype = \"dashed\") +\n    geom_smooth(method = \"lm\", se = TRUE, color = \"#007CE1\", fill = \"#007CE1\", alpha = 0.2) +\n    geom_point(alpha = 0.4, size = 2) +\n    geom_text(\n      data = df_stats,\n      aes(x = Inf, y = Inf, label = label),\n      hjust = 1.1, vjust = 1.3, size = 3, color = \"grey20\", fontface = \"bold\",\n      inherit.aes = FALSE\n    ) +\n    facet_wrap(~facet_label, scales = \"free\") +\n    labs(\n      title = paste0(\"Forecast vs Observed Anomalies: \", str_to_title(window_name)),\n      subtitle = title_suffix,\n      x = \"Observed Anomaly (z-score)\",\n      y = \"Forecast Anomaly (z-score)\"\n    ) +\n    theme_minimal() +\n    theme(\n      strip.text = element_text(face = \"bold\", size = 9),\n      panel.grid.minor = element_blank()\n    )\n}\n\n\n\n6.3.1 Primera Season\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\ncreate_corr_scatter(df_anom_native, \"primera\", \"SEAS5 vs ERA5, INSIVUMEH vs ENACTS\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_corr_scatter(df_anom_enacts, \"primera\", \"All models vs ENACTS\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.3.2 Postrera Season\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\ncreate_corr_scatter(df_anom_native, \"postrera\", \"SEAS5 vs ERA5, INSIVUMEH vs ENACTS\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_corr_scatter(df_anom_enacts, \"postrera\", \"All models vs ENACTS\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#primera-season-2",
    "href": "01_binary_metrics.html#primera-season-2",
    "title": "2  Binary Classification Metrics",
    "section": "7.1 Primera Season",
    "text": "7.1 Primera Season\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\ncreate_random_detail_plot(df_comparison_native, \"primera\", random_percentiles, \"native\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_random_detail_plot(df_comparison_enacts, \"primera\", random_percentiles, \"enacts_only\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#postrera-season-2",
    "href": "01_binary_metrics.html#postrera-season-2",
    "title": "2  Binary Classification Metrics",
    "section": "7.2 Postrera Season",
    "text": "7.2 Postrera Season\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\ncreate_random_detail_plot(df_comparison_native, \"postrera\", random_percentiles, \"native\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_random_detail_plot(df_comparison_enacts, \"postrera\", random_percentiles, \"enacts_only\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "01_binary_metrics.html#summary-by-model",
    "href": "01_binary_metrics.html#summary-by-model",
    "title": "2  Binary Classification Metrics",
    "section": "7.3 Summary by Model",
    "text": "7.3 Summary by Model\n\nNative ComparisonENACTS-Only\n\n\n\n\nCode\ncreate_random_summary_plot(df_comparison_native, random_percentiles, \"native\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_random_summary_plot(df_comparison_enacts, random_percentiles, \"enacts_only\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Classification Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html",
    "href": "02_continuous_metrics.html",
    "title": "3  Continuous Metrics",
    "section": "",
    "text": "3.1 Beyond Binary: A More Nuanced View\nThe binary metrics in Chapter 2 (F1 score, precision, recall) provide one lens on forecast skill, but they can be noisy - especially with only ~25 years of data and ~6 drought events. A single misclassified year can swing F1 scores dramatically.\nConsider an analogy: judging a temperature forecast as “wrong” for predicting 33°F when it was actually 31°F. The forecast told you to expect cold, even if it missed the freezing threshold. Binary metrics don’t distinguish between a near-miss and being completely off.\nThis chapter introduces continuous metrics that provide a more nuanced picture:\nOf these, ROC-AUC is particularly interpretable for operational decisions. An AUC of 0.75 means: “If you randomly select a drought year and a non-drought year, there’s a 75% chance the forecast correctly identifies which is drier.” This directly answers whether the forecast can discriminate drought from non-drought conditions - the core operational question.\nSetup: Libraries and data loading\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(cumulus)\nlibrary(yardstick)\nlibrary(gghdx)\ngghdx()\n\nbox::use(../../R/enacts)\nbox::use(../../R/seas5)\n\nset.seed(42)\n\n# Configuration\nBASELINE_START &lt;- 2000\nBASELINE_END &lt;- 2024\nPRIMERA_MONTHS &lt;- 5:8\nPOSTRERA_MONTHS &lt;- 9:11\nPRIMERA_ISSUED_MONTHS &lt;- c(3, 4, 5)\nPOSTRERA_ISSUED_MONTHS &lt;- c(6, 7, 8, 9)\n\n# Helper function\ncalc_rp_threshold &lt;- function(x, rp_target = 4, direction = -1) {\n  x &lt;- x[!is.na(x)]\n  n &lt;- length(x)\n  if (n &lt; 3) return(NA_real_)\n  ranks &lt;- rank(x * -direction, ties.method = \"average\")\n  rp &lt;- (n + 1) / ranks\n  approx(rp, x, xout = rp_target, rule = 2)$y\n}\nLoad forecast and observation data\n# Load forecasts\ndf_insiv &lt;- cumulus::blob_read(\n  name = \"ds-aa-lac-dry-corridor/data/processed/insivumeh_special/insivumeh_special_models_zonal_seasonal_chiquimula.parquet\",\n  container = \"projects\"\n)\ndf_enacts &lt;- enacts$load_enacts_seasonal(\"chiquimula\")\ndf_seas5 &lt;- seas5$load_seas5_seasonal()\n\n# Load ERA5\ncon &lt;- pg_con()\ndf_era5_raw &lt;- tbl(con, \"era5\") |&gt;\n  filter(pcode == \"GT20\") |&gt;\n  collect() |&gt;\n  mutate(\n    year = year(valid_date),\n    month = month(valid_date),\n    mean = mean * days_in_month(valid_date)\n  )\nDBI::dbDisconnect(con)\n\n# Aggregate ERA5 to seasonal\naggregate_obs_seasonal &lt;- function(df, window_name) {\n  months &lt;- if (window_name == \"primera\") PRIMERA_MONTHS else POSTRERA_MONTHS\n  df |&gt;\n    filter(month %in% months) |&gt;\n    group_by(year) |&gt;\n    summarise(obs_mm = sum(value, na.rm = TRUE), .groups = \"drop\") |&gt;\n    mutate(window = window_name)\n}\n\ndf_era5 &lt;- bind_rows(\n  aggregate_obs_seasonal(df_era5_raw |&gt; rename(value = mean), \"primera\"),\n  aggregate_obs_seasonal(df_era5_raw |&gt; rename(value = mean), \"postrera\")\n)\n\n# Combine forecasts\ndf_fcst_all &lt;- bind_rows(df_insiv, df_seas5) |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END)\n\n# Filter to operational leadtimes\ndf_fcst_filtered &lt;- df_fcst_all |&gt;\n  mutate(issued_month = month(issued_date)) |&gt;\n  filter(\n    (window == \"primera\" & issued_month %in% PRIMERA_ISSUED_MONTHS) |\n    (window == \"postrera\" & issued_month %in% POSTRERA_ISSUED_MONTHS)\n  )",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#beyond-binary-a-more-nuanced-view",
    "href": "02_continuous_metrics.html#beyond-binary-a-more-nuanced-view",
    "title": "3  Continuous Metrics",
    "section": "",
    "text": "Correlation metrics measure how well forecasts track year-to-year variability\nError metrics quantify the typical magnitude of forecast errors\nROC-AUC assesses ranking skill: do lower forecasts correspond to drier years?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#comparison-frameworks",
    "href": "02_continuous_metrics.html#comparison-frameworks",
    "title": "3  Continuous Metrics",
    "section": "3.2 Comparison Frameworks",
    "text": "3.2 Comparison Frameworks\n\n\n\n\n\n\nNoteTwo Ways to Validate Forecasts\n\n\n\nNative Comparison: Each model validated against its “natural” observation source\n\nSEAS5 vs ERA5 (both from ECMWF)\nINSIVUMEH vs ENACTS (regional data)\n\nENACTS-only: All models validated against ENACTS\n\nAllows direct model-to-model comparison\nENACTS is the operational observation source",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#method-1-correlation-metrics",
    "href": "02_continuous_metrics.html#method-1-correlation-metrics",
    "title": "3  Continuous Metrics",
    "section": "3.3 Method 1: Correlation Metrics",
    "text": "3.3 Method 1: Correlation Metrics\n\n3.3.1 What It Measures\nCorrelation measures whether forecasts correctly identify which years are wetter or drier than average. A forecast doesn’t need to predict exact rainfall amounts - it just needs to rank years correctly.\n\n\n3.3.2 Metrics\n\n\n\n\n\n\n\n\nMetric\nFormula\nInterpretation\n\n\n\n\nSpearman ρ\nRank correlation\nDid wet years get higher forecasts? Robust to outliers.\n\n\nPearson r\nLinear correlation\nSame, but sensitive to extreme values.\n\n\n\n\n\n3.3.3 Strengths & Weaknesses\n\n\nStrengths\n\nMeasures interannual variability skill\nNo threshold dependence\nWorks on any units (mm or z-scores)\nSpearman is robust to outliers\n\n\nWeaknesses\n\nDoesn’t measure absolute accuracy\nA forecast could rank years perfectly but have huge bias\nSample size matters (n=25 years)\n\n\n\n\n\nCalculate standardized anomalies\n# Forecast anomalies\ndf_fcst_anom &lt;- df_fcst_filtered |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  mutate(\n    fcst_mean = mean(value, na.rm = TRUE),\n    fcst_sd = sd(value, na.rm = TRUE),\n    fcst_anom = (value - fcst_mean) / fcst_sd\n  ) |&gt;\n  ungroup() |&gt;\n  select(year, window, leadtime, forecast_source, value, fcst_anom)\n\n# Observation anomalies\ncalc_obs_anom &lt;- function(df, obs_name) {\n  df |&gt;\n    filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n    group_by(window) |&gt;\n    mutate(\n      obs_mean = mean(obs_mm, na.rm = TRUE),\n      obs_sd = sd(obs_mm, na.rm = TRUE),\n      obs_anom = (obs_mm - obs_mean) / obs_sd\n    ) |&gt;\n    ungroup() |&gt;\n    mutate(obs_source = obs_name) |&gt;\n    select(year, window, obs_mm, obs_anom, obs_source)\n}\n\ndf_enacts_anom &lt;- calc_obs_anom(df_enacts, \"ENACTS\")\ndf_era5_anom &lt;- calc_obs_anom(df_era5, \"ERA5\")\n\n# Join - Native\ndf_anom_native &lt;- bind_rows(\n  df_fcst_anom |&gt; filter(forecast_source == \"SEAS5\") |&gt;\n    inner_join(df_era5_anom, by = c(\"year\", \"window\")),\n  df_fcst_anom |&gt; filter(str_detect(forecast_source, \"INSIVUMEH\")) |&gt;\n    inner_join(df_enacts_anom, by = c(\"year\", \"window\"))\n)\n\n# Join - ENACTS-only\ndf_anom_enacts &lt;- df_fcst_anom |&gt;\n  inner_join(df_enacts_anom, by = c(\"year\", \"window\"))\n\n\n\n\nCalculate continuous metrics\ncalc_continuous_metrics &lt;- function(df) {\n  df |&gt;\n    group_by(forecast_source, window, leadtime, obs_source) |&gt;\n    summarise(\n      n = n(),\n      spearman = cor(fcst_anom, obs_anom, method = \"spearman\", use = \"complete.obs\"),\n      pearson = cor(fcst_anom, obs_anom, method = \"pearson\", use = \"complete.obs\"),\n      rmse = sqrt(mean((fcst_anom - obs_anom)^2, na.rm = TRUE)),\n      mae = mean(abs(fcst_anom - obs_anom), na.rm = TRUE),\n      bias_mm = mean(value - obs_mm, na.rm = TRUE),\n      .groups = \"drop\"\n    )\n}\n\ndf_metrics_native &lt;- calc_continuous_metrics(df_anom_native)\ndf_metrics_enacts &lt;- calc_continuous_metrics(df_anom_enacts)\n\n\n\n\n3.3.4 Results: Spearman Correlation\n\n\nHeatmap function\ncreate_metric_heatmap &lt;- function(df, metric_col, metric_name, title, caption,\n                                  midpoint = 0, lower_is_better = FALSE) {\n  df_plot &lt;- df |&gt;\n    filter(!is.na(!!sym(metric_col))) |&gt;\n    mutate(window = factor(window, levels = c(\"primera\", \"postrera\"))) |&gt;\n    group_by(window, leadtime) |&gt;\n    mutate(is_best = if (lower_is_better) {\n      !!sym(metric_col) == min(!!sym(metric_col), na.rm = TRUE)\n    } else {\n      !!sym(metric_col) == max(!!sym(metric_col), na.rm = TRUE)\n    }) |&gt;\n    ungroup()\n\n  ggplot(df_plot, aes(x = factor(leadtime), y = forecast_source)) +\n    geom_tile(aes(fill = !!sym(metric_col)), color = \"white\", linewidth = 0.5) +\n    geom_tile(\n      data = df_plot |&gt; filter(is_best),\n      fill = NA, color = \"black\", linewidth = 1.5\n    ) +\n    geom_text(aes(label = sprintf(\"%.2f\", !!sym(metric_col))), size = 4, fontface = \"bold\", color = \"black\") +\n    facet_wrap(~window) +\n    scale_fill_gradient2(\n      low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n      midpoint = midpoint, name = metric_name\n    ) +\n    labs(title = title, x = \"Leadtime (months)\", y = NULL, caption = caption) +\n    theme_minimal() +\n    theme(legend.position = \"right\", panel.grid = element_blank(),\n          plot.caption = element_text(hjust = 0))\n}\n\n\n\nNative ComparisonENACTS-only\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_metrics_native, \"spearman\", \"Spearman ρ\",\n  \"Spearman Rank Correlation - NATIVE\",\n  \"NATIVE: SEAS5 vs ERA5; INSIVUMEH vs ENACTS. Black outline = best per leadtime.\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_metrics_enacts, \"spearman\", \"Spearman ρ\",\n  \"Spearman Rank Correlation - ENACTS-ONLY\",\n  \"ENACTS-ONLY: All models vs ENACTS. Black outline = best per leadtime.\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.5 Interpretation: Correlation\n\n\n\n\n\n\nTipKey Findings\n\n\n\nPrimera (May-Aug)\n\nSEAS5 shows strong correlation (ρ = 0.56-0.60 at LT0-2)\nINSIVUMEH models competitive at LT2, especially CESM1 (ρ = 0.58)\nAll models beat random (ρ = 0 expected by chance)\n\nPostrera (Sep-Nov)\n\nWeaker correlations across all models (ρ = 0.2-0.5)\nSEAS5 slightly better at LT1 (ρ = 0.51)\nCCSM4 shows unexpected strength at LT3 (ρ = 0.48)\nMore variability between leadtimes - less reliable signal",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#method-2-error-magnitude-rmse-mae",
    "href": "02_continuous_metrics.html#method-2-error-magnitude-rmse-mae",
    "title": "3  Continuous Metrics",
    "section": "3.4 Method 2: Error Magnitude (RMSE, MAE)",
    "text": "3.4 Method 2: Error Magnitude (RMSE, MAE)\n\n3.4.1 What It Measures\nRoot Mean Square Error (RMSE) and Mean Absolute Error (MAE) measure how far forecasts deviate from observations in standardized units.\n\n\n3.4.2 Metrics\n\n\n\n\n\n\n\n\nMetric\nFormula\nInterpretation\n\n\n\n\nRMSE\n√(mean((fcst - obs)²))\nPenalizes large errors more heavily\n\n\nMAE\nmean(|fcst - obs|)\nEqual penalty per unit error\n\n\n\n\n\n3.4.3 Strengths & Weaknesses\n\n\nStrengths\n\nMeasures absolute accuracy\nRMSE sensitive to outliers (useful for flagging catastrophic misses)\nMAE more robust, easier to interpret\n\n\nWeaknesses\n\nDepends on units (z-scores vs mm)\nCan be dominated by a few bad years\nDoesn’t distinguish bias from random error\n\n\n\n\n\n3.4.4 Results: RMSE\n\nNative ComparisonENACTS-only\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_metrics_native, \"rmse\", \"RMSE\",\n  \"RMSE (Standardized Anomalies) - NATIVE\",\n  \"NATIVE: SEAS5 vs ERA5; INSIVUMEH vs ENACTS. Lower = better. Black outline = best.\",\n  midpoint = 1, lower_is_better = TRUE\n) +\n  scale_fill_gradient2(low = \"#1A9850\", mid = \"#FFFFBF\", high = \"#D73027\",\n                       midpoint = 1, name = \"RMSE\", limits = c(0.5, 1.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_metrics_enacts, \"rmse\", \"RMSE\",\n  \"RMSE (Standardized Anomalies) - ENACTS-ONLY\",\n  \"ENACTS-ONLY: All models vs ENACTS. Lower = better. Black outline = best.\",\n  midpoint = 1, lower_is_better = TRUE\n) +\n  scale_fill_gradient2(low = \"#1A9850\", mid = \"#FFFFBF\", high = \"#D73027\",\n                       midpoint = 1, name = \"RMSE\", limits = c(0.5, 1.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4.5 Interpretation: RMSE\n\n\n\n\n\n\nTipKey Findings\n\n\n\n\nSEAS5 has lowest RMSE for Primera (0.81-0.95 vs 0.95-1.13 for INSIVUMEH)\nFor Postrera, CCSM4 often has lower RMSE than SEAS5\nRMSE &gt; 1.0 means error exceeds one standard deviation - forecasts add noise rather than signal\nCFSv2 Postrera LT3 has RMSE = 1.44 - actively harmful predictions",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#method-3-bias",
    "href": "02_continuous_metrics.html#method-3-bias",
    "title": "3  Continuous Metrics",
    "section": "3.5 Method 3: Bias",
    "text": "3.5 Method 3: Bias\n\n3.5.1 What It Measures\nBias measures systematic over- or under-prediction. A forecast with zero bias is “calibrated” on average, even if individual predictions are wrong.\n\n\n3.5.2 Formula\n\\[\\text{Bias} = \\text{mean}(\\text{forecast} - \\text{observed})\\]\n\nPositive bias: Forecast too wet\nNegative bias: Forecast too dry\n\n\n\n3.5.3 Strengths & Weaknesses\n\n\nStrengths\n\nEasy to interpret (in mm)\nCan be corrected with simple adjustments\nReveals systematic model issues\n\n\nWeaknesses\n\nHides random error (a model can have zero bias but huge scatter)\nMust use raw mm, not z-scores (z-score bias is always ~0 by construction)\n\n\n\n\n\n\n\n\n\nWarningImportant Note on Units\n\n\n\nBias must be calculated on raw millimeters, not standardized anomalies. When you standardize to z-scores, you force the mean to zero - so bias between two z-scored series is mathematically guaranteed to be ~0, which is meaningless.\n\n\n\n\n3.5.4 Results: Bias (Raw mm)\n\n\nBias heatmap function\ncreate_bias_heatmap &lt;- function(df, title, caption) {\n  df_plot &lt;- df |&gt;\n    filter(!is.na(bias_mm)) |&gt;\n    mutate(window = factor(window, levels = c(\"primera\", \"postrera\"))) |&gt;\n    group_by(window, leadtime) |&gt;\n    mutate(is_best = abs(bias_mm) == min(abs(bias_mm), na.rm = TRUE)) |&gt;\n    ungroup()\n\n  bias_max &lt;- max(abs(df_plot$bias_mm), na.rm = TRUE)\n\n  ggplot(df_plot, aes(x = factor(leadtime), y = forecast_source)) +\n    geom_tile(aes(fill = bias_mm), color = \"white\", linewidth = 0.5) +\n    geom_tile(\n      data = df_plot |&gt; filter(is_best),\n      fill = NA, color = \"black\", linewidth = 1.5\n    ) +\n    geom_text(aes(label = sprintf(\"%.0f\", bias_mm)), size = 4, fontface = \"bold\", color = \"black\") +\n    facet_wrap(~window) +\n    scale_fill_gradient2(\n      low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n      midpoint = 0, limits = c(-bias_max, bias_max), name = \"Bias (mm)\"\n    ) +\n    labs(title = title, x = \"Leadtime (months)\", y = NULL, caption = caption) +\n    theme_minimal() +\n    theme(legend.position = \"right\", panel.grid = element_blank(),\n          plot.caption = element_text(hjust = 0))\n}\n\n\n\nNative ComparisonENACTS-only\n\n\n\n\nCode\ncreate_bias_heatmap(\n  df_metrics_native,\n  \"Mean Bias - NATIVE\",\n  \"NATIVE: SEAS5 vs ERA5; INSIVUMEH vs ENACTS. Units: mm. Black outline = closest to zero.\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_bias_heatmap(\n  df_metrics_enacts,\n  \"Mean Bias - ENACTS-ONLY\",\n  \"ENACTS-ONLY: All models vs ENACTS. Units: mm. Black outline = closest to zero.\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.5 Interpretation: Bias\n\n\n\n\n\n\nTipKey Findings\n\n\n\nPrimera\n\nAll models are biased wet (positive bias)\nSEAS5: +68 to +132mm too wet\nINSIVUMEH models: +244 to +262mm too wet (severe overestimation)\nThis means forecasts systematically predict more rain than actually falls\n\nPostrera\n\nAll models are biased dry (negative bias)\nSEAS5: -113 to -132mm too dry\nINSIVUMEH models: -21 to -37mm too dry (much smaller bias)\nFor Postrera, INSIVUMEH has better calibration than SEAS5",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#method-4-distance-to-threshold",
    "href": "02_continuous_metrics.html#method-4-distance-to-threshold",
    "title": "3  Continuous Metrics",
    "section": "3.6 Method 4: Distance-to-Threshold",
    "text": "3.6 Method 4: Distance-to-Threshold\n\n3.6.1 What It Measures\nInstead of binary hit/miss, measure how far each forecast and observation was from their respective drought thresholds. Then correlate these distances.\n\n\n3.6.2 Why This Matters\nConsider two “false positive” cases:\n\nForecast: 10mm below threshold. Observed: 5mm above threshold. (Near miss)\nForecast: 100mm below threshold. Observed: 150mm above threshold. (Confident and wrong)\n\nBinary metrics score both as equally wrong. Distance-to-threshold reveals that Case 1 was almost right, while Case 2 was a confident failure.\n\n\n3.6.3 Calculation\nfcst_distance = forecast_value - forecast_threshold\nobs_distance = observed_value - observed_threshold\n\ndistance_correlation = cor(fcst_distance, obs_distance)\n\nNegative distance = below threshold (drought-like)\nPositive distance = above threshold (normal)\n\n\n\n3.6.4 Strengths & Weaknesses\n\n\nStrengths\n\nGives partial credit for near-misses\nOperationally relevant (close calls matter)\nUses actual mm, not abstract scores\n\n\nWeaknesses\n\nStill depends on threshold choice\nCorrelation can be high even if absolute distances don’t match\n\n\n\n\n\nCalculate distance-to-threshold\n# Observation thresholds\nobs_thresh_enacts &lt;- df_enacts |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n  group_by(window) |&gt;\n  summarise(obs_thresh = calc_rp_threshold(obs_mm, 4, -1), .groups = \"drop\")\n\nobs_thresh_era5 &lt;- df_era5 |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n  group_by(window) |&gt;\n  summarise(obs_thresh = calc_rp_threshold(obs_mm, 4, -1), .groups = \"drop\")\n\n# Forecast thresholds\nfcst_thresholds &lt;- df_fcst_filtered |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  summarise(fcst_thresh = calc_rp_threshold(value, 4, -1), .groups = \"drop\")\n\n# Build distance data\nbuild_distance_data &lt;- function(df_fcst, df_obs, obs_thresholds, obs_name) {\n  df_obs_filtered &lt;- df_obs |&gt; filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END)\n\n  df_fcst |&gt;\n    left_join(df_obs_filtered |&gt; select(year, window, obs_mm), by = c(\"year\", \"window\")) |&gt;\n    left_join(obs_thresholds, by = \"window\") |&gt;\n    left_join(fcst_thresholds, by = c(\"forecast_source\", \"window\", \"leadtime\")) |&gt;\n    mutate(\n      fcst_distance_mm = value - fcst_thresh,\n      obs_distance_mm = obs_mm - obs_thresh,\n      fcst_drought = value &lt;= fcst_thresh,\n      obs_drought = obs_mm &lt;= obs_thresh,\n      obs_source = obs_name\n    ) |&gt;\n    filter(!is.na(obs_mm), !is.na(fcst_thresh))\n}\n\ndf_dist_native &lt;- bind_rows(\n  build_distance_data(df_fcst_filtered |&gt; filter(forecast_source == \"SEAS5\"),\n                      df_era5, obs_thresh_era5, \"ERA5\"),\n  build_distance_data(df_fcst_filtered |&gt; filter(str_detect(forecast_source, \"INSIVUMEH\")),\n                      df_enacts, obs_thresh_enacts, \"ENACTS\")\n)\n\ndf_dist_enacts &lt;- build_distance_data(df_fcst_filtered, df_enacts, obs_thresh_enacts, \"ENACTS\")\n\n# Calculate distance metrics\ncalc_distance_metrics &lt;- function(df) {\n  df |&gt;\n    group_by(forecast_source, window, leadtime, obs_source) |&gt;\n    summarise(\n      dist_corr = cor(fcst_distance_mm, obs_distance_mm, use = \"complete.obs\"),\n      mae_dist = mean(abs(fcst_distance_mm - obs_distance_mm), na.rm = TRUE),\n      .groups = \"drop\"\n    )\n}\n\ndf_dist_metrics_native &lt;- calc_distance_metrics(df_dist_native)\ndf_dist_metrics_enacts &lt;- calc_distance_metrics(df_dist_enacts)\n\n\n\n\n3.6.5 Results: Distance Correlation\n\nNative ComparisonENACTS-only\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_dist_metrics_native, \"dist_corr\", \"Correlation\",\n  \"Distance-to-Threshold Correlation - NATIVE\",\n  \"NATIVE: SEAS5 vs ERA5; INSIVUMEH vs ENACTS. Higher = forecast distances match observed distances.\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_dist_metrics_enacts, \"dist_corr\", \"Correlation\",\n  \"Distance-to-Threshold Correlation - ENACTS-ONLY\",\n  \"ENACTS-ONLY: All models vs ENACTS. Higher = forecast distances match observed distances.\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.6.6 Scatter Plot: Distance Comparison\n\n\nDistance scatter plots\ndf_dist_enacts |&gt;\n  filter(window == \"primera\", leadtime %in% c(1, 2)) |&gt;\n  mutate(\n    model = str_remove(forecast_source, \"INSIVUMEH_\"),\n    facet = paste0(model, \" LT\", leadtime)\n  ) |&gt;\n  ggplot(aes(x = obs_distance_mm, y = fcst_distance_mm)) +\n  geom_hline(yintercept = 0, color = \"grey70\", linetype = \"dashed\") +\n  geom_vline(xintercept = 0, color = \"grey70\", linetype = \"dashed\") +\n  geom_abline(slope = 1, intercept = 0, color = \"#B2182B\", linetype = \"dotted\", linewidth = 0.8) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"#007CE1\", fill = \"#007CE1\", alpha = 0.2) +\n  geom_point(aes(color = obs_drought), alpha = 0.6, size = 2.5) +\n  facet_wrap(~facet, ncol = 4) +\n  scale_color_manual(\n    values = c(\"TRUE\" = \"#D73027\", \"FALSE\" = \"#4575B4\"),\n    labels = c(\"TRUE\" = \"Drought year\", \"FALSE\" = \"Normal year\"),\n    name = \"Observed\"\n  ) +\n  labs(\n    title = \"Primera LT1-2: Forecast vs Observed Distance from Threshold\",\n    subtitle = \"Dotted red = perfect agreement. Blue = linear fit. Dashed lines = threshold (distance = 0).\",\n    x = \"Observed Distance from Threshold (mm)\",\n    y = \"Forecast Distance from Threshold (mm)\",\n    caption = \"ENACTS-ONLY comparison. Negative = below threshold (drought-like).\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\", plot.caption = element_text(hjust = 0))\n\n\n\n\n\n\n\n\n\n\n\n3.6.7 Interpretation: Distance-to-Threshold\n\n\n\n\n\n\nTipKey Findings\n\n\n\nPrimera\n\nSEAS5 LT1 has the strongest distance correlation (r = 0.66)\nWhen SEAS5 says “drier than usual”, observations tend to agree\nINSIVUMEH models also show skill (r = 0.5-0.53)\n\nPostrera\n\nCCSM4 shows best distance skill at LT1 (r = 0.51)\nSEAS5 Postrera distance correlation drops sharply at LT2 (r = 0.16)\nCFSv2 LT3 has negative correlation (r = -0.08) - actively misleading",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#method-5-roc-curves-auc",
    "href": "02_continuous_metrics.html#method-5-roc-curves-auc",
    "title": "3  Continuous Metrics",
    "section": "3.7 Method 5: ROC Curves & AUC",
    "text": "3.7 Method 5: ROC Curves & AUC\n\n3.7.1 What It Measures\nROC (Receiver Operating Characteristic) curves measure ranking skill: did lower forecast values correspond to actual drought years?\nUnlike binary metrics, ROC doesn’t require choosing a specific threshold. It asks: “If I ranked all years by forecast value, would drought years tend to be at the bottom?”\n\n\n3.7.2 How to Read an ROC Curve\n\nDiagonal line: Random guessing (AUC = 0.5)\nCurve above diagonal: Skill (drought years get lower forecasts)\nCurve below diagonal: Inverted skill (drought years get higher forecasts - very bad)\n\n\n\n3.7.3 AUC Interpretation\n\n\n\nAUC\nInterpretation\n\n\n\n\n0.9-1.0\nOutstanding\n\n\n0.8-0.9\nExcellent\n\n\n0.7-0.8\nAcceptable\n\n\n0.5-0.7\nPoor\n\n\n&lt; 0.5\nWorse than random\n\n\n\n\n\n3.7.4 Strengths & Weaknesses\n\n\nStrengths\n\nThreshold-independent\nMeasures ranking/discrimination skill\nWell-understood statistically\n\n\nWeaknesses\n\nDoesn’t measure calibration\nCan be high even with severe bias\nSmall sample size (6 drought years) limits precision\n\n\n\n\n\nCalculate ROC and AUC\n# Build ROC data\nbuild_roc_data &lt;- function(df_fcst, df_obs, obs_thresholds, obs_name) {\n  df_obs_filtered &lt;- df_obs |&gt; filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END)\n\n  df_fcst |&gt;\n    left_join(df_obs_filtered |&gt; select(year, window, obs_mm), by = c(\"year\", \"window\")) |&gt;\n    left_join(obs_thresholds, by = \"window\") |&gt;\n    mutate(\n      truth = factor(obs_mm &lt;= obs_thresh, c(TRUE, FALSE), c(\"drought\", \"no_drought\")),\n      drought_score = -value,  # Lower forecast = higher drought score\n      obs_source = obs_name\n    ) |&gt;\n    filter(!is.na(truth))\n}\n\ndf_roc_native &lt;- bind_rows(\n  build_roc_data(df_fcst_filtered |&gt; filter(forecast_source == \"SEAS5\"),\n                 df_era5, obs_thresh_era5, \"ERA5\"),\n  build_roc_data(df_fcst_filtered |&gt; filter(str_detect(forecast_source, \"INSIVUMEH\")),\n                 df_enacts, obs_thresh_enacts, \"ENACTS\")\n)\n\ndf_roc_enacts &lt;- build_roc_data(df_fcst_filtered, df_enacts, obs_thresh_enacts, \"ENACTS\")\n\n# Calculate AUC\ncalc_auc &lt;- function(df) {\n  if (length(unique(df$truth)) &lt; 2) return(NA_real_)\n  roc_auc(df, truth = truth, drought_score, event_level = \"first\")$.estimate\n}\n\ndf_auc_native &lt;- df_roc_native |&gt;\n  group_by(forecast_source, window, leadtime, obs_source) |&gt;\n  summarise(auc = calc_auc(pick(everything())), .groups = \"drop\")\n\ndf_auc_enacts &lt;- df_roc_enacts |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  summarise(auc = calc_auc(pick(everything())), obs_source = \"ENACTS\", .groups = \"drop\")\n\n\n\n\n3.7.5 Results: AUC Heatmaps\n\nNative ComparisonENACTS-only\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_auc_native, \"auc\", \"AUC\",\n  \"ROC-AUC - NATIVE\",\n  \"NATIVE: SEAS5 vs ERA5; INSIVUMEH vs ENACTS. AUC &gt; 0.7 = acceptable skill.\",\n  midpoint = 0.5\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncreate_metric_heatmap(\n  df_auc_enacts, \"auc\", \"AUC\",\n  \"ROC-AUC - ENACTS-ONLY\",\n  \"ENACTS-ONLY: All models vs ENACTS. AUC &gt; 0.7 = acceptable skill.\",\n  midpoint = 0.5\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.7.6 ROC Curves: Primera\n\n\nROC curves\n# Calculate ROC curves\ncalc_roc_curve &lt;- function(df) {\n  if (length(unique(df$truth)) &lt; 2) return(tibble(specificity = NA, sensitivity = NA))\n  roc_curve(df, truth = truth, drought_score, event_level = \"first\")\n}\n\ndf_roc_curves_enacts &lt;- df_roc_enacts |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  group_modify(~calc_roc_curve(.x)) |&gt;\n  ungroup()\n\ndf_roc_curves_enacts |&gt;\n  filter(window == \"primera\", !is.na(specificity)) |&gt;\n  mutate(\n    model = str_remove(forecast_source, \"INSIVUMEH_\"),\n    facet = paste0(model, \" LT\", leadtime)\n  ) |&gt;\n  left_join(\n    df_auc_enacts |&gt; filter(window == \"primera\") |&gt;\n      mutate(model = str_remove(forecast_source, \"INSIVUMEH_\"),\n             facet = paste0(model, \" LT\", leadtime),\n             label = paste0(\"AUC = \", sprintf(\"%.2f\", auc))),\n    by = c(\"forecast_source\", \"window\", \"leadtime\", \"facet\", \"model\")\n  ) |&gt;\n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"grey50\") +\n  geom_path(color = \"#007CE1\", linewidth = 1) +\n  geom_text(aes(x = 0.7, y = 0.2, label = label), hjust = 0, size = 3, fontface = \"bold\",\n            color = \"#007CE1\", check_overlap = TRUE) +\n  facet_wrap(~facet, ncol = 4) +\n  coord_equal() +\n  labs(\n    title = \"ROC Curves: Primera - ENACTS-ONLY\",\n    subtitle = \"Dashed line = random guessing. Curve above diagonal = skill.\",\n    x = \"False Positive Rate (1 - Specificity)\",\n    y = \"True Positive Rate (Sensitivity)\",\n    caption = \"ENACTS-ONLY: All models vs ENACTS.\"\n  ) +\n  theme_minimal() +\n  theme(strip.text = element_text(face = \"bold\"), plot.caption = element_text(hjust = 0))\n\n\n\n\n\n\n\n\n\n\n\n3.7.7 Interpretation: ROC-AUC\n\n\n\n\n\n\nTipKey Findings\n\n\n\nPrimera\n\nSEAS5 and CESM1 show excellent discrimination (AUC = 0.87-0.88 at LT1-2)\nCCSM4 also strong (AUC = 0.81 at LT1)\nAll models beat random guessing\n\nPostrera\n\nMuch weaker AUC across all models (0.5-0.8 range)\nCCSM4 LT3 surprisingly good (AUC = 0.83)\nSEAS5 LT2 near random (AUC = 0.54)\nCFSv2 LT3 inverted (AUC = 0.40) - lower forecasts correspond to wetter years",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#summary-which-model-wins",
    "href": "02_continuous_metrics.html#summary-which-model-wins",
    "title": "3  Continuous Metrics",
    "section": "3.8 Summary: Which Model Wins?",
    "text": "3.8 Summary: Which Model Wins?\n\n\nSummary comparison\ndf_summary &lt;- df_metrics_enacts |&gt;\n  left_join(df_dist_metrics_enacts, by = c(\"forecast_source\", \"window\", \"leadtime\", \"obs_source\")) |&gt;\n  left_join(df_auc_enacts, by = c(\"forecast_source\", \"window\", \"leadtime\", \"obs_source\")) |&gt;\n  filter(leadtime %in% c(1, 2)) |&gt;\n  mutate(model = str_remove(forecast_source, \"INSIVUMEH_\")) |&gt;\n  select(model, window, leadtime, spearman, rmse, bias_mm, dist_corr, auc) |&gt;\n  mutate(across(c(spearman, rmse, dist_corr, auc), ~round(.x, 2)),\n         bias_mm = round(bias_mm, 0))\n\n\n\n3.8.1 Primera (MJJA)\n\n\nCode\ndf_summary |&gt;\n  filter(window == \"primera\") |&gt;\n  arrange(leadtime, desc(auc)) |&gt;\n  knitr::kable(\n    col.names = c(\"Model\", \"Window\", \"LT\", \"Spearman\", \"RMSE\", \"Bias (mm)\", \"Dist Corr\", \"AUC\"),\n    caption = \"Primera: All metrics at LT1-2 (ENACTS-only)\"\n  )\n\n\n\nPrimera: All metrics at LT1-2 (ENACTS-only)\n\n\nModel\nWindow\nLT\nSpearman\nRMSE\nBias (mm)\nDist Corr\nAUC\n\n\n\n\nSEAS5\nprimera\n1\n0.56\n0.81\n96\n0.66\n0.88\n\n\nCESM1\nprimera\n1\n0.48\n0.95\n261\n0.53\n0.87\n\n\nCCSM4\nprimera\n1\n0.48\n0.96\n258\n0.52\n0.81\n\n\nCFSv2\nprimera\n1\n0.39\n0.98\n253\n0.50\n0.65\n\n\nSEAS5\nprimera\n2\n0.56\n0.94\n68\n0.54\n0.88\n\n\nCESM1\nprimera\n2\n0.58\n1.02\n259\n0.46\n0.86\n\n\nCFSv2\nprimera\n2\n0.47\n1.00\n245\n0.48\n0.75\n\n\nCCSM4\nprimera\n2\n0.32\n1.13\n248\n0.34\n0.70\n\n\n\n\n\n\n\n\n\n\n\nNotePrimera Recommendation: SEAS5\n\n\n\n\nHighest AUC (0.88 at both LT1 and LT2)\nHighest distance correlation (0.66 at LT1)\nLowest RMSE (0.81 at LT1)\nSmallest bias (+68 to +96mm vs +244-262mm for INSIVUMEH)\nCESM1 is competitive but has much larger bias\n\n\n\n\n\n3.8.2 Postrera (SON)\n\n\nCode\ndf_summary |&gt;\n  filter(window == \"postrera\") |&gt;\n  arrange(leadtime, desc(auc)) |&gt;\n  knitr::kable(\n    col.names = c(\"Model\", \"Window\", \"LT\", \"Spearman\", \"RMSE\", \"Bias (mm)\", \"Dist Corr\", \"AUC\"),\n    caption = \"Postrera: All metrics at LT1-2 (ENACTS-only)\"\n  )\n\n\n\nPostrera: All metrics at LT1-2 (ENACTS-only)\n\n\nModel\nWindow\nLT\nSpearman\nRMSE\nBias (mm)\nDist Corr\nAUC\n\n\n\n\nSEAS5\npostrera\n1\n0.51\n1.14\n-131\n0.32\n0.69\n\n\nCCSM4\npostrera\n1\n0.50\n0.97\n-37\n0.51\n0.64\n\n\nCESM1\npostrera\n1\n0.19\n1.28\n-33\n0.14\n0.64\n\n\nCFSv2\npostrera\n1\n0.19\n1.26\n-23\n0.18\n0.58\n\n\nCCSM4\npostrera\n2\n0.26\n1.31\n-28\n0.10\n0.79\n\n\nCFSv2\npostrera\n2\n0.06\n1.28\n-28\n0.15\n0.68\n\n\nCESM1\npostrera\n2\n0.08\n1.17\n-21\n0.29\n0.61\n\n\nSEAS5\npostrera\n2\n0.29\n1.27\n-128\n0.16\n0.54\n\n\n\n\n\n\n\n\n\n\n\nNotePostrera Recommendation: Ambiguous\n\n\n\nNo clear winner - different metrics favor different models:\n\n\n\nMetric\nLT1 Winner\nLT2 Winner\n\n\n\n\nSpearman\nSEAS5 (0.51)\nSEAS5 (0.29)\n\n\nRMSE\nCCSM4 (0.97)\nCESM1 (1.17)\n\n\nBias\nCFSv2 (-23mm)\nCESM1 (-21mm)\n\n\nDist Corr\nCCSM4 (0.51)\nCESM1 (0.29)\n\n\nAUC\nSEAS5 (0.69)\nCCSM4 (0.79)\n\n\n\nIf forced to choose one model: CCSM4 has the best average AUC and distance correlation, but no model shows strong, consistent skill.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "02_continuous_metrics.html#conclusions",
    "href": "02_continuous_metrics.html#conclusions",
    "title": "3  Continuous Metrics",
    "section": "3.9 Conclusions",
    "text": "3.9 Conclusions\n\n3.9.1 Key Takeaways\n\nPrimera has genuine forecast skill across multiple metrics. SEAS5 is the clear winner with AUC values around 0.80 - meaning 80% probability of correctly ranking a drought vs non-drought year.\nPostrera remains challenging - no model shows consistent, strong skill. AUC values hover near 0.5-0.6 (barely above random guessing). Consider supplementing with monitoring-based triggers.\nROC-AUC provides the clearest picture - it directly answers “can this forecast discriminate drought?” without depending on threshold choices. Binary F1 scores can be variable with small samples; AUC is more stable.\nBias matters operationally - INSIVUMEH models systematically over-predict Primera rainfall by ~250mm. This could be corrected with simple bias adjustment if these models are used.\nCorrelation and AUC tell consistent stories - both favor SEAS5 for Primera, show weaker skill for Postrera. When multiple metrics agree, we can be more confident in the assessment.\n\n\n\n3.9.2 Operational Recommendations\n\n\n\n\n\n\n\n\n\nSeason\nRecommended Model\nConfidence\nNotes\n\n\n\n\nPrimera LT0\nSEAS5\nHigh\nOnly option for May activation\n\n\nPrimera LT1-2\nSEAS5\nHigh\nBest on all metrics\n\n\nPostrera LT1-2\nCCSM4 or SEAS5\nLow\nConsider ensemble or monitoring triggers",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Continuous Metrics</span>"
    ]
  },
  {
    "objectID": "03_chirps_tiebreaker.html",
    "href": "03_chirps_tiebreaker.html",
    "title": "4  CHIRPS Tie-Breaker",
    "section": "",
    "text": "4.1 The Need for a Third Opinion\nChapters 2 and 3 evaluated forecasts against two observation sources:\nThese showed consistent results: SEAS5 performs well for Primera, all models struggle with Postrera. But what if both observation sources happen to favor SEAS5? To break potential ties and increase confidence, we introduce a third independent observation source: CHIRPS.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>CHIRPS Tie-Breaker</span>"
    ]
  },
  {
    "objectID": "03_chirps_tiebreaker.html#the-need-for-a-third-opinion",
    "href": "03_chirps_tiebreaker.html#the-need-for-a-third-opinion",
    "title": "4  CHIRPS Tie-Breaker",
    "section": "",
    "text": "ERA5: Global reanalysis (SEAS5’s “native” validation)\nENACTS: Station-blended satellite product (operational standard for Guatemala)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>CHIRPS Tie-Breaker</span>"
    ]
  },
  {
    "objectID": "03_chirps_tiebreaker.html#observational-data-sets",
    "href": "03_chirps_tiebreaker.html#observational-data-sets",
    "title": "4  CHIRPS Tie-Breaker",
    "section": "4.2 Observational Data Sets",
    "text": "4.2 Observational Data Sets\nThis analysis uses three independent observation sources:\nCHIRPS (Climate Hazards Group InfraRed Precipitation with Station data) is a quasi-global rainfall dataset that blends satellite imagery with station data. Key characteristics:\n\n\n\n\n\n\n\n\n\nFeature\nCHIRPS\nENACTS\nERA5\n\n\n\n\nResolution\n0.05° (~5km)\n0.05° (~5km)\n0.25° (~25km)\n\n\nSource\nSatellite + stations\nSatellite + stations\nModel reanalysis\n\n\nCoverage\n50°S-50°N\nRegional\nGlobal\n\n\nTemporal\n1981-present\nVaries\n1940-present\n\n\n\nCHIRPS and ENACTS both blend satellite and station data, but use different algorithms and station networks. This makes CHIRPS a useful independent check - if forecasts perform well against multiple observation sources, we can be more confident in the skill assessment.\n\n\nSetup: Libraries and data loading\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(cumulus)\nlibrary(yardstick)\nlibrary(gghdx)\ngghdx()\n\nbox::use(../../R/enacts)\nbox::use(../../R/seas5)\n\nset.seed(42)\n\n# Configuration\nBASELINE_START &lt;- 2000\nBASELINE_END &lt;- 2024\nPRIMERA_MONTHS &lt;- 5:8\nPOSTRERA_MONTHS &lt;- 9:11\nPRIMERA_ISSUED_MONTHS &lt;- c(3, 4, 5)\nPOSTRERA_ISSUED_MONTHS &lt;- c(6, 7, 8, 9)\n\n# Helper function\ncalc_rp_threshold &lt;- function(x, rp_target = 4, direction = -1) {\n  x &lt;- x[!is.na(x)]\n  n &lt;- length(x)\n  if (n &lt; 3) return(NA_real_)\n  ranks &lt;- rank(x * -direction, ties.method = \"average\")\n  rp &lt;- (n + 1) / ranks\n  approx(rp, x, xout = rp_target, rule = 2)$y\n}\n\n\n\n\nLoad forecast and CHIRPS data\n# Load forecasts\ndf_insiv &lt;- cumulus::blob_read(\n  name = \"ds-aa-lac-dry-corridor/data/processed/insivumeh_special/insivumeh_special_models_zonal_seasonal_chiquimula.parquet\",\n  container = \"projects\"\n)\ndf_seas5 &lt;- seas5$load_seas5_seasonal()\n\n# Load CHIRPS - extracted via GEE script\ndf_chirps_raw &lt;- cumulus::blob_read(\n  name = \"ds-aa-lac-dry-corridor/raw/chirps/2026_cadc_drought_v3_aoi_chirps_monthly_historical.parquet\",\n  container = \"projects\"\n)\n\n# Wrangle CHIRPS to seasonal totals\ndf_chirps &lt;- df_chirps_raw |&gt;\n  filter(ADM1_NAME == \"Chiquimula\") |&gt;\n  mutate(\n    year = year(date),\n    month = month(date),\n    window = case_when(\n      month %in% PRIMERA_MONTHS ~ \"primera\",\n      month %in% POSTRERA_MONTHS ~ \"postrera\",\n      TRUE ~ NA_character_\n    )\n  ) |&gt;\n  filter(!is.na(window), year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n  group_by(year, window) |&gt;\n  summarise(obs_mm = sum(value, na.rm = TRUE), .groups = \"drop\")\n\n# Combine forecasts\ndf_fcst_all &lt;- bind_rows(df_insiv, df_seas5) |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END)\n\n# Filter to operational leadtimes\ndf_fcst_filtered &lt;- df_fcst_all |&gt;\n  mutate(issued_month = month(issued_date)) |&gt;\n  filter(\n    (window == \"primera\" & issued_month %in% PRIMERA_ISSUED_MONTHS) |\n    (window == \"postrera\" & issued_month %in% POSTRERA_ISSUED_MONTHS)\n  )\n\n# Join forecasts with CHIRPS\ndf_joined &lt;- df_fcst_filtered |&gt;\n  left_join(df_chirps, by = c(\"year\", \"window\")) |&gt;\n  filter(!is.na(obs_mm)) # there are not any NA - any ways",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>CHIRPS Tie-Breaker</span>"
    ]
  },
  {
    "objectID": "03_chirps_tiebreaker.html#metrics-against-chirps",
    "href": "03_chirps_tiebreaker.html#metrics-against-chirps",
    "title": "4  CHIRPS Tie-Breaker",
    "section": "4.3 Metrics Against CHIRPS",
    "text": "4.3 Metrics Against CHIRPS\n\n\n\n\n\n\nNoteWhy Rank-Based Metrics Work Across Different Observation Sources\n\n\n\nBecause CHIRPS and ENACTS have different climatologies (CHIRPS shows ~300mm higher Primera totals), comparing raw mm errors across sources would be misleading. Instead, we focus on rank-based metrics:\n\nSpearman correlation: Evaluates the full ranking across all years. If observations rank years as [driest, 2nd, 3rd, … wettest], how well does the forecast reproduce that ordering? Larger rank errors are penalized more heavily.\nROC-AUC: Evaluates separation between drought and non-drought years only. Do drought years consistently get lower forecasts than non-drought years? It ignores ranking within each group—scrambling the order of non-drought years doesn’t affect AUC.\n\nWhen they diverge: A forecast could perfectly separate drought from non-drought (AUC=1) but scramble rankings within each group (moderate Spearman). Or it could track the overall wet-dry gradient well (high Spearman) but fail to place drought years at the bottom (low AUC).\nThese metrics allow fair comparison of forecast skill across observation sources with different absolute values.\n\n\nSpearman & ROC-AUC continue to tell a similar story. Models are skillful in primera with SEAS5 as the dominant competitor. Postrera skill remains low, dubious, and messy. Different models win at different leadtimes with some surprising results of INSIVUMEH provided models showing stronger predictive power at greater leadtimes. SEAS5 remains competitive, but less dominant\n\n\nCalculate thresholds and metrics\n# Forecast thresholds\ndf_fcst_thresh &lt;- df_joined |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  summarise(fcst_thresh = calc_rp_threshold(value), .groups = \"drop\")\n\n# CHIRPS observation thresholds\ndf_obs_thresh &lt;- df_chirps |&gt;\n  group_by(window) |&gt;\n  summarise(obs_thresh = calc_rp_threshold(obs_mm), .groups = \"drop\")\n\n# Join thresholds\ndf_analysis &lt;- df_joined |&gt;\n  left_join(df_fcst_thresh, by = c(\"forecast_source\", \"window\", \"leadtime\")) |&gt;\n  left_join(df_obs_thresh, by = \"window\") |&gt;\n  mutate(\n    fcst_drought = value &lt; fcst_thresh,\n    obs_drought = obs_mm &lt; obs_thresh\n  )\n\n# Continuous metrics\ndf_metrics &lt;- df_analysis |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  summarise(\n    spearman = cor(value, obs_mm, method = \"spearman\", use = \"complete.obs\"),\n    bias_mm = mean(value - obs_mm, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt; \n     mutate(\n      window = fct_relevel(window, \"primera\", \"postrera\")\n    )\n\n\n# vectorized calc AUC\ncalc_auc &lt;- function(truth, drought_score) {                                                                                                                                           \n  if (length(unique(truth)) &lt; 2) return(NA_real_)                                                                                                                           \n  roc_auc_vec(truth, drought_score, event_level = \"first\")                                                                                                                            \n}                                                                                                                                                                                                 \n\ndf_roc &lt;- df_analysis |&gt;\n  mutate(\n    truth = factor(\n      obs_drought,\n      levels = c(TRUE, FALSE),\n      labels = c(\"drought\", \"no_drought\")\n      ),\n    # roc_auc is just rank based, magnitude doesnt matter just invert to +drought +score\n    drought_score = -value\n  )\n\ndf_auc &lt;- df_roc |&gt;                                                                                 \n  group_by(forecast_source, window, leadtime) |&gt;                                                                                                                                      \n  summarise(auc = calc_auc(truth, drought_score), .groups = \"drop\")   \n\n\n\n4.3.1 Spearman Correlation\n\n\nCode\ncreate_metric_heatmap &lt;- function(df, metric_col, metric_name, title, caption, midpoint = 0) {\n  df_plot &lt;- df |&gt;\n    group_by(window, leadtime) |&gt;\n    mutate(is_best = !!sym(metric_col) == max(!!sym(metric_col), na.rm = TRUE)) |&gt;\n    ungroup() \n \n\n  ggplot(df_plot, aes(x = factor(leadtime), y = forecast_source)) +\n    geom_tile(aes(fill = !!sym(metric_col)), color = \"white\", linewidth = 0.5) +\n    geom_tile(\n      data = df_plot |&gt; filter(is_best),\n      fill = NA, color = \"black\", linewidth = 1.5\n    ) +\n    geom_text(aes(label = sprintf(\"%.2f\", !!sym(metric_col))), size = 4, fontface = \"bold\", color = \"black\") +\n    facet_wrap(~window) +\n    scale_fill_gradient2(\n      low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n      midpoint = midpoint, name = metric_name\n    ) +\n    labs(title = title, x = \"Leadtime (months)\", y = NULL, caption = caption) +\n    theme_minimal() +\n    theme(legend.position = \"right\", panel.grid = element_blank(),\n          plot.caption = element_text(hjust = 0))\n}\n\ncreate_metric_heatmap(\n  df = df_metrics,\n  metric_col = \"spearman\",\n  metric_name =\"Spearman ρ\",\n  title = \"Spearman Correlation - CHIRPS\",\n  caption = \"All forecasts validated against CHIRPS. Black border = best per leadtime/season.\"\n)\n\n\n\n\n\n\n\n\n\nSurprise finding: INSIVUMEH_CESM1 shows the strongest Spearman correlation for Primera at LT1 (0.72) and LT2 (0.52), outperforming SEAS5. This is the opposite of what we saw with ERA5 and ENACTS.\n\n\n4.3.2 ROC-AUC\n\n\nCode\ncreate_metric_heatmap(\n  df= df_auc, \n  metric_col = \"auc\",\n  metric_name = \"AUC\",\n  title = \"ROC-AUC - CHIRPS\",\n  caption = \"All forecasts validated against CHIRPS. AUC &gt; 0.7 = acceptable skill. Black border = best.\",\n  midpoint = 0.5\n)\n\n\n\n\n\n\n\n\n\n\n\n4.3.3 F1 Score (Binary)\nFor completeness, here’s the binary F1 score against CHIRPS - the same metric used in Chapter 2. Story remains consistent.\n\n\nCode\n# Calculate F1\ndf_f1 &lt;- df_analysis |&gt;\n  mutate(\n    truth = factor(obs_drought, levels = c(TRUE, FALSE), labels = c(\"drought\", \"no_drought\")),\n    estimate = factor(fcst_drought, levels = c(TRUE, FALSE), labels = c(\"drought\", \"no_drought\"))\n  ) |&gt;\n  group_by(forecast_source, window, leadtime) |&gt;\n  summarise(\n    f1 = f_meas_vec(truth, estimate, event_level = \"first\"),\n    precision = precision_vec(truth, estimate, event_level = \"first\"),\n    recall = recall_vec(truth, estimate, event_level = \"first\"),\n    .groups = \"drop\"\n  )\n\n# F1 heatmap\ndf_plot &lt;- df_f1 |&gt;\n  group_by(window, leadtime) |&gt;\n  mutate(is_best = f1 == max(f1, na.rm = TRUE)) |&gt;\n  ungroup() |&gt; \n  mutate(\n    window = fct_relevel(window, \"primera\",\"postrera\")\n  )\n\nggplot(df_plot, aes(x = factor(leadtime), y = forecast_source)) +\n  geom_tile(aes(fill = f1), color = \"white\", linewidth = 0.5) +\n  geom_tile(\n    data = df_plot |&gt; filter(is_best),\n    fill = NA, color = \"black\", linewidth = 1.5\n  ) +\n  geom_text(aes(label = sprintf(\"%.0f%%\", f1 * 100)), size = 4, fontface = \"bold\", color = \"black\") +\n  facet_wrap(~window) +\n  scale_fill_gradientn(\n    colors = c(\"#D73027\", \"#FC8D59\", \"#FEE08B\", \"#D9EF8B\", \"#91CF60\", \"#1A9850\"),\n    limits = c(0, 1), labels = scales::percent, name = \"F1\"\n  ) +\n  labs(\n    title = \"F1 Score - CHIRPS\",\n    x = \"Leadtime (months)\", y = NULL,\n    caption = \"All forecasts validated against CHIRPS. Black border = best per leadtime/season.\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\", panel.grid = element_blank(),\n        plot.caption = element_text(hjust = 0))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>CHIRPS Tie-Breaker</span>"
    ]
  },
  {
    "objectID": "03_chirps_tiebreaker.html#conclusions",
    "href": "03_chirps_tiebreaker.html#conclusions",
    "title": "4  CHIRPS Tie-Breaker",
    "section": "4.4 Conclusions",
    "text": "4.4 Conclusions\n\n4.4.1 What We’ve Learned So Far\n\nPrimera skill is robust: Multiple models show genuine skill (AUC &gt; 0.7) across all three observation sources. This isn’t an artifact of one particular dataset.\nPostrera remains unresolved: No model shows reliable, consistent skill. CCSM4’s apparent better performance at longer leadtimes is suspicious - skill should not improve with leadtime. This inverted pattern is likely noise from the small sample (only ~6 drought events in 25 years).\n\n\n\n4.4.2 Open Questions\nThe tie-breaker analysis raises more questions than it answers for Postrera:\n\nWhy the inverted skill pattern? CCSM4 showing better skill at LT2-3 than LT1 is backwards - forecast skill should degrade with leadtime. Is this genuine or noise?\nWhich model is actually better? With different metrics favoring different models at different leadtimes, we cannot make a confident recommendation.\nIs the poor skill a data artifact? Could temporal trends in the observation sources be affecting our skill estimates?\n\n\n\n4.4.3 Next Steps\nTo better understand the poor Postrera skill and the contradictory patterns across models, the next chapter examines temporal drift - whether systematic trends in forecasts or observations might explain some of what we’re seeing.\n\n\n\n\n\n\nTipTechnical Details\n\n\n\n\n\n\n4.4.4 CHIRPS vs ENACTS Comparison\nHow different are CHIRPS and ENACTS for Chiquimula? Understanding this helps interpret why forecast skill might differ across observation sources.\n\n\nCompare CHIRPS and ENACTS observations\n# Load ENACTS for comparison\ndf_enacts_compare &lt;- enacts$load_enacts_seasonal(\"chiquimula\")\n\ndf_compare &lt;- df_chirps |&gt;\n  rename(chirps = obs_mm) |&gt;\n  left_join(\n    df_enacts_compare |&gt; select(year, window, enacts = obs_mm),\n    by = c(\"year\", \"window\")\n  ) |&gt;\n  filter(!is.na(enacts))\n\n# Correlation\ncorr_primera &lt;- df_compare |&gt; filter(window == \"primera\") |&gt;\n  summarise(r = cor(chirps, enacts)) |&gt; pull(r)\ncorr_postrera &lt;- df_compare |&gt; filter(window == \"postrera\") |&gt;\n  summarise(r = cor(chirps, enacts)) |&gt; pull(r)\n\ndf_compare |&gt;\n  ggplot(aes(x = enacts, y = chirps)) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"grey50\") +\n  geom_point(alpha = 0.6, size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"#007CE1\", fill = \"#007CE1\", alpha = 0.2) +\n  facet_wrap(~str_to_title(window), scales = \"free\") +\n  labs(\n    title = \"CHIRPS vs ENACTS: Same Region, Different Estimates\",\n    subtitle = sprintf(\"Primera r = %.2f, Postrera r = %.2f\", corr_primera, corr_postrera),\n    x = \"ENACTS (mm)\",\n    y = \"CHIRPS (mm)\",\n    caption = \"Dashed line = 1:1 agreement. CHIRPS consistently higher for Primera.\"\n  )\n\n\n\n\n\n\n\n\n\nCHIRPS and ENACTS are correlated but not identical. CHIRPS tends to estimate higher rainfall for Primera (~300mm more on average). This means a forecast calibrated to one source may not match the other perfectly - making the three-source comparison a meaningful robustness check.\n\n\n4.4.5 Forecast Bias\nBias (mean forecast - observed) is shown for completeness, but is not consequential for our framework since we use rank-based metrics and model-specific thresholds rather than raw precipitation values.\n\n\nCode\ndf_plot &lt;- df_metrics |&gt;\n  group_by(window, leadtime) |&gt;\n  mutate(is_best = abs(bias_mm) == min(abs(bias_mm), na.rm = TRUE)) |&gt;\n  ungroup()\n\nbias_max &lt;- max(abs(df_plot$bias_mm), na.rm = TRUE)\n\nggplot(df_plot, aes(x = factor(leadtime), y = forecast_source)) +\n  geom_tile(aes(fill = bias_mm), color = \"white\", linewidth = 0.5) +\n  geom_tile(\n    data = df_plot |&gt; filter(is_best),\n    fill = NA, color = \"black\", linewidth = 1.5\n  ) +\n  geom_text(aes(label = sprintf(\"%.0f\", bias_mm)), size = 4, fontface = \"bold\", color = \"black\") +\n  facet_wrap(~window) +\n  scale_fill_gradient2(\n    low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n    midpoint = 0, limits = c(-bias_max, bias_max), name = \"Bias (mm)\"\n  ) +\n  labs(\n    title = \"Forecast Bias - CHIRPS\",\n    x = \"Leadtime (months)\", y = NULL,\n    caption = \"Units: millimeters. Bias = mean(forecast - observed). Negative = dry bias.\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\", panel.grid = element_blank(),\n        plot.caption = element_text(hjust = 0))\n\n\n\n\n\n\n\n\n\nAgainst CHIRPS, all models have dry bias for Primera and wet bias for Postrera. SEAS5 has the largest dry bias for Primera (~280mm under), while INSIVUMEH models are closer (~120mm under).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>CHIRPS Tie-Breaker</span>"
    ]
  },
  {
    "objectID": "04_forecast_drift.html",
    "href": "04_forecast_drift.html",
    "title": "5  Temporal Drift",
    "section": "",
    "text": "5.1 Why Check for Drift?\nSkill metrics assume the forecast-observation relationship is stationary over time. If either forecasts or observations have systematic trends (drift), skill estimates may be misleading:\nThis chapter examines temporal trends in both forecasts and observations.\nSetup\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(cumulus)\nlibrary(gghdx)\ngghdx()\n\nbox::use(../../R/enacts)\nbox::use(../../R/seas5)\n\nBASELINE_START &lt;- 2000\nBASELINE_END &lt;- 2024\nPRIMERA_MONTHS &lt;- 5:8\nPOSTRERA_MONTHS &lt;- 9:11\nLoad all data\n# Forecasts\ndf_insiv &lt;- cumulus::blob_read(\n  name = \"ds-aa-lac-dry-corridor/data/processed/insivumeh_special/insivumeh_special_models_zonal_seasonal_chiquimula.parquet\",\n  container = \"projects\"\n)\ndf_seas5 &lt;- seas5$load_seas5_seasonal()\n\n# Observations\ndf_enacts &lt;- enacts$load_enacts_seasonal(\"chiquimula\")\n\n# ERA5\ncon &lt;- pg_con()\ndf_era5_raw &lt;- tbl(con, \"era5\") |&gt;\n  filter(pcode == \"GT20\") |&gt;\n  collect() |&gt;\n  mutate(\n    year = year(valid_date),\n    month = month(valid_date),\n    mean = mean * days_in_month(valid_date)\n  )\nDBI::dbDisconnect(con)\n\ndf_era5 &lt;- bind_rows(\n  df_era5_raw |&gt; rename(value = mean) |&gt;\n    filter(month %in% PRIMERA_MONTHS) |&gt;\n    group_by(year) |&gt;\n    summarise(obs_mm = sum(value, na.rm = TRUE), .groups = \"drop\") |&gt;\n    mutate(window = \"primera\"),\n  df_era5_raw |&gt; rename(value = mean) |&gt;\n    filter(month %in% POSTRERA_MONTHS) |&gt;\n    group_by(year) |&gt;\n    summarise(obs_mm = sum(value, na.rm = TRUE), .groups = \"drop\") |&gt;\n    mutate(window = \"postrera\")\n)\n\n# CHIRPS\ndf_chirps_raw &lt;- cumulus::blob_read(\n  name = \"ds-aa-lac-dry-corridor/raw/chirps/2026_cadc_drought_v3_aoi_chirps_monthly_historical.parquet\",\n  container = \"projects\"\n)\n\ndf_chirps &lt;- df_chirps_raw |&gt;\n  filter(ADM1_NAME == \"Chiquimula\") |&gt;\n  mutate(year = year(date), month = month(date)) |&gt;\n  mutate(\n    window = case_when(\n      month %in% PRIMERA_MONTHS ~ \"primera\",\n      month %in% POSTRERA_MONTHS ~ \"postrera\",\n      TRUE ~ NA_character_\n    )\n  ) |&gt;\n  filter(!is.na(window)) |&gt;\n  group_by(year, window) |&gt;\n  summarise(obs_mm = sum(value, na.rm = TRUE), .groups = \"drop\")\n\n# Combine\ndf_obs_all &lt;- bind_rows(\n  df_enacts |&gt; mutate(source = \"ENACTS\"),\n  df_era5 |&gt; mutate(source = \"ERA5\"),\n  df_chirps |&gt; mutate(source = \"CHIRPS\")\n) |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END)\n\ndf_fcst_all &lt;- bind_rows(df_insiv, df_seas5) |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n  mutate(model = str_remove(forecast_source, \"INSIVUMEH_\"))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Drift</span>"
    ]
  },
  {
    "objectID": "04_forecast_drift.html#why-check-for-drift",
    "href": "04_forecast_drift.html#why-check-for-drift",
    "title": "5  Temporal Drift",
    "section": "",
    "text": "A model that performed well historically might be degrading\nApparent “skill” might reflect coincidental alignment of trends rather than genuine predictive ability\nDifferent observation sources might have different drift characteristics",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Drift</span>"
    ]
  },
  {
    "objectID": "04_forecast_drift.html#observation-drift",
    "href": "04_forecast_drift.html#observation-drift",
    "title": "5  Temporal Drift",
    "section": "5.2 Observation Drift",
    "text": "5.2 Observation Drift\nAre the three observation sources showing consistent trends, or is one drifting differently?\n\n\nCalculate observation trends\nobs_trends &lt;- df_obs_all |&gt;\n  group_by(source, window) |&gt;\n  summarise(\n    n = n(),\n    mean_mm = mean(obs_mm),\n    trend_per_year = coef(lm(obs_mm ~ year))[2],\n    trend_pvalue = summary(lm(obs_mm ~ year))$coefficients[2, 4],\n    total_trend = trend_per_year * (BASELINE_END - BASELINE_START),\n    trend_pct = total_trend / mean_mm * 100,\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(significant = trend_pvalue &lt; 0.1)\n\n\n\n\nCode\nobs_trends |&gt;\n  mutate(\n    trend_summary = sprintf(\"%.1f mm/yr (%+.0f%% total)%s\",\n                            trend_per_year, trend_pct,\n                            ifelse(significant, \"*\", \"\"))\n  ) |&gt;\n  select(source, window, trend_summary) |&gt;\n  pivot_wider(names_from = window, values_from = trend_summary) |&gt;\n  knitr::kable(caption = \"Observation trends 2000-2024. * = p &lt; 0.1\")\n\n\n\nObservation trends 2000-2024. * = p &lt; 0.1\n\n\nsource\npostrera\nprimera\n\n\n\n\nCHIRPS\n3.2 mm/yr (+34% total)\n-3.6 mm/yr (-9% total)\n\n\nENACTS\n-2.3 mm/yr (-10% total)\n-9.4 mm/yr (-36% total)*\n\n\nERA5\n3.8 mm/yr (+17% total)\n-2.8 mm/yr (-8% total)\n\n\n\n\n\n\n\nCode\ndf_obs_all |&gt;\n  ggplot(aes(x = year, y = obs_mm, color = source)) +\n  geom_line(alpha = 0.7) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = TRUE, alpha = 0.2) +\n  facet_wrap(~str_to_title(window), scales = \"free_y\") +\n  labs(\n    title = \"Observation Time Series by Source\",\n    subtitle = \"Lines show linear trends with 95% CI\",\n    x = \"Year\", y = \"Seasonal Rainfall (mm)\", color = \"Source\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nKey finding: ENACTS shows a stronger drying trend for Primera (-9.4 mm/yr) than ERA5 or CHIRPS. This could affect skill comparisons if forecasts are compared against different observation sources.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Drift</span>"
    ]
  },
  {
    "objectID": "04_forecast_drift.html#forecast-drift",
    "href": "04_forecast_drift.html#forecast-drift",
    "title": "5  Temporal Drift",
    "section": "5.3 Forecast Drift",
    "text": "5.3 Forecast Drift\nDo forecasts show systematic trends over time?\n\n\nCode\ndf_fcst_all |&gt;\n  filter(leadtime %in% c(1, 2, 3)) |&gt;\n  ggplot(aes(x = year, y = value, color = model)) +\n  geom_line(alpha = 0.7) +\n  geom_point(alpha = 0.5, size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.8) +\n  facet_grid(rows = vars(paste0(\"LT\", leadtime)), cols = vars(str_to_title(window)), scales = \"free_y\") +\n  labs(\n    title = \"Forecast Time Series by Leadtime\",\n    subtitle = \"Lines show linear trends\",\n    x = \"Year\", y = \"Forecast (mm)\", color = \"Model\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nCalculate forecast trends\nfcst_trends &lt;- df_fcst_all |&gt;\n  filter(leadtime %in% c(1, 2, 3)) |&gt;\n  group_by(model, window, leadtime) |&gt;\n  summarise(\n    trend_per_year = coef(lm(value ~ year))[2],\n    trend_pvalue = summary(lm(value ~ year))$coefficients[2, 4],\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    significant = trend_pvalue &lt; 0.1,\n    trend_fmt = sprintf(\"%.1f%s\", trend_per_year, ifelse(significant, \"*\", \"\"))\n  )\n\nfcst_trends |&gt;\n  select(model, window, leadtime, trend_fmt) |&gt;\n  pivot_wider(names_from = leadtime, values_from = trend_fmt, names_prefix = \"LT\") |&gt;\n  arrange(window, model) |&gt;\n  knitr::kable(caption = \"Forecast trends (mm/year). * = p &lt; 0.1\")\n\n\n\nForecast trends (mm/year). * = p &lt; 0.1\n\n\nmodel\nwindow\nLT1\nLT2\nLT3\n\n\n\n\nCCSM4\npostrera\n-1.1\n-0.4\n-0.5\n\n\nCESM1\npostrera\n-1.1*\n0.0\n0.0\n\n\nCFSv2\npostrera\n0.2\n0.2\n-1.4\n\n\nSEAS5\npostrera\n0.3\n0.6\n0.2\n\n\nCCSM4\nprimera\n-3.2\n-0.8\n-3.1\n\n\nCESM1\nprimera\n-0.1\n-2.3\n-1.7\n\n\nCFSv2\nprimera\n-1.2\n-4.6*\n-2.1\n\n\nSEAS5\nprimera\n-4.5*\n-3.6\n-0.1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Drift</span>"
    ]
  },
  {
    "objectID": "04_forecast_drift.html#differential-drift-forecast-error-over-time",
    "href": "04_forecast_drift.html#differential-drift-forecast-error-over-time",
    "title": "5  Temporal Drift",
    "section": "5.4 Differential Drift (Forecast Error Over Time)",
    "text": "5.4 Differential Drift (Forecast Error Over Time)\nMore important than individual trends is whether the forecast-observation relationship is changing.\n\n\nCode\ndf_error &lt;- df_fcst_all |&gt;\n  filter(leadtime %in% c(1, 2, 3)) |&gt;\n  left_join(df_enacts |&gt; select(year, window, obs_mm), by = c(\"year\", \"window\")) |&gt;\n  mutate(error = value - obs_mm)\n\ndf_error |&gt;\n  ggplot(aes(x = year, y = error, color = model)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"grey50\") +\n  geom_line(alpha = 0.7) +\n  geom_point(alpha = 0.5, size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.8) +\n  facet_grid(rows = vars(paste0(\"LT\", leadtime)), cols = vars(str_to_title(window)), scales = \"free_y\") +\n  labs(\n    title = \"Forecast Error Over Time (vs ENACTS)\",\n    subtitle = \"Positive = wet bias, Negative = dry bias. Lines show trends.\",\n    x = \"Year\", y = \"Error (mm)\", color = \"Model\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nerror_trends &lt;- df_error |&gt;\n  group_by(model, window, leadtime) |&gt;\n  summarise(\n    trend_per_year = coef(lm(error ~ year))[2],\n    trend_pvalue = summary(lm(error ~ year))$coefficients[2, 4],\n    total_drift = trend_per_year * (BASELINE_END - BASELINE_START),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    significant = trend_pvalue &lt; 0.1,\n    drift_fmt = sprintf(\"%.1f (%+.0f mm total)%s\",\n                        trend_per_year, total_drift,\n                        ifelse(significant, \"*\", \"\"))\n  )\n\nerror_trends |&gt;\n  filter(leadtime == 1) |&gt;\n  select(model, window, drift_fmt) |&gt;\n  pivot_wider(names_from = window, values_from = drift_fmt) |&gt;\n  knitr::kable(caption = \"Error drift at LT1 (mm/year). * = p &lt; 0.1\")\n\n\n\nError drift at LT1 (mm/year). * = p &lt; 0.1\n\n\nmodel\npostrera\nprimera\n\n\n\n\nCCSM4\n1.3 (+30 mm total)\n6.2 (+148 mm total)\n\n\nCESM1\n1.3 (+30 mm total)\n9.2 (+222 mm total)*\n\n\nCFSv2\n2.5 (+60 mm total)\n8.2 (+197 mm total)*\n\n\nSEAS5\n2.7 (+64 mm total)\n4.9 (+116 mm total)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Drift</span>"
    ]
  },
  {
    "objectID": "04_forecast_drift.html#split-half-stability",
    "href": "04_forecast_drift.html#split-half-stability",
    "title": "5  Temporal Drift",
    "section": "5.5 Split-Half Stability",
    "text": "5.5 Split-Half Stability\nAnother way to assess stationarity: does skill differ between the first and second half of the record?\n\n\nCode\ndf_split &lt;- df_fcst_all |&gt;\n  filter(leadtime == 1) |&gt;\n  left_join(df_enacts |&gt; select(year, window, obs_mm), by = c(\"year\", \"window\")) |&gt;\n  mutate(period = ifelse(year &lt;= 2012, \"2000-2012\", \"2013-2024\"))\n\nsplit_corr &lt;- df_split |&gt;\n  group_by(model, window, period) |&gt;\n  summarise(cor = cor(value, obs_mm, use = \"complete.obs\"), .groups = \"drop\")\n\nsplit_corr |&gt;\n  ggplot(aes(x = model, y = cor, fill = period)) +\n  geom_col(position = \"dodge\", alpha = 0.8) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  facet_wrap(~str_to_title(window)) +\n  scale_fill_manual(values = c(\"2000-2012\" = \"#1A9850\", \"2013-2024\" = \"#D73027\")) +\n  labs(\n    title = \"Forecast-Observation Correlation by Period (LT1)\",\n    subtitle = \"If skill is stable, bars should be similar height\",\n    x = \"Model\", y = \"Correlation with ENACTS\", fill = \"Period\"\n  ) +\n  theme(legend.position = \"bottom\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nCode\nsplit_corr |&gt;\n  pivot_wider(names_from = period, values_from = cor) |&gt;\n  mutate(change = `2013-2024` - `2000-2012`) |&gt;\n  select(model, window, `2000-2012`, `2013-2024`, change) |&gt;\n  mutate(across(where(is.numeric), ~sprintf(\"%.2f\", .x))) |&gt;\n  pivot_wider(names_from = window, values_from = c(`2000-2012`, `2013-2024`, change)) |&gt;\n  knitr::kable(caption = \"Split-half correlations and change\")\n\n\n\nSplit-half correlations and change\n\n\n\n\n\n\n\n\n\n\n\nmodel\n2000-2012_postrera\n2000-2012_primera\n2013-2024_postrera\n2013-2024_primera\nchange_postrera\nchange_primera\n\n\n\n\nCCSM4\n0.67\n0.46\n0.44\n0.38\n-0.23\n-0.08\n\n\nCESM1\n-0.21\n0.56\n0.52\n0.40\n0.73\n-0.16\n\n\nCFSv2\n0.20\n0.53\n0.20\n0.34\n0.00\n-0.19\n\n\nSEAS5\n0.65\n0.40\n0.09\n0.74\n-0.56\n0.34",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Drift</span>"
    ]
  },
  {
    "objectID": "04_forecast_drift.html#summary",
    "href": "04_forecast_drift.html#summary",
    "title": "5  Temporal Drift",
    "section": "5.6 Summary",
    "text": "5.6 Summary\n\n5.6.1 What We Found\nTemporal drift in climate data products is a known issue across many sources and regions - satellite algorithms evolve, station networks change, and reanalysis systems are updated. We observe drift here as well:\n\nObservations: ENACTS shows a stronger drying trend for Primera than other sources\nForecasts: Some significant trends exist (e.g., SEAS5 Primera drying ~4 mm/yr)\nError drift: INSIVUMEH Primera errors are trending positive, accumulating ~150-200mm over 24 years\nSplit-half stability: Postrera correlations are highly unstable between early and late periods\n\n\n\n5.6.2 Does This Explain Poor Postrera Skill?\nNo. The drift magnitudes are modest relative to interannual variability, and drift affects both seasons similarly. The split-half instability for Postrera is concerning, but this is more likely a symptom of limited sample size than evidence that drift is causing poor skill.\nBottom line: Drift is present but does not explain the contradictory Postrera patterns from Chapter 3. The investigation continues in the next chapter with internal consistency diagnostics.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Drift</span>"
    ]
  },
  {
    "objectID": "05_inter_lt_correlation.html",
    "href": "05_inter_lt_correlation.html",
    "title": "6  Leadtime Diagnostics",
    "section": "",
    "text": "6.1 Motivation\nIn Chapter 4, CCSM4 showed a suspicious pattern for Postrera: skill at LT2-3 exceeded skill at LT1. This is backwards—forecast skill should degrade with increasing leadtime, not improve.\nThis chapter investigates whether this pattern reflects genuine predictability or a data quality issue by examining:\nSetup\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(cumulus)\nlibrary(gghdx)\ngghdx()\n\nbox::use(../../R/enacts)\nbox::use(../../R/seas5)\n\nBASELINE_START &lt;- 2000\nBASELINE_END &lt;- 2024\nPRIMERA_MONTHS &lt;- 5:8\nPOSTRERA_MONTHS &lt;- 9:11\nLoad forecast and observation data\n# Forecasts\ndf_insiv &lt;- cumulus::blob_read(\n  name = \"ds-aa-lac-dry-corridor/data/processed/insivumeh_special/insivumeh_special_models_zonal_seasonal_chiquimula.parquet\",\n  container = \"projects\"\n)\ndf_seas5 &lt;- seas5$load_seas5_seasonal()\n\n# Observations\ndf_enacts &lt;- enacts$load_enacts_seasonal(\"chiquimula\")\n\n# Combine forecasts\ndf_fcst_all &lt;- bind_rows(df_insiv, df_seas5) |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n  mutate(model = str_remove(forecast_source, \"INSIVUMEH_\"))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Leadtime Diagnostics</span>"
    ]
  },
  {
    "objectID": "05_inter_lt_correlation.html#motivation",
    "href": "05_inter_lt_correlation.html#motivation",
    "title": "6  Leadtime Diagnostics",
    "section": "",
    "text": "Inter-leadtime correlations: Do consecutive leadtimes agree with each other?\nCoefficient of Variation (CV): Do forecasts show realistic interannual variability?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Leadtime Diagnostics</span>"
    ]
  },
  {
    "objectID": "05_inter_lt_correlation.html#inter-leadtime-correlations",
    "href": "05_inter_lt_correlation.html#inter-leadtime-correlations",
    "title": "6  Leadtime Diagnostics",
    "section": "6.2 Inter-Leadtime Correlations",
    "text": "6.2 Inter-Leadtime Correlations\nFor a well-behaved forecast system, predictions at different leadtimes for the same target season should be correlated—they’re all trying to predict the same thing. If LT1 and LT2 forecasts are uncorrelated, one or both must be essentially random.\n\n\nFunction to compute correlation matrix\ncompute_lt_corr_matrix &lt;- function(df, model_name, window_name) {\n  df_wide &lt;- df |&gt;\n    filter(model == model_name, window == window_name, !is.na(leadtime)) |&gt;\n    select(year, leadtime, value) |&gt;\n    pivot_wider(names_from = leadtime, values_from = value, names_prefix = \"LT\")\n\n  # Add observations\n  df_obs &lt;- df_enacts |&gt;\n    filter(window == window_name, year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n    select(year, obs_mm)\n\n  df_wide &lt;- df_wide |&gt;\n    left_join(df_obs, by = \"year\") |&gt;\n    rename(OBS = obs_mm)\n\n  # Compute correlation matrix - only keep LT and OBS columns\n  cor_mat &lt;- df_wide |&gt;\n    select(starts_with(\"LT\"), OBS) |&gt;\n    cor(use = \"pairwise.complete.obs\")\n\n  cor_mat\n}\n\n\n\n\nSEAS5 inter-leadtime correlations\nmodels &lt;- unique(df_fcst_all$model)\nwindows &lt;- c(\"primera\", \"postrera\")\n\n# Compute all correlation matrices\ncorr_results &lt;- expand_grid(model = models, window = windows) |&gt;\n  mutate(\n    corr_mat = map2(model, window, ~compute_lt_corr_matrix(df_fcst_all, .x, .y))\n  )\n\n\n\n6.2.1 SEAS5 Correlation Patterns\n\n\nCode\n# Extract SEAS5 matrices\nseas5_primera &lt;- corr_results |&gt;\n  filter(model == \"SEAS5\", window == \"primera\") |&gt;\n  pull(corr_mat) |&gt;\n  pluck(1)\n\nseas5_postrera &lt;- corr_results |&gt;\n  filter(model == \"SEAS5\", window == \"postrera\") |&gt;\n  pull(corr_mat) |&gt;\n  pluck(1)\n\n# Convert to long format for plotting (lower triangle only)\nmat_to_df &lt;- function(mat, label) {\n  var_order &lt;- c(\"LT4\", \"LT3\", \"LT2\", \"LT1\", \"OBS\")\n  # Filter to only include columns that exist in the matrix\n  var_order &lt;- var_order[var_order %in% colnames(mat)]\n\n  as.data.frame(mat) |&gt;\n    mutate(row = rownames(mat)) |&gt;\n    pivot_longer(-row, names_to = \"col\", values_to = \"cor\") |&gt;\n    mutate(\n      label = label,\n      row_idx = match(row, var_order),\n      col_idx = match(col, var_order)\n    ) |&gt;\n    # Keep lower triangle only (row_idx &gt; col_idx)\n    filter(row_idx &gt; col_idx) |&gt;\n    select(-row_idx, -col_idx)\n}\n\ndf_seas5_corr &lt;- bind_rows(\n  mat_to_df(seas5_primera, \"Primera\"),\n  mat_to_df(seas5_postrera, \"Postrera\")\n) |&gt; \n  mutate(\n    label = fct_relevel(label, \"Primera\",\"Postrera\")\n  )\n\ndf_seas5_corr |&gt;\n  mutate(\n    row = factor(row, levels = rev(c(\"LT4\", \"LT3\", \"LT2\", \"LT1\", \"OBS\"))),\n    col = factor(col, levels = c(\"LT4\", \"LT3\", \"LT2\", \"LT1\", \"OBS\"))\n  ) |&gt;\n  ggplot(aes(x = col, y = row, fill = cor)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = sprintf(\"%.2f\", cor)), size = 3, fontface = \"bold\", color = \"black\") +\n  facet_wrap(~label, scales = \"free_x\") +\n  scale_fill_gradient2(low = \"#D73027\", mid = \"white\", high = \"#1A9850\", midpoint = 0.4, limits = c(-1, 1)) +\n  labs(\n    title = \"SEAS5: Inter-Leadtime Correlations\",\n    subtitle = \"Adjacent leadtimes should correlate strongly\",\n    x = NULL, y = NULL, fill = \"Correlation\",\n    caption = \"Omitting leadtime 0 as it is not available from INSVIUMEH provided models\"\n  ) +\n  theme(legend.position = \"right\",plot.caption = element_text(hjust= 0,vjust=-1))\n\n\n\n\n\n\n\n\n\nSEAS5 shows expected behavior: Adjacent leadtimes correlate at 0.6-0.9. LT1 correlates best with observations. This is a well-behaved forecast system.\n\n\n6.2.2 INSIVUMEH Correlation Patterns\n\n\nCode\n# Extract INSIVUMEH matrices for all models\ninsiv_models &lt;- c(\"CFSv2\", \"CCSM4\", \"CESM1\")\n\ndf_insiv_corr &lt;- map_dfr(insiv_models, function(m) {\n  map_dfr(windows, function(w) {\n    mat &lt;- corr_results |&gt;\n      filter(model == m, window == w) |&gt;\n      pull(corr_mat) |&gt;\n      pluck(1)\n\n    if (is.null(mat)) return(NULL)\n\n    mat_to_df(mat, m) |&gt;\n      mutate(window = w)\n  })\n})\n\ndf_insiv_corr |&gt;\n  filter(!is.na(row), !is.na(col)) |&gt;\n  mutate(\n    row = factor(row, levels = rev(c(\"LT4\", \"LT3\", \"LT2\", \"LT1\", \"OBS\"))),\n    col = factor(col, levels = c(\"LT4\", \"LT3\", \"LT2\", \"LT1\", \"OBS\")),\n    window = str_to_title(window),\n    window = fct_relevel(window, \"Primera\",\"Postrera\")\n  ) |&gt;\n  ggplot(aes(x = col, y = row, fill = cor)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = sprintf(\"%.2f\", cor)), size = 2.5, fontface = \"bold\", color = \"black\") +\n  facet_grid(label ~ window, scales = \"free_x\") +\n  scale_fill_gradient2(low = \"#D73027\", mid = \"white\", high = \"#1A9850\", midpoint = 0.4, limits = c(-1, 1)) +\n  labs(\n    title = \"INSIVUMEH Models: Inter-Leadtime Correlations\",\n    subtitle = \"Postrera shows near-zero or negative correlations between leadtimes\",\n    x = NULL, y = NULL, fill = \"Correlation\"\n  ) +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nCritical finding: INSIVUMEH Postrera forecasts show near-zero or negative correlations between leadtimes. When LT1 predicts wet, LT2 might predict dry for the same target season. This is a fundamental data quality issue—these forecasts are not internally consistent.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Leadtime Diagnostics</span>"
    ]
  },
  {
    "objectID": "05_inter_lt_correlation.html#coefficient-of-variation",
    "href": "05_inter_lt_correlation.html#coefficient-of-variation",
    "title": "6  Leadtime Diagnostics",
    "section": "6.3 Coefficient of Variation",
    "text": "6.3 Coefficient of Variation\nAnother diagnostic: do forecasts show realistic interannual variability? A forecast that barely varies from year to year isn’t providing useful information, even if it occasionally correlates with observations.\n\n\nCalculate CV for forecasts and observations\n# Forecast CV\nfcst_cv &lt;- df_fcst_all |&gt;\n  filter(leadtime == 1) |&gt;\n  group_by(model, window) |&gt;\n  summarise(\n    mean_mm = mean(value),\n    sd_mm = sd(value),\n    cv_pct = sd_mm / mean_mm * 100,\n    .groups = \"drop\"\n  )\n\n# Observation CV\nobs_cv &lt;- df_enacts |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n  group_by(window) |&gt;\n  summarise(\n    mean_mm = mean(obs_mm),\n    sd_mm = sd(obs_mm),\n    cv_pct = sd_mm / mean_mm * 100,\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(model = \"ENACTS (Observed)\")\n\n# Combine\ncv_all &lt;- bind_rows(\n  fcst_cv |&gt; mutate(type = \"Forecast\"),\n  obs_cv |&gt; mutate(type = \"Observed\")\n)\n\n\n\n\nCode\ncv_all |&gt;\n  mutate(\n    model = factor(model, levels = c(\"ENACTS (Observed)\", \"SEAS5\", \"CFSv2\", \"CCSM4\", \"CESM1\")),\n    window = fct_relevel(str_to_title(window), \"Primera\", \"Postrera\")\n  ) |&gt;\n  ggplot(aes(x = model, y = cv_pct, fill = type)) +\n  geom_col(alpha = 0.8) +\n  geom_hline(\n    data = obs_cv |&gt; mutate(window = fct_relevel(str_to_title(window), \"Primera\", \"Postrera\")),\n    aes(yintercept = cv_pct),\n    linetype = \"dashed\", color = \"grey30\"\n  ) +\n  facet_wrap(~window) +\n  labs(\n    title = \"Coefficient of Variation: Forecasts vs Observations\",\n    subtitle = \"Dashed line = observed variability. Forecasts should approach this level.\",\n    x = NULL, y = \"CV (%)\", fill = NULL\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\ncv_all |&gt;\n  select(model, window, cv_pct) |&gt;\n  mutate(window = str_to_title(window)) |&gt;\n  pivot_wider(names_from = window, values_from = cv_pct) |&gt;\n  select(model, Primera, Postrera) |&gt;\n  mutate(across(where(is.numeric), ~sprintf(\"%.1f%%\", .x))) |&gt;\n  knitr::kable(caption = \"Coefficient of Variation by Model and Season\")\n\n\n\nCoefficient of Variation by Model and Season\n\n\nmodel\nPrimera\nPostrera\n\n\n\n\nCCSM4\n11.9%\n6.6%\n\n\nCESM1\n16.3%\n3.5%\n\n\nCFSv2\n14.0%\n6.9%\n\n\nSEAS5\n12.5%\n11.1%\n\n\nENACTS (Observed)\n25.9%\n22.6%\n\n\n\n\n\nKey findings (shown here for LT1 for simplicity):\n\nObserved CV: Primera ~14%, Postrera ~23%\nSEAS5: Reasonable CV for both seasons (8-15%), approaching observed variability\nINSIVUMEH Primera: Moderate CV (10-18%), usable signal\nINSIVUMEH Postrera: Very low CV (3-7%), forecasts barely vary from climatology\n\nThis pattern holds across all leadtimes—see the expanded analysis in the collapsible section below.\n\n\n\n\n\n\nTipRobustness Check: CV Across Leadtimes and Observation Sources\n\n\n\n\n\nDoes the low CV pattern for INSIVUMEH Postrera hold across all leadtimes? And how does it compare to multiple observation sources?\n\n\nLoad additional observation sources and calculate CV across leadtimes\n# Forecast CV across all leadtimes\nfcst_cv_all_lt &lt;- df_fcst_all |&gt;\n  group_by(model, window, leadtime) |&gt;\n  summarise(\n    mean_mm = mean(value),\n    sd_mm = sd(value),\n    cv_pct = sd_mm / mean_mm * 100,\n    .groups = \"drop\"\n  )\n\n# Load ERA5\ncon &lt;- pg_con()\ndf_era5_raw &lt;- tbl(con, \"era5\") |&gt;\n  filter(pcode == \"GT20\") |&gt;\n  collect() |&gt;\n  mutate(\n    year = year(valid_date),\n    month = month(valid_date),\n    mean = mean * days_in_month(valid_date)\n  )\nDBI::dbDisconnect(con)\n\ndf_era5 &lt;- bind_rows(\n  df_era5_raw |&gt;\n    filter(month %in% PRIMERA_MONTHS) |&gt;\n    group_by(year) |&gt;\n    summarise(obs_mm = sum(mean), .groups = \"drop\") |&gt;\n    mutate(window = \"primera\"),\n  df_era5_raw |&gt;\n    filter(month %in% POSTRERA_MONTHS) |&gt;\n    group_by(year) |&gt;\n    summarise(obs_mm = sum(mean), .groups = \"drop\") |&gt;\n    mutate(window = \"postrera\")\n) |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END)\n\n# Load CHIRPS\ndf_chirps_raw &lt;- cumulus::blob_read(\n  name = \"ds-aa-lac-dry-corridor/raw/chirps/2026_cadc_drought_v3_aoi_chirps_monthly_historical.parquet\",\n  container = \"projects\"\n)\n\ndf_chirps &lt;- df_chirps_raw |&gt;\n  filter(ADM1_NAME == \"Chiquimula\") |&gt;\n  mutate(\n    year = year(date),\n    month = month(date),\n    window = case_when(\n      month %in% PRIMERA_MONTHS ~ \"primera\",\n      month %in% POSTRERA_MONTHS ~ \"postrera\",\n      TRUE ~ NA_character_\n    )\n  ) |&gt;\n  filter(!is.na(window), year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n  group_by(year, window) |&gt;\n  summarise(obs_mm = sum(value, na.rm = TRUE), .groups = \"drop\")\n\n# Calculate observed CV from all three sources\nobs_cv_all &lt;- bind_rows(\n  df_enacts |&gt;\n    filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END) |&gt;\n    group_by(window) |&gt;\n    summarise(cv_pct = sd(obs_mm) / mean(obs_mm) * 100, .groups = \"drop\") |&gt;\n    mutate(source = \"ENACTS\"),\n  df_era5 |&gt;\n    group_by(window) |&gt;\n    summarise(cv_pct = sd(obs_mm) / mean(obs_mm) * 100, .groups = \"drop\") |&gt;\n    mutate(source = \"ERA5\"),\n  df_chirps |&gt;\n    group_by(window) |&gt;\n    summarise(cv_pct = sd(obs_mm) / mean(obs_mm) * 100, .groups = \"drop\") |&gt;\n    mutate(source = \"CHIRPS\")\n)\n\n\n\n\nCreate color-coded CV table\nlibrary(gt)\n\n# Get observed CV range for reference\nobs_cv_summary &lt;- obs_cv_all |&gt;\n  group_by(window) |&gt;\n  summarise(\n    obs_mean = mean(cv_pct),\n    obs_min = min(cv_pct),\n    obs_max = max(cv_pct),\n    .groups = \"drop\"\n  )\n\n# Create wide table for gt\ndf_cv_wide &lt;- fcst_cv_all_lt |&gt;\n  mutate(\n    col_name = paste0(str_to_title(window), \" LT\", leadtime)\n  ) |&gt;\n  select(model, col_name, cv_pct) |&gt;\n  pivot_wider(names_from = col_name, values_from = cv_pct)\n\n# Add observed row (using mean across sources)\nobs_row &lt;- obs_cv_all |&gt;\n  group_by(window) |&gt;\n  summarise(cv_pct = mean(cv_pct), .groups = \"drop\") |&gt;\n  mutate(col_name = paste0(str_to_title(window), \" (Obs)\")) |&gt;\n  pivot_wider(names_from = col_name, values_from = cv_pct) |&gt;\n  mutate(model = \"Observed (mean)\")\n\n# Combine and order\ndf_cv_wide &lt;- df_cv_wide |&gt;\n  mutate(model = factor(model, levels = c(\"SEAS5\", \"CFSv2\", \"CCSM4\", \"CESM1\"))) |&gt;\n  arrange(model) |&gt;\n  mutate(model = as.character(model))\n\n# Define column order\nprimera_cols &lt;- paste0(\"Primera LT\", 1:3)\npostrera_cols &lt;- paste0(\"Postrera LT\", 1:3)\nall_cols &lt;- c(primera_cols, postrera_cols)\nall_cols &lt;- all_cols[all_cols %in% names(df_cv_wide)]\n\ndf_cv_wide |&gt;\n  select(model, all_of(all_cols)) |&gt;\n  gt(rowname_col = \"model\") |&gt;\n  tab_header(\n    title = \"Coefficient of Variation by Model, Season, and Leadtime\",\n    subtitle = \"Higher CV = more interannual variability (closer to observations)\"\n  ) |&gt;\n  tab_spanner(label = \"Primera\", columns = starts_with(\"Primera\")) |&gt;\n  tab_spanner(label = \"Postrera\", columns = starts_with(\"Postrera\")) |&gt;\n  fmt_number(columns = everything(), decimals = 1, pattern = \"{x}%\") |&gt;\n  data_color(\n    columns = starts_with(\"Primera\"),\n    palette = c(\"#D73027\", \"#FFFFBF\", \"#1A9850\"),\n    domain = c(0, obs_cv_summary |&gt; filter(window == \"primera\") |&gt; pull(obs_mean))\n  ) |&gt;\n  data_color(\n    columns = starts_with(\"Postrera\"),\n    palette = c(\"#D73027\", \"#FFFFBF\", \"#1A9850\"),\n    domain = c(0, obs_cv_summary |&gt; filter(window == \"postrera\") |&gt; pull(obs_mean))\n  ) |&gt;\n  tab_source_note(\n    source_note = sprintf(\n      \"Observed CV: Primera %.0f-%.0f%%, Postrera %.0f-%.0f%% (across ENACTS, ERA5, CHIRPS)\",\n      obs_cv_summary |&gt; filter(window == \"primera\") |&gt; pull(obs_min),\n      obs_cv_summary |&gt; filter(window == \"primera\") |&gt; pull(obs_max),\n      obs_cv_summary |&gt; filter(window == \"postrera\") |&gt; pull(obs_min),\n      obs_cv_summary |&gt; filter(window == \"postrera\") |&gt; pull(obs_max)\n    )\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_stub()\n  ) |&gt;\n  cols_label_with(fn = ~str_remove(.x, \"Primera |Postrera \"))\n\n\n\n\n\n\n\n\nCoefficient of Variation by Model, Season, and Leadtime\n\n\nHigher CV = more interannual variability (closer to observations)\n\n\n\nPrimera\nPostrera\n\n\nLT1\nLT2\nLT3\nLT1\nLT2\nLT3\n\n\n\n\nSEAS5\n12.5%\n11.8%\n10.5%\n11.1%\n11.2%\n11.5%\n\n\nCFSv2\n14.0%\n11.5%\n10.7%\n6.9%\n5.1%\n6.1%\n\n\nCCSM4\n11.9%\n12.2%\n10.1%\n6.6%\n6.8%\n9.0%\n\n\nCESM1\n16.3%\n11.7%\n12.3%\n3.5%\n3.2%\n6.3%\n\n\n\nObserved CV: Primera 18-26%, Postrera 22-34% (across ENACTS, ERA5, CHIRPS)\n\n\n\n\n\n\n\n\n\n\nCode\n# Postrera-only bar plot with labeled observation lines\nobs_cv_postrera &lt;- obs_cv_all |&gt;\n  filter(window == \"postrera\") |&gt;\n  mutate(label_x = 3.5)  # Position for labels\n\nfcst_cv_all_lt |&gt;\n  filter(window == \"postrera\") |&gt;\n  ggplot(aes(x = factor(leadtime), y = cv_pct, fill = model)) +\n  geom_col(position = \"dodge\", alpha = 0.8) +\n  geom_hline(\n    data = obs_cv_postrera,\n    aes(yintercept = cv_pct, linetype = source),\n    inherit.aes = FALSE,\n    color = \"grey30\"\n  ) +\n  geom_text(\n    data = obs_cv_postrera,\n    aes(x = label_x, y = cv_pct, label = source),\n    inherit.aes = FALSE,\n    hjust = 0, vjust = -0.3, size = 3, color = \"grey30\"\n  ) +\n  scale_linetype_manual(values = c(\"ENACTS\" = \"dashed\", \"ERA5\" = \"dotted\", \"CHIRPS\" = \"longdash\")) +\n  coord_cartesian(xlim = c(0.5, 4.2), clip = \"off\") +\n  labs(\n    title = \"Postrera: Forecast CV vs Observed Variability\",\n    subtitle = \"INSIVUMEH models show almost no interannual variation\",\n    x = \"Leadtime\", y = \"CV (%)\", fill = \"Model\"\n  ) +\n  theme(legend.position = \"bottom\") +\n  guides(linetype = \"none\")\n\n\n\n\n\n\n\n\n\nThe pattern holds: INSIVUMEH Postrera CV remains very low (3-7%) across all leadtimes, far below observed variability from any source. This is not a leadtime-specific issue—the Postrera forecasts systematically lack interannual signal.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Leadtime Diagnostics</span>"
    ]
  },
  {
    "objectID": "05_inter_lt_correlation.html#implications",
    "href": "05_inter_lt_correlation.html#implications",
    "title": "6  Leadtime Diagnostics",
    "section": "6.4 Implications",
    "text": "6.4 Implications\nThe diagnostic tests reveal a fundamental issue with INSIVUMEH Postrera forecasts:\n\n\n\n\n\n\n\n\n\nDiagnostic\nSEAS5\nINSIVUMEH Primera\nINSIVUMEH Postrera\n\n\n\n\nInter-LT correlation\nHigh (0.6-0.9)\nModerate (0.3-0.7)\nNear-zero or negative\n\n\nCV vs observed\n~70% of observed\n~80% of observed\n~20% of observed\n\n\nSignal strength\nStrong\nModerate\nLimited\n\n\n\nThis explains the suspicious patterns in Chapter 4:\n\nWhy CCSM4 LT3 &gt; LT1 for Postrera: If leadtimes are uncorrelated, apparent “skill” at any leadtime is random. By chance, LT3 happened to correlate better with observations than LT1 in this sample.\nWhy INSIVUMEH Postrera shows erratic skill: The forecasts contain almost no signal—they’re essentially flat lines near climatology with random noise.\nWhy skill scores are unreliable: When forecasts don’t vary, correlations are dominated by noise and small-sample coincidences.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Leadtime Diagnostics</span>"
    ]
  },
  {
    "objectID": "05_inter_lt_correlation.html#the-real-tie-breaker-for-postrera",
    "href": "05_inter_lt_correlation.html#the-real-tie-breaker-for-postrera",
    "title": "6  Leadtime Diagnostics",
    "section": "6.5 The Real Tie-Breaker for Postrera",
    "text": "6.5 The Real Tie-Breaker for Postrera\nThe skill metrics in earlier chapters were inconclusive for Postrera—no model showed consistent skill, and the patterns were erratic. These diagnostics break the tie.\nThe issue isn’t that INSIVUMEH Postrera forecasts lack skill—it’s that they lack signal. A forecast that barely varies from climatology and shows no internal consistency between leadtimes cannot be operationally useful, regardless of what correlation metrics suggest.\nPostrera recommendation: SEAS5\nNot because SEAS5 has demonstrated strong Postrera skill (it hasn’t), but because:\n\nSEAS5 is internally consistent—different leadtimes agree with each other\nSEAS5 shows realistic interannual variability\nSEAS5 at least behaves like a forecast, even if predictability is limited\n\nINSIVUMEH Postrera forecasts show limited predictive signal for this region and season.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Leadtime Diagnostics</span>"
    ]
  },
  {
    "objectID": "05_inter_lt_correlation.html#final-recommendations",
    "href": "05_inter_lt_correlation.html#final-recommendations",
    "title": "6  Leadtime Diagnostics",
    "section": "6.6 Final Recommendations",
    "text": "6.6 Final Recommendations\n\n\n\n\n\n\n\n\nSeason\nRecommended\nRationale\n\n\n\n\nPrimera\nSEAS5\nClear skill advantage across metrics and observation sources\n\n\nPostrera\nSEAS5\nINSIVUMEH forecasts show limited signal (low CV, weak inter-leadtime consistency)\n\n\n\nFor Postrera specifically, consider:\n\nAcknowledging limited predictability in operational communications\nUsing shorter-range forecasts when available\nSupplementing forecast-based triggers with monitoring-based indicators (e.g., observed rainfall deficits or VHI during early Postrera)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Leadtime Diagnostics</span>"
    ]
  },
  {
    "objectID": "06_enso_stratification.html",
    "href": "06_enso_stratification.html",
    "title": "7  ENSO Stratification",
    "section": "",
    "text": "7.1 Motivation\nPrevious chapters established that Postrera forecasts have limited skill compared to Primera. But skill metrics computed over the full historical record may mask important variation: are there conditions under which Postrera forecasts are more or less reliable?\nENSO (El Niño-Southern Oscillation) is the dominant mode of interannual climate variability affecting Central America. This chapter investigates whether forecast skill differs by ENSO phase, with a focus on identifying when Postrera forecasts might be trustworthy.\nSetup\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(cumulus)\nlibrary(gghdx)\ngghdx()\n\nbox::use(../../R/enacts)\nbox::use(../../R/seas5)\n\nBASELINE_START &lt;- 2000\nBASELINE_END &lt;- 2024\nPRIMERA_MONTHS &lt;- 5:8\nPOSTRERA_MONTHS &lt;- 9:11\nLoad all data\n# Forecasts\ndf_insiv &lt;- cumulus::blob_read(\n  name = \"ds-aa-lac-dry-corridor/data/processed/insivumeh_special/insivumeh_special_models_zonal_seasonal_chiquimula.parquet\",\n  container = \"projects\"\n)\ndf_seas5 &lt;- seas5$load_seas5_seasonal()\n\n# ENACTS observations\ndf_enacts &lt;- enacts$load_enacts_seasonal(\"chiquimula\")\n\n# ERA5 observations\ncon &lt;- pg_con()\ndf_era5_raw &lt;- tbl(con, \"era5\") |&gt;\n  filter(pcode == \"GT20\") |&gt;\n  collect() |&gt;\n  mutate(\n    year = year(valid_date),\n    month = month(valid_date),\n    mean = mean * days_in_month(valid_date)\n  )\nDBI::dbDisconnect(con)\n\ndf_era5 &lt;- bind_rows(\n df_era5_raw |&gt; rename(value = mean) |&gt;\n    filter(month %in% PRIMERA_MONTHS) |&gt;\n    group_by(year) |&gt;\n    summarise(obs_mm = sum(value, na.rm = TRUE), .groups = \"drop\") |&gt;\n    mutate(window = \"primera\"),\n  df_era5_raw |&gt; rename(value = mean) |&gt;\n    filter(month %in% POSTRERA_MONTHS) |&gt;\n    group_by(year) |&gt;\n    summarise(obs_mm = sum(value, na.rm = TRUE), .groups = \"drop\") |&gt;\n    mutate(window = \"postrera\")\n)\n\n# CHIRPS observations\ndf_chirps_raw &lt;- cumulus::blob_read(\n  name = \"ds-aa-lac-dry-corridor/raw/chirps/2026_cadc_drought_v3_aoi_chirps_monthly_historical.parquet\",\n  container = \"projects\"\n)\n\ndf_chirps &lt;- df_chirps_raw |&gt;\n  filter(ADM1_NAME == \"Chiquimula\") |&gt;\n  mutate(year = year(date), month = month(date)) |&gt;\n  mutate(\n    window = case_when(\n      month %in% PRIMERA_MONTHS ~ \"primera\",\n      month %in% POSTRERA_MONTHS ~ \"postrera\",\n      TRUE ~ NA_character_\n    )\n  ) |&gt;\n  filter(!is.na(window)) |&gt;\n  group_by(year, window) |&gt;\n  summarise(obs_mm = sum(value, na.rm = TRUE), .groups = \"drop\")\n\n# Combine observations\ndf_obs_all &lt;- bind_rows(\n  df_enacts |&gt; mutate(obs_source = \"ENACTS\"),\n  df_era5 |&gt; mutate(obs_source = \"ERA5\"),\n  df_chirps |&gt; mutate(obs_source = \"CHIRPS\")\n) |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END)\n\n# ONI data\ndf_oni_raw &lt;- cumulus::load_oni()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ENSO Stratification</span>"
    ]
  },
  {
    "objectID": "06_enso_stratification.html#enso-classification",
    "href": "06_enso_stratification.html#enso-classification",
    "title": "7  ENSO Stratification",
    "section": "7.2 ENSO Classification",
    "text": "7.2 ENSO Classification\nWe classify ENSO phase using the Oceanic Niño Index (ONI) for the concurrent season: - Primera (May-Aug): Use May-Aug (MJJA) ONI - Postrera (Sep-Nov): Use Sep-Nov (SON) ONI\nWe compare two classification approaches:\n\n“Any month”: El Niño if any month has ONI ≥ +0.5°C; La Niña if any month ≤ -0.5°C\n“Seasonal average”: El Niño if seasonal mean ONI ≥ +0.5°C; La Niña if mean ≤ -0.5°C\n\n\n\nENSO classification function\nclassify_enso &lt;- function(\n    df_oni_raw,\n    months,\n    threshold_type = c(\"any\", \"average\"),\n    threshold = 0.5,\n    start_year = BASELINE_START,\n    end_year = BASELINE_END\n) {\n  threshold_type &lt;- match.arg(threshold_type)\n\n  df_seasonal &lt;- df_oni_raw |&gt;\n    rename(year = yr, month = mon, oni = anom) |&gt;\n    filter(month %in% months) |&gt;\n    group_by(year) |&gt;\n    summarise(\n      oni_mean = mean(oni, na.rm = TRUE),\n      oni_max = max(oni, na.rm = TRUE),\n      oni_min = min(oni, na.rm = TRUE),\n      .groups = \"drop\"\n    )\n\n  df_seasonal |&gt;\n    mutate(\n      enso_phase = case_when(\n        threshold_type == \"any\" & oni_max &gt;= threshold & oni_min &gt; -threshold ~ \"El Nino\",\n        threshold_type == \"any\" & oni_min &lt;= -threshold & oni_max &lt; threshold ~ \"La Nina\",\n        threshold_type == \"average\" & oni_mean &gt;= threshold ~ \"El Nino\",\n        threshold_type == \"average\" & oni_mean &lt;= -threshold ~ \"La Nina\",\n        TRUE ~ \"Neutral\"\n      )\n    ) |&gt;\n    filter(year &gt;= start_year, year &lt;= end_year)\n}\n\n\n\n\nProcess ONI data with both methods\n# \"Any month\" threshold\ndf_oni_any &lt;- bind_rows(\n  classify_enso(df_oni_raw, months = PRIMERA_MONTHS, threshold_type = \"any\") |&gt;\n    mutate(window = \"primera\"),\n  classify_enso(df_oni_raw, months = POSTRERA_MONTHS, threshold_type = \"any\") |&gt;\n    mutate(window = \"postrera\")\n)\n\n# \"Seasonal average\" threshold\ndf_oni_avg &lt;- bind_rows(\n  classify_enso(df_oni_raw, months = PRIMERA_MONTHS, threshold_type = \"average\") |&gt;\n    mutate(window = \"primera\"),\n  classify_enso(df_oni_raw, months = POSTRERA_MONTHS, threshold_type = \"average\") |&gt;\n    mutate(window = \"postrera\")\n)\n\n\n\n\nSample size plotting function\nplot_sample_sizes &lt;- function(df_oni, subtitle) {\n  df_oni |&gt;\n    mutate(\n      enso_phase = factor(enso_phase, levels = c(\"La Nina\", \"Neutral\", \"El Nino\")),\n      window = factor(window, levels = c(\"primera\", \"postrera\"))\n    ) |&gt;\n    count(window, enso_phase) |&gt;\n    ggplot(aes(x = enso_phase, y = n, fill = enso_phase)) +\n    geom_col(alpha = 0.85) +\n    geom_text(aes(label = n), vjust = -0.5, fontface = \"bold\", size = 5) +\n    facet_wrap(~str_to_title(window)) +\n    scale_fill_manual(\n      values = c(\"La Nina\" = \"#2166AC\", \"Neutral\" = \"#B0B0B0\", \"El Nino\" = \"#B2182B\")\n    ) +\n    scale_y_continuous(limits = c(0, 20)) +\n    labs(\n      title = \"Sample Sizes by ENSO Phase (2000-2024)\",\n      subtitle = subtitle,\n      x = NULL, y = \"Number of Years\"\n    ) +\n    theme(legend.position = \"none\")\n}\n\n\n\nAny MonthSeasonal Average\n\n\n\n\nCode\nplot_sample_sizes(df_oni_any, \"Any month ≥ ±0.5°C\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_sample_sizes(df_oni_avg, \"Seasonal mean ≥ ±0.5°C\")\n\n\n\n\n\n\n\n\n\n\n\n\nThe “any month” threshold classifies more years as active ENSO (especially for Primera), while “seasonal average” results in more Neutral classifications.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ENSO Stratification</span>"
    ]
  },
  {
    "objectID": "06_enso_stratification.html#postrera-skill-by-enso-phase",
    "href": "06_enso_stratification.html#postrera-skill-by-enso-phase",
    "title": "7  ENSO Stratification",
    "section": "7.3 Postrera Skill by ENSO Phase",
    "text": "7.3 Postrera Skill by ENSO Phase\nWe examine SEAS5 ranking ability (Spearman correlation) at leadtime 1 by ENSO phase, validated against all three observation sources.\n\n\nJoin forecast and observation data\ndf_fcst &lt;- df_seas5 |&gt;\n  filter(year &gt;= BASELINE_START, year &lt;= BASELINE_END, leadtime == 1)\n\n# Join with \"any month\" classification\ndf_joined_any &lt;- df_fcst |&gt;\n  left_join(df_obs_all, by = c(\"year\", \"window\")) |&gt;\n  left_join(df_oni_any |&gt; select(year, window, enso_phase), by = c(\"year\", \"window\")) |&gt;\n  filter(!is.na(obs_mm), !is.na(enso_phase))\n\n# Join with \"seasonal average\" classification\ndf_joined_avg &lt;- df_fcst |&gt;\n  left_join(df_obs_all, by = c(\"year\", \"window\")) |&gt;\n  left_join(df_oni_avg |&gt; select(year, window, enso_phase), by = c(\"year\", \"window\")) |&gt;\n  filter(!is.na(obs_mm), !is.na(enso_phase))\n\n\n\n\nSkill calculation and plotting functions\ncalc_postrera_skill &lt;- function(df_joined) {\n  df_joined |&gt;\n    filter(window == \"postrera\") |&gt;\n    group_by(obs_source, enso_phase) |&gt;\n    summarise(\n      n_years = n(),\n      spearman = cor(value, obs_mm, method = \"spearman\"),\n      .groups = \"drop\"\n    ) |&gt;\n    mutate(enso_phase = factor(enso_phase, levels = c(\"La Nina\", \"Neutral\", \"El Nino\")))\n}\n\nplot_postrera_skill &lt;- function(postrera_skill, subtitle) {\n  postrera_skill |&gt;\n    ggplot(aes(x = enso_phase, y = spearman, fill = enso_phase)) +\n    geom_col(alpha = 0.85, width = 0.6) +\n    geom_hline(yintercept = 0, linetype = \"solid\", color = \"grey40\") +\n    geom_hline(yintercept = 0.5, linetype = \"dotted\", color = \"#1A9850\", linewidth = 0.8) +\n    geom_text(aes(label = sprintf(\"%.2f\", spearman)), vjust = -0.5, fontface = \"bold\", size = 3.5) +\n    facet_wrap(~obs_source) +\n    scale_fill_manual(\n      values = c(\"La Nina\" = \"#2166AC\", \"Neutral\" = \"#B0B0B0\", \"El Nino\" = \"#B2182B\")\n    ) +\n    scale_y_continuous(limits = c(-0.3, 1.05)) +\n    labs(\n      title = \"SEAS5 Postrera Ranking Skill by ENSO Phase (LT1)\",\n      subtitle = subtitle,\n      x = NULL, y = \"Spearman Correlation\"\n    ) +\n    theme(legend.position = \"none\")\n}\n\ntable_postrera_skill &lt;- function(postrera_skill) {\n  postrera_skill |&gt;\n    mutate(spearman = round(spearman, 2)) |&gt;\n    select(obs_source, enso_phase, spearman) |&gt;\n    pivot_wider(names_from = enso_phase, values_from = spearman) |&gt;\n    select(obs_source, `La Nina`, Neutral, `El Nino`) |&gt;\n    knitr::kable(\n      col.names = c(\"Obs Source\", \"La Niña\", \"Neutral\", \"El Niño\"),\n      caption = \"SEAS5 Postrera Spearman correlation by ENSO phase and observation source (LT1)\"\n    )\n}\n\npostrera_skill_any &lt;- calc_postrera_skill(df_joined_any)\npostrera_skill_avg &lt;- calc_postrera_skill(df_joined_avg)\n\n\n\nAny MonthSeasonal Average\n\n\n\n\nCode\ntable_postrera_skill(postrera_skill_any)\n\n\n\nSEAS5 Postrera Spearman correlation by ENSO phase and observation source (LT1)\n\n\nObs Source\nLa Niña\nNeutral\nEl Niño\n\n\n\n\nCHIRPS\n0.12\n0.60\n0.32\n\n\nENACTS\n0.49\n0.54\n0.13\n\n\nERA5\n0.44\n0.94\n0.00\n\n\n\n\n\n\n\nCode\nplot_postrera_skill(postrera_skill_any, \"Any month ≥ ±0.5°C. Dotted line = moderate skill (0.5).\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntable_postrera_skill(postrera_skill_avg)\n\n\n\nSEAS5 Postrera Spearman correlation by ENSO phase and observation source (LT1)\n\n\nObs Source\nLa Niña\nNeutral\nEl Niño\n\n\n\n\nCHIRPS\n0.12\n0.31\n0.29\n\n\nENACTS\n0.47\n0.52\n0.33\n\n\nERA5\n0.43\n0.69\n-0.17\n\n\n\n\n\n\n\nCode\nplot_postrera_skill(postrera_skill_avg, \"Seasonal mean ≥ ±0.5°C. Dotted line = moderate skill (0.5).\")\n\n\n\n\n\n\n\n\n\n\n\n\nKey finding: Results vary by ENSO classification method and observation source. With the “any month” threshold, El Niño years show consistently low skill, while Neutral years show the highest skill.\nThe results are sensitive to observation source choice, which complicates interpretation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ENSO Stratification</span>"
    ]
  },
  {
    "objectID": "06_enso_stratification.html#the-operational-question-can-we-use-seas5-during-el-niño",
    "href": "06_enso_stratification.html#the-operational-question-can-we-use-seas5-during-el-niño",
    "title": "7  ENSO Stratification",
    "section": "7.4 The Operational Question: Can We Use SEAS5 During El Niño?",
    "text": "7.4 The Operational Question: Can We Use SEAS5 During El Niño?\nThe within-phase correlation analysis above has limited statistical power with only 6-10 years per phase. A more practical question is: “If I know it’s an El Niño year, is the SEAS5 forecast more or less reliable than usual?”\n\n\nOperational analysis setup\n# Use ENACTS as reference, rank against ALL 25 years\ndf_operational &lt;- df_joined_any |&gt;\n  filter(window == \"postrera\", obs_source == \"ENACTS\") |&gt;\n  mutate(\n    # Rank against all years (not within phase)\n    obs_rank_all = rank(obs_mm),\n    fcst_rank_all = rank(value),\n    rank_error = abs(obs_rank_all - fcst_rank_all),\n    abs_error = abs(value - obs_mm),\n    bias = value - obs_mm\n  )\n\n\n\n7.4.1 What SEAS5 Gets Right: The Average ENSO Signal\n\n\nCode\ndf_operational |&gt;\n  group_by(enso_phase) |&gt;\n  summarise(\n    n = n(),\n    `Observed Mean (mm)` = round(mean(obs_mm)),\n    `Forecast Mean (mm)` = round(mean(value)),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(`Observed Mean (mm)`) |&gt;\n  knitr::kable(caption = \"SEAS5 correctly predicts El Niño as driest on average\")\n\n\n\nSEAS5 correctly predicts El Niño as driest on average\n\n\nenso_phase\nn\nObserved Mean (mm)\nForecast Mean (mm)\n\n\n\n\nEl Nino\n9\n513\n375\n\n\nLa Nina\n10\n542\n445\n\n\nNeutral\n6\n603\n426\n\n\n\n\n\nSEAS5 captures the average ENSO signal: El Niño years are driest (513mm observed), and SEAS5 predicts them as driest (375mm forecast). The phase ordering is correct.\n\n\n7.4.2 What SEAS5 Gets Wrong: Individual Year Accuracy\n\n\nCode\ndf_operational |&gt;\n  group_by(enso_phase) |&gt;\n  summarise(\n    n = n(),\n    `Mean Abs Error (mm)` = round(mean(abs_error)),\n    `Mean Rank Error` = round(mean(rank_error), 1),\n    .groups = \"drop\"\n  ) |&gt;\n  knitr::kable(caption = \"Forecast error by ENSO phase (ranked against all 25 years)\")\n\n\n\nForecast error by ENSO phase (ranked against all 25 years)\n\n\nenso_phase\nn\nMean Abs Error (mm)\nMean Rank Error\n\n\n\n\nEl Nino\n9\n139\n6.2\n\n\nLa Nina\n10\n119\n5.7\n\n\nNeutral\n6\n177\n4.8\n\n\n\n\n\nEl Niño years have the highest rank error (6.2 positions) - meaning SEAS5 systematically misplaces El Niño years relative to where they actually fall.\n\n\n7.4.3 The Critical Failure: El Niño Year Details\n\n\nCode\ndf_operational |&gt;\n  filter(enso_phase == \"El Nino\") |&gt;\n  arrange(obs_rank_all) |&gt;\n  select(year, obs_mm, value, obs_rank_all, fcst_rank_all, rank_error) |&gt;\n  mutate(\n    obs_mm = round(obs_mm),\n    value = round(value)\n  ) |&gt;\n  knitr::kable(\n    col.names = c(\"Year\", \"Observed (mm)\", \"Forecast (mm)\", \"Obs Rank (of 25)\", \"Fcst Rank (of 25)\", \"Rank Error\"),\n    caption = \"El Niño Postrera years ranked against ALL 25 years\"\n  )\n\n\n\nEl Niño Postrera years ranked against ALL 25 years\n\n\n\n\n\n\n\n\n\n\nYear\nObserved (mm)\nForecast (mm)\nObs Rank (of 25)\nFcst Rank (of 25)\nRank Error\n\n\n\n\n2009\n372\n358\n2\n4\n2\n\n\n2019\n397\n402\n4\n9\n5\n\n\n2023\n463\n424\n6\n13\n7\n\n\n2015\n465\n328\n7\n1\n6\n\n\n2018\n485\n338\n8\n2\n6\n\n\n2002\n491\n348\n9\n3\n6\n\n\n2004\n538\n382\n13\n8\n5\n\n\n2006\n545\n431\n14\n14\n0\n\n\n2014\n857\n366\n25\n6\n19\n\n\n\n\n\nThe 2014 problem: This El Niño year was the wettest Postrera in 25 years (857mm, rank 25), but SEAS5 predicted it as moderately dry (366mm, rank 6). A 19-position error is catastrophic for operational use.\nSeveral other El Niño years (2015, 2018, 2002) show 6-position errors where SEAS5 predicted them as the driest years but they were actually mid-range.\n\n\n7.4.4 Drought Detection During El Niño (ROC-AUC)\nThe ranking analysis above focused on all years, but operationally we care about drought detection. Can SEAS5 identify which El Niño years will be droughts (RP4 threshold)?\n\n\nCode\n# RP4 drought threshold\ndrought_thresh &lt;- quantile(\n  df_joined_any |&gt; filter(window == \"postrera\", obs_source == \"ENACTS\") |&gt; pull(obs_mm),\n  0.25\n)\n\n\n\n\nCode\ndf_drought &lt;- df_joined_any |&gt;\n  filter(window == \"postrera\", obs_source == \"ENACTS\") |&gt;\n  mutate(is_drought = obs_mm &lt;= drought_thresh)\n\ndf_drought |&gt;\n  group_by(enso_phase) |&gt;\n  summarise(\n    `Years` = n(),\n    `Droughts` = sum(is_drought),\n    `Drought Rate` = scales::percent(mean(is_drought), accuracy = 1),\n    .groups = \"drop\"\n  ) |&gt;\n  knitr::kable(caption = sprintf(\"Drought frequency by ENSO phase (threshold: %d mm)\", round(drought_thresh)))\n\n\n\nDrought frequency by ENSO phase (threshold: 465 mm)\n\n\nenso_phase\nYears\nDroughts\nDrought Rate\n\n\n\n\nEl Nino\n9\n4\n44%\n\n\nLa Nina\n10\n2\n20%\n\n\nNeutral\n6\n1\n17%\n\n\n\n\n\nEl Niño years have a 44% drought rate - more than double the rate for other phases. This is valuable climatological information.\n\n\nCode\ndf_elnino &lt;- df_drought |&gt; filter(enso_phase == \"El Nino\")\n\ndf_elnino |&gt;\n  arrange(obs_mm) |&gt;\n  mutate(\n    obs_mm = round(obs_mm),\n    value = round(value),\n    fcst_rank = rank(value)\n  ) |&gt;\n  select(year, obs_mm, value, is_drought, fcst_rank) |&gt;\n  knitr::kable(\n    col.names = c(\"Year\", \"Observed (mm)\", \"Forecast (mm)\", \"Drought?\", \"Fcst Rank\"),\n    caption = \"El Niño years sorted by observed rainfall\"\n  )\n\n\n\nEl Niño years sorted by observed rainfall\n\n\nYear\nObserved (mm)\nForecast (mm)\nDrought?\nFcst Rank\n\n\n\n\n2009\n372\n358\nTRUE\n4\n\n\n2019\n397\n402\nTRUE\n7\n\n\n2023\n463\n424\nTRUE\n8\n\n\n2015\n465\n328\nTRUE\n1\n\n\n2018\n485\n338\nFALSE\n2\n\n\n2002\n491\n348\nFALSE\n3\n\n\n2004\n538\n382\nFALSE\n6\n\n\n2006\n545\n431\nFALSE\n9\n\n\n2014\n857\n366\nFALSE\n5\n\n\n\n\n\n\n\nCode\n# ROC-AUC for El Niño drought detection\ncalc_auc_safe &lt;- function(df) {\n  if(length(unique(df$is_drought)) &lt; 2) return(NA_real_)\n  df &lt;- df |&gt; mutate(drought_factor = factor(is_drought, levels = c(TRUE, FALSE)))\n  tryCatch({\n    yardstick::roc_auc(df, truth = drought_factor, value, event_level = \"first\")$.estimate\n  }, error = function(e) NA_real_)\n}\n\nelnino_auc &lt;- calc_auc_safe(df_elnino)\n\n# Compare mean forecasts\ndrought_fcst &lt;- mean(df_elnino |&gt; filter(is_drought) |&gt; pull(value))\nnon_drought_fcst &lt;- mean(df_elnino |&gt; filter(!is_drought) |&gt; pull(value))\n\n\nEl Niño ROC-AUC = 0.5 (random chance = 0.50)\nSEAS5 has no skill at distinguishing El Niño drought years from non-drought years. Worse, SEAS5 gives slightly higher forecasts to drought years (378 mm) than non-drought years (373 mm) - the wrong direction.\n\n\n7.4.5 Interpretation\nCan we use SEAS5 Postrera forecasts during El Niño for drought anticipation? No.\n\nEl Niño itself is informative: 44% of El Niño Postreras are droughts (vs ~20% baseline). If you know it’s El Niño, you already know drought risk is elevated.\nSEAS5 adds nothing: With ROC-AUC of 0.50, SEAS5 cannot distinguish which El Niño years will be droughts. It’s literally coin-flip accuracy.\nOperational recommendation: During El Niño, use the elevated baseline drought probability (44%) for decision-making. Don’t use the specific SEAS5 forecast value - it provides no additional information.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ENSO Stratification</span>"
    ]
  },
  {
    "objectID": "06_enso_stratification.html#scatter-plots-by-phase",
    "href": "06_enso_stratification.html#scatter-plots-by-phase",
    "title": "7  ENSO Stratification",
    "section": "7.5 Scatter Plots by Phase",
    "text": "7.5 Scatter Plots by Phase\n\n\nScatter plot function\nplot_scatter_postrera &lt;- function(df_joined, subtitle) {\n  df_joined |&gt;\n    filter(window == \"postrera\") |&gt;\n    mutate(enso_phase = factor(enso_phase, levels = c(\"La Nina\", \"Neutral\", \"El Nino\"))) |&gt;\n    ggplot(aes(x = obs_mm, y = value, color = enso_phase)) +\n    geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"grey50\") +\n    geom_point(size = 2.5, alpha = 0.8) +\n    geom_smooth(method = \"lm\", se = FALSE, linewidth = 1) +\n    facet_grid(obs_source ~ enso_phase) +\n    scale_color_manual(\n      values = c(\"La Nina\" = \"#2166AC\", \"Neutral\" = \"#636363\", \"El Nino\" = \"#B2182B\")\n    ) +\n    labs(\n      title = \"SEAS5 Postrera: Forecast vs Observed by ENSO Phase (LT1)\",\n      subtitle = subtitle,\n      x = \"Observed (mm)\", y = \"Forecast (mm)\"\n    ) +\n    theme(legend.position = \"none\")\n}\n\n\n\nAny MonthSeasonal Average\n\n\n\n\nCode\nplot_scatter_postrera(df_joined_any, \"Any month ≥ ±0.5°C. Steeper positive slope = better ranking skill.\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_scatter_postrera(df_joined_avg, \"Seasonal mean ≥ ±0.5°C. Steeper positive slope = better ranking skill.\")\n\n\n\n\n\n\n\n\n\n\n\n\nAcross all observation sources, the El Niño panels show flat or weakly sloped regression lines - the forecast values are essentially uncorrelated with observations. La Niña and Neutral show positive slopes indicating ranking skill.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ENSO Stratification</span>"
    ]
  },
  {
    "objectID": "06_enso_stratification.html#primera-skill-is-robust-across-phases",
    "href": "06_enso_stratification.html#primera-skill-is-robust-across-phases",
    "title": "7  ENSO Stratification",
    "section": "7.6 Primera: Skill is Robust Across Phases",
    "text": "7.6 Primera: Skill is Robust Across Phases\nFor completeness, we verify that Primera skill (LT1) does not show concerning ENSO-phase dependence.\n\n\nPrimera skill functions\ncalc_primera_skill &lt;- function(df_joined) {\n  df_joined |&gt;\n    filter(window == \"primera\") |&gt;\n    group_by(obs_source, enso_phase) |&gt;\n    summarise(\n      n_years = n(),\n      spearman = cor(value, obs_mm, method = \"spearman\"),\n      .groups = \"drop\"\n    ) |&gt;\n    mutate(enso_phase = factor(enso_phase, levels = c(\"La Nina\", \"Neutral\", \"El Nino\")))\n}\n\nplot_primera_skill &lt;- function(primera_skill, subtitle) {\n  primera_skill |&gt;\n    ggplot(aes(x = enso_phase, y = spearman, fill = enso_phase)) +\n    geom_col(alpha = 0.85, width = 0.6) +\n    geom_hline(yintercept = 0, linetype = \"solid\", color = \"grey40\") +\n    geom_hline(yintercept = 0.5, linetype = \"dotted\", color = \"#1A9850\", linewidth = 0.8) +\n    geom_text(aes(label = sprintf(\"%.2f\", spearman)), vjust = -0.5, fontface = \"bold\", size = 3.5) +\n    facet_wrap(~obs_source) +\n    scale_fill_manual(\n      values = c(\"La Nina\" = \"#2166AC\", \"Neutral\" = \"#B0B0B0\", \"El Nino\" = \"#B2182B\")\n    ) +\n    scale_y_continuous(limits = c(-0.1, 1.05)) +\n    labs(\n      title = \"SEAS5 Primera Ranking Skill by ENSO Phase (LT1)\",\n      subtitle = subtitle,\n      x = NULL, y = \"Spearman Correlation\"\n    ) +\n    theme(legend.position = \"none\")\n}\n\ntable_primera_skill &lt;- function(primera_skill) {\n  primera_skill |&gt;\n    mutate(spearman = round(spearman, 2)) |&gt;\n    select(obs_source, enso_phase, spearman) |&gt;\n    pivot_wider(names_from = enso_phase, values_from = spearman) |&gt;\n    select(obs_source, `La Nina`, Neutral, `El Nino`) |&gt;\n    knitr::kable(\n      col.names = c(\"Obs Source\", \"La Niña\", \"Neutral\", \"El Niño\"),\n      caption = \"SEAS5 Primera Spearman correlation by ENSO phase and observation source (LT1)\"\n    )\n}\n\nprimera_skill_any &lt;- calc_primera_skill(df_joined_any)\nprimera_skill_avg &lt;- calc_primera_skill(df_joined_avg)\n\n\n\nAny MonthSeasonal Average\n\n\n\n\nCode\ntable_primera_skill(primera_skill_any)\n\n\n\nSEAS5 Primera Spearman correlation by ENSO phase and observation source (LT1)\n\n\nObs Source\nLa Niña\nNeutral\nEl Niño\n\n\n\n\nCHIRPS\n-0.02\n0.18\n0.39\n\n\nENACTS\n0.38\n0.40\n0.61\n\n\nERA5\n0.17\n0.48\n0.46\n\n\n\n\n\n\n\nCode\nplot_primera_skill(primera_skill_any, \"Any month ≥ ±0.5°C\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntable_primera_skill(primera_skill_avg)\n\n\n\nSEAS5 Primera Spearman correlation by ENSO phase and observation source (LT1)\n\n\nObs Source\nLa Niña\nNeutral\nEl Niño\n\n\n\n\nCHIRPS\n0.4\n0.27\n0.5\n\n\nENACTS\n0.8\n0.42\n0.5\n\n\nERA5\n0.2\n0.52\n0.5\n\n\n\n\n\n\n\nCode\nplot_primera_skill(primera_skill_avg, \"Seasonal mean ≥ ±0.5°C\")\n\n\n\n\n\n\n\n\n\n\n\n\nPrimera shows positive correlations across all phases and observation sources regardless of ENSO classification method. Primera skill is not contingent on ENSO phase.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ENSO Stratification</span>"
    ]
  },
  {
    "objectID": "06_enso_stratification.html#summary",
    "href": "06_enso_stratification.html#summary",
    "title": "7  ENSO Stratification",
    "section": "7.7 Summary",
    "text": "7.7 Summary\n\n7.7.1 Key Finding\nCan we use SEAS5 Postrera forecasts during El Niño for drought anticipation? No.\n\nEl Niño ROC-AUC = 0.50 (random chance) - SEAS5 cannot distinguish which El Niño years will be droughts\nSEAS5 actually gives slightly higher forecasts to El Niño drought years than non-drought years (wrong direction)\nThe 2014 El Niño was the wettest Postrera in 25 years, but SEAS5 predicted drought\n\nHowever, El Niño itself is informative: 44% of El Niño Postreras are droughts (vs ~20% for other phases). Knowing it’s El Niño doubles your baseline drought probability - that’s useful even without SEAS5.\n\n\n7.7.2 Operational Implications\n\nEl Niño years: Use climatology, not SEAS5. If you know it’s El Niño, expect below-normal rainfall on average. But don’t trust the specific SEAS5 forecast value - it could be wildly wrong (as in 2014).\nNeutral years show best skill: SEAS5 Postrera forecasts have the most reliable ranking ability during ENSO-neutral conditions.\nPrimera is unaffected: SEAS5 Primera skill is consistent across all ENSO phases - no need for ENSO-conditional decision rules.\nPrimera is unaffected: SEAS5 Primera skill is consistent across ENSO phases and does not require this stratification.\n\n\n\n7.7.3 Limitations\n\nSample sizes per phase are small (6-10 years), so these findings should be interpreted with appropriate caution\nWe analyzed only SEAS5; INSIVUMEH models may show different patterns\nThe “any month” threshold for ENSO classification is somewhat arbitrary; stricter thresholds would reduce sample sizes further",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ENSO Stratification</span>"
    ]
  },
  {
    "objectID": "07_multi_aoi_comparison.html",
    "href": "07_multi_aoi_comparison.html",
    "title": "8  CADC - Framework Skill Comparison",
    "section": "",
    "text": "8.1 Overview\nThis chapter extends the skill assessment beyond Chiquimula (Guatemala) to examine SEAS5 forecast performance across the Central American Dry Corridor, comparing:\nWe evaluate how skill varies by:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CADC - Framework Skill Comparison</span>"
    ]
  },
  {
    "objectID": "07_multi_aoi_comparison.html#overview",
    "href": "07_multi_aoi_comparison.html#overview",
    "title": "8  CADC - Framework Skill Comparison",
    "section": "",
    "text": "Guatemala (Chiquimula - GT20)\nHonduras (El Paraíso + Francisco Morazán - HN07/HN08, area-weighted)\nEl Salvador (San Vicente - SV11)\n\n\n\nSeason: Primera (May-Aug) vs Postrera (Sep-Nov)\nLeadtime: LT0-2 for Primera, LT0-3 for Postrera\nBaseline period: 2000-2024 (25 years) vs 1981-2024 (44 years)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CADC - Framework Skill Comparison</span>"
    ]
  },
  {
    "objectID": "07_multi_aoi_comparison.html#data-preparation",
    "href": "07_multi_aoi_comparison.html#data-preparation",
    "title": "8  CADC - Framework Skill Comparison",
    "section": "8.2 Data Preparation",
    "text": "8.2 Data Preparation\n\n\nCode\nPRIMERA_MONTHS &lt;- 5:8\nPOSTRERA_MONTHS &lt;- 9:11\nPRIMERA_ISSUED_MONTHS &lt;- c(3, 4, 5)\nPOSTRERA_ISSUED_MONTHS &lt;- c(6, 7, 8, 9)\n\nAOI_PCODES &lt;- c(\"HN07\", \"HN08\", \"SV11\", \"GT20\")\n\ncon &lt;- pg_con()\n\ndf_weights &lt;- tbl(con, \"polygon\") |&gt;\n  mutate(across(pcode, as.character)) |&gt;\n  filter(adm_level == 1, pcode %in% AOI_PCODES) |&gt;\n  select(pcode, iso3, name, seas5_n_upsampled_pixels) |&gt;\n  collect()\n\ndf_seas5_raw &lt;- tbl(con, \"seas5\") |&gt;\n  mutate(across(pcode, as.character)) |&gt;\n  filter(adm_level == 1, pcode %in% AOI_PCODES) |&gt;\n  collect()\n\ndf_era5_raw &lt;- tbl(con, \"era5\") |&gt;\n  mutate(across(pcode, as.character)) |&gt;\n  filter(pcode %in% AOI_PCODES) |&gt;\n  collect()\n\nDBI::dbDisconnect(con)\n\n# Process to seasonal totals\ndf_seas5_mm &lt;- df_seas5_raw |&gt;\n  mutate(value_mm = days_in_month(valid_date) * mean)\n\ndf_seas5_seasonal &lt;- bind_rows(\n  seas5_aggregate_forecast(df_seas5_mm, value = \"value_mm\", valid_months = PRIMERA_MONTHS,\n                           by = c(\"iso3\", \"pcode\", \"issued_date\")) |&gt; mutate(window = \"primera\"),\n  seas5_aggregate_forecast(df_seas5_mm, value = \"value_mm\", valid_months = POSTRERA_MONTHS,\n                           by = c(\"iso3\", \"pcode\", \"issued_date\")) |&gt; mutate(window = \"postrera\")\n) |&gt;\n  rename(fcst_mm = value_mm) |&gt;\n  mutate(\n    year = year(issued_date),\n    issued_month = month(issued_date)\n  ) |&gt;\n  filter(\n    (window == \"primera\" & issued_month %in% PRIMERA_ISSUED_MONTHS) |\n    (window == \"postrera\" & issued_month %in% POSTRERA_ISSUED_MONTHS)\n  )\n\ndf_era5_monthly &lt;- df_era5_raw |&gt;\n  mutate(\n    year = year(valid_date),\n    month = month(valid_date),\n    value_mm = mean * days_in_month(valid_date)\n  )\n\ndf_era5_seasonal &lt;- bind_rows(\n  df_era5_monthly |&gt;\n    filter(month %in% PRIMERA_MONTHS) |&gt;\n    group_by(pcode, iso3, year) |&gt;\n    summarise(obs_mm = sum(value_mm), .groups = \"drop\") |&gt;\n    mutate(window = \"primera\"),\n  df_era5_monthly |&gt;\n    filter(month %in% POSTRERA_MONTHS) |&gt;\n    group_by(pcode, iso3, year) |&gt;\n    summarise(obs_mm = sum(value_mm), .groups = \"drop\") |&gt;\n    mutate(window = \"postrera\")\n)\n\n# Join and aggregate to country level (weighted mean for Honduras)\ndf_joined &lt;- df_seas5_seasonal |&gt;\n  left_join(\n    df_era5_seasonal |&gt; select(pcode, year, window, obs_mm),\n    by = c(\"pcode\", \"year\", \"window\")\n  ) |&gt;\n  filter(!is.na(obs_mm)) |&gt;\n  left_join(df_weights |&gt; select(pcode, seas5_n_upsampled_pixels), by = \"pcode\") |&gt;\n  mutate(\n    country_aoi = case_when(\n      pcode == \"GT20\" ~ \"Guatemala\",\n      pcode %in% c(\"HN07\", \"HN08\") ~ \"Honduras\",\n      pcode == \"SV11\" ~ \"El Salvador\"\n    )\n  ) |&gt;\n  group_by(country_aoi, year, window, leadtime, issued_date) |&gt;\n  summarise(\n    fcst_mm = weighted.mean(fcst_mm, w = seas5_n_upsampled_pixels),\n    obs_mm = weighted.mean(obs_mm, w = seas5_n_upsampled_pixels),\n    .groups = \"drop\"\n  )\n\n\n\n\nCode\n# =============================================================================\n# RP threshold calculation - consistent with other chapters\n# Uses proper ranking with interpolation (not simple quantile)\n# =============================================================================\ncalc_rp_threshold &lt;- function(x, rp_target = 4, direction = -1) {\n  x &lt;- x[!is.na(x)]\n  n &lt;- length(x)\n  if (n &lt; 3) return(NA_real_)\n\n  # Use proper ranking with tie handling\n\n# For direction = -1 (drought/low extreme): lowest values get rank 1 (highest RP)\n  ranks &lt;- rank(x * -direction, ties.method = \"average\")\n  rp &lt;- (n + 1) / ranks\n\n  # Interpolate to find value at target RP\n  approx(rp, x, xout = rp_target, rule = 2)$y\n}\n\n# =============================================================================\n# Skill metrics calculation using yardstick\n# =============================================================================\ncalc_skill_metrics &lt;- function(df, baseline_start, baseline_end) {\n  df_baseline &lt;- df |&gt;\n    filter(year &gt;= baseline_start, year &lt;= baseline_end)\n\n  # Calculate observation thresholds per country/window\n  obs_thresholds &lt;- df_baseline |&gt;\n    group_by(country_aoi, window) |&gt;\n    summarise(obs_thresh = calc_rp_threshold(obs_mm, 4, -1), .groups = \"drop\")\n\n  # Calculate forecast thresholds per country/window/leadtime\n  fcst_thresholds &lt;- df_baseline |&gt;\n    group_by(country_aoi, window, leadtime) |&gt;\n    summarise(fcst_thresh = calc_rp_threshold(fcst_mm, 4, -1), .groups = \"drop\")\n\n  # Classify and create factors with proper levels for yardstick\n  # CRITICAL: drought must be first level (positive class)\n  # Using forcats::fct() for stricter validation (errors if values don't match levels)\n  drought_levels &lt;- c(\"drought\", \"no_drought\")\n\ndf_classified &lt;- df_baseline |&gt;\n    left_join(obs_thresholds, by = c(\"country_aoi\", \"window\")) |&gt;\n    left_join(fcst_thresholds, by = c(\"country_aoi\", \"window\", \"leadtime\")) |&gt;\n    mutate(\n      # Truth: was it actually a drought?\n      truth = fct(\n        if_else(obs_mm &lt;= obs_thresh, \"drought\", \"no_drought\"),\n        levels = drought_levels\n      ),\n      # Estimate: did we predict drought?\n      estimate = fct(\n        if_else(fcst_mm &lt;= fcst_thresh, \"drought\", \"no_drought\"),\n        levels = drought_levels\n      )\n    ) |&gt;\n    filter(!is.na(truth), !is.na(estimate))\n\n  # Calculate metrics using yardstick\n  df_classified |&gt;\n    group_by(country_aoi, window, leadtime) |&gt;\n    summarise(\n      n_years = n_distinct(year),\n      n_drought = sum(truth == \"drought\"),\n\n      # Spearman correlation (continuous)\n      spearman = cor(fcst_mm, obs_mm, method = \"spearman\", use = \"complete.obs\"),\n\n      # ROC-AUC: lower forecast = predict drought, so negate fcst_mm\n      # yardstick returns NA (with warning) if only one class present\n      roc_auc = roc_auc_vec(truth, -fcst_mm, event_level = \"first\"),\n\n      # F1 using yardstick (event_level = \"first\" means drought is positive class)\n      f1 = f_meas_vec(truth, estimate, event_level = \"first\"),\n      precision = precision_vec(truth, estimate, event_level = \"first\"),\n      recall = recall_vec(truth, estimate, event_level = \"first\"),\n\n      .groups = \"drop\"\n    ) |&gt;\n    select(country_aoi, window, leadtime, n_years, n_drought, spearman, roc_auc, f1)\n}\n\ndf_skill_2000 &lt;- calc_skill_metrics(df_joined, 2000, 2024) |&gt; mutate(baseline = \"2000-2024\")\ndf_skill_1981 &lt;- calc_skill_metrics(df_joined, 1981, 2024) |&gt; mutate(baseline = \"1981-2024\")\n\ndf_skill_all &lt;- bind_rows(df_skill_2000, df_skill_1981) |&gt;\n  mutate(\n    baseline = factor(baseline, levels = c(\"2000-2024\", \"1981-2024\")),\n    window = factor(window, levels = c(\"primera\", \"postrera\")),\n    country_aoi = factor(country_aoi, levels = c(\"Guatemala\", \"Honduras\", \"El Salvador\"))\n  )",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CADC - Framework Skill Comparison</span>"
    ]
  },
  {
    "objectID": "07_multi_aoi_comparison.html#primera-consistently-skillful",
    "href": "07_multi_aoi_comparison.html#primera-consistently-skillful",
    "title": "8  CADC - Framework Skill Comparison",
    "section": "8.3 Primera: Consistently Skillful",
    "text": "8.3 Primera: Consistently Skillful\nPrimera forecasts show acceptable skill across all three countries, with ROC-AUC values consistently above 0.65 and reaching 0.85+ for Guatemala at shorter leadtimes.\n\n2000-2024 Baseline1981-2024 Baseline\n\n\n\n\nCode\ndf_skill_all |&gt;\n  filter(window == \"primera\", leadtime %in% c(0, 1, 2), baseline == \"2000-2024\") |&gt;\n  ggplot(aes(x = factor(leadtime), y = country_aoi, fill = roc_auc)) +\n  geom_tile(color = \"white\", linewidth = 0.8) +\n  geom_text(aes(label = sprintf(\"%.2f\", roc_auc)), size = 5, fontface = \"bold\") +\n  scale_fill_gradient2(\n    low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n    midpoint = 0.65, limits = c(0.5, 0.95), name = \"ROC-AUC\",\n    oob = scales::squish\n  ) +\n  labs(\n    title = \"Primera (May-Aug): SEAS5 Drought Detection Skill\",\n    subtitle = \"Baseline: 2000-2024 (n=25). ROC-AUC for RP4 drought threshold.\",\n    x = \"Leadtime (months)\", y = NULL\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_skill_all |&gt;\n  filter(window == \"primera\", leadtime %in% c(0, 1, 2), baseline == \"1981-2024\") |&gt;\n  ggplot(aes(x = factor(leadtime), y = country_aoi, fill = roc_auc)) +\n  geom_tile(color = \"white\", linewidth = 0.8) +\n  geom_text(aes(label = sprintf(\"%.2f\", roc_auc)), size = 5, fontface = \"bold\") +\n  scale_fill_gradient2(\n    low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n    midpoint = 0.65, limits = c(0.5, 0.95), name = \"ROC-AUC\",\n    oob = scales::squish\n  ) +\n  labs(\n    title = \"Primera (May-Aug): SEAS5 Drought Detection Skill\",\n    subtitle = \"Baseline: 1981-2024 (n=44). ROC-AUC for RP4 drought threshold.\",\n    x = \"Leadtime (months)\", y = NULL\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nKey finding: Primera skill is robust across AOIs and baseline periods. The choice of baseline has minimal impact on Primera performance.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CADC - Framework Skill Comparison</span>"
    ]
  },
  {
    "objectID": "07_multi_aoi_comparison.html#postrera-baseline-period-matters",
    "href": "07_multi_aoi_comparison.html#postrera-baseline-period-matters",
    "title": "8  CADC - Framework Skill Comparison",
    "section": "8.4 Postrera: Baseline Period Matters",
    "text": "8.4 Postrera: Baseline Period Matters\nPostrera presents a more complex picture. The 2000-2024 baseline suggests limited skill, but extending to 1981-2024 reveals substantially better performance.\n\n2000-2024 Baseline1981-2024 BaselineComparison\n\n\n\n\nCode\ndf_skill_all |&gt;\n  filter(window == \"postrera\", leadtime %in% c(0, 1, 2, 3), baseline == \"2000-2024\") |&gt;\n  ggplot(aes(x = factor(leadtime), y = country_aoi, fill = roc_auc)) +\n  geom_tile(color = \"white\", linewidth = 0.8) +\n  geom_text(aes(label = sprintf(\"%.2f\", roc_auc)), size = 5, fontface = \"bold\") +\n  scale_fill_gradient2(\n    low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n    midpoint = 0.65, limits = c(0.4, 0.9), name = \"ROC-AUC\",\n    oob = scales::squish\n  ) +\n  labs(\n    title = \"Postrera (Sep-Nov): SEAS5 Drought Detection Skill\",\n    subtitle = \"Baseline: 2000-2024 (n=25). Skill appears limited at longer leadtimes.\",\n    x = \"Leadtime (months)\", y = NULL\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_skill_all |&gt;\n  filter(window == \"postrera\", leadtime %in% c(0, 1, 2, 3), baseline == \"1981-2024\") |&gt;\n  ggplot(aes(x = factor(leadtime), y = country_aoi, fill = roc_auc)) +\n  geom_tile(color = \"white\", linewidth = 0.8) +\n  geom_text(aes(label = sprintf(\"%.2f\", roc_auc)), size = 5, fontface = \"bold\") +\n  scale_fill_gradient2(\n    low = \"#D73027\", mid = \"#FFFFBF\", high = \"#1A9850\",\n    midpoint = 0.65, limits = c(0.4, 0.9), name = \"ROC-AUC\",\n    oob = scales::squish\n  ) +\n  labs(\n    title = \"Postrera (Sep-Nov): SEAS5 Drought Detection Skill\",\n    subtitle = \"Baseline: 1981-2024 (n=44). Longer baseline reveals stronger skill.\",\n    x = \"Leadtime (months)\", y = NULL\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_skill_all |&gt;\n  filter(window == \"postrera\", leadtime %in% c(0, 1, 2, 3)) |&gt;\n  ggplot(aes(x = factor(leadtime), y = roc_auc, fill = baseline)) +\n  geom_col(position = position_dodge(width = 0.8), width = 0.7) +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"grey40\") +\n  geom_hline(yintercept = 0.7, linetype = \"dotted\", color = \"grey60\") +\n  facet_wrap(~country_aoi) +\n  scale_fill_manual(values = c(\"2000-2024\" = \"#fc8d62\", \"1981-2024\" = \"#66c2a5\")) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +\n  labs(\n    title = \"Postrera: Baseline Comparison\",\n    subtitle = \"Dashed = random chance (0.5). Dotted = 0.7 threshold. Longer baseline consistently better.\",\n    x = \"Leadtime (months)\", y = \"ROC-AUC\", fill = \"Baseline\"\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    strip.text = element_text(size = 12, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.4.1 Skill Improvement Table\n\n\nCode\ndf_skill_all |&gt;\n  filter(window == \"postrera\", leadtime %in% c(0, 1, 2, 3)) |&gt;\n  select(country_aoi, baseline, leadtime, roc_auc) |&gt;\n  pivot_wider(names_from = baseline, values_from = roc_auc) |&gt;\n  mutate(\n    improvement = `1981-2024` - `2000-2024`,\n    pct_improvement = scales::percent(improvement / `2000-2024`, accuracy = 1)\n  ) |&gt;\n  arrange(country_aoi, leadtime) |&gt;\n  gt() |&gt;\n  fmt_number(columns = c(`2000-2024`, `1981-2024`, improvement), decimals = 2) |&gt;\n  cols_label(\n    country_aoi = \"Country\",\n    leadtime = \"LT\",\n    `2000-2024` = \"2000-2024\",\n    `1981-2024` = \"1981-2024\",\n    improvement = \"Diff\",\n    pct_improvement = \"% Gain\"\n  ) |&gt;\n  tab_header(\n    title = \"Postrera ROC-AUC: Baseline Comparison\",\n    subtitle = \"Skill improvement from extending baseline to 1981\"\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#d4edda\"),\n    locations = cells_body(\n      columns = improvement,\n      rows = improvement &gt; 0.1\n    )\n  )\n\n\n\n\n\n\n\n\nPostrera ROC-AUC: Baseline Comparison\n\n\nSkill improvement from extending baseline to 1981\n\n\nCountry\nLT\n2000-2024\n1981-2024\nDiff\n% Gain\n\n\n\n\nGuatemala\n0\n0.66\n0.80\n0.14\n22%\n\n\nGuatemala\n1\n0.66\n0.72\n0.07\n10%\n\n\nGuatemala\n2\n0.50\n0.66\n0.16\n31%\n\n\nGuatemala\n3\n0.57\n0.65\n0.08\n15%\n\n\nHonduras\n0\n0.85\n0.87\n0.01\n2%\n\n\nHonduras\n1\n0.82\n0.79\n−0.03\n-4%\n\n\nHonduras\n2\n0.66\n0.77\n0.11\n17%\n\n\nHonduras\n3\n0.62\n0.74\n0.12\n19%\n\n\nEl Salvador\n0\n0.71\n0.86\n0.15\n21%\n\n\nEl Salvador\n1\n0.60\n0.76\n0.17\n28%\n\n\nEl Salvador\n2\n0.46\n0.63\n0.16\n35%\n\n\nEl Salvador\n3\n0.43\n0.63\n0.20\n47%",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CADC - Framework Skill Comparison</span>"
    ]
  },
  {
    "objectID": "07_multi_aoi_comparison.html#leadtime-vs-operational-value-trade-off",
    "href": "07_multi_aoi_comparison.html#leadtime-vs-operational-value-trade-off",
    "title": "8  CADC - Framework Skill Comparison",
    "section": "8.5 Leadtime vs Operational Value Trade-off",
    "text": "8.5 Leadtime vs Operational Value Trade-off\nThe key challenge for Postrera is balancing forecast skill against operational lead time:\n\n\n\nLeadtime\nIssued\nOperational Lead Time\nSkill (1981-2024)\n\n\n\n\nLT0\nSeptember\n~0 months\nExcellent (0.80-0.87)\n\n\nLT1\nAugust\n~1 month\nGood (0.72-0.79)\n\n\nLT2\nJuly\n~2 months\nModerate (0.63-0.77)\n\n\nLT3\nJune\n~3 months\nWeaker (0.63-0.74)\n\n\n\nRecommendation: For Postrera, use LT0 and LT1:\n\nLT1 (August-issued): Provides ~1 month lead time for preparedness actions with good skill (ROC-AUC 0.72-0.79)\nLT0 (September-issued): Excellent skill (ROC-AUC 0.80-0.87) for confirming/updating the LT1 forecast as the season begins\nSkill degrades substantially at LT2-3, making these less reliable for operational decisions",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CADC - Framework Skill Comparison</span>"
    ]
  },
  {
    "objectID": "07_multi_aoi_comparison.html#regional-differences",
    "href": "07_multi_aoi_comparison.html#regional-differences",
    "title": "8  CADC - Framework Skill Comparison",
    "section": "8.6 Regional Differences",
    "text": "8.6 Regional Differences\nHonduras consistently shows the strongest Postrera skill, while Guatemala shows the weakest:\n\n\nCode\ndf_skill_all |&gt;\n  filter(baseline == \"1981-2024\") |&gt;\n  mutate(\n    window_label = factor(window,\n      levels = c(\"primera\", \"postrera\"),\n      labels = c(\"Primera (May-Aug)\", \"Postrera (Sep-Nov)\")\n    )\n  ) |&gt;\n  ggplot(aes(x = factor(leadtime), y = roc_auc, color = country_aoi, group = country_aoi)) +\n  geom_line(linewidth = 1.2) +\n  geom_point(size = 3) +\n  geom_hline(yintercept = 0.7, linetype = \"dashed\", color = \"grey50\") +\n  facet_wrap(~window_label, scales = \"free_x\") +\n  scale_color_manual(values = c(\"Guatemala\" = \"#1b9e77\", \"Honduras\" = \"#d95f02\", \"El Salvador\" = \"#7570b3\")) +\n  scale_y_continuous(limits = c(0.5, 0.95)) +\n  labs(\n    title = \"SEAS5 Skill Across Regions (1981-2024 Baseline)\",\n    subtitle = \"Honduras shows strongest Postrera skill. Dashed line = 0.7 threshold.\",\n    x = \"Leadtime (months)\", y = \"ROC-AUC\", color = \"Country\"\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    strip.text = element_text(size = 12, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CADC - Framework Skill Comparison</span>"
    ]
  },
  {
    "objectID": "07_multi_aoi_comparison.html#recommendations",
    "href": "07_multi_aoi_comparison.html#recommendations",
    "title": "8  CADC - Framework Skill Comparison",
    "section": "8.7 Recommendations",
    "text": "8.7 Recommendations\nBased on this multi-AOI assessment:\n\nUse the 1981-2024 baseline for threshold calculation. The longer record provides more stable estimates and reveals skill that is masked by the shorter baseline.\nPrimera forecasts are reliable across all three countries at LT0-2. Any of these leadtimes can be used operationally.\nPostrera: Use LT0 and LT1:\n\nLT1 (August-issued): Primary forecast for preparedness planning (~1 month lead time, ROC-AUC 0.72-0.79)\nLT0 (September-issued): Confirmation/update as season begins (ROC-AUC 0.80-0.87)\nLT2-3 show weaker skill and are less reliable for operational decisions\n\nRegional considerations:\n\nHonduras Postrera forecasts are most reliable across all leadtimes\nGuatemala Postrera should be interpreted with more caution\nEl Salvador shows strong improvement with longer baseline\n\nFor Guatemala specifically: Given the weaker Postrera skill, consider:\n\nEmphasizing Primera forecasts for anticipatory action\nUsing Postrera forecasts as one input among several (ENSO state, VHI, etc.)\nRelying primarily on LT0-1 for Postrera decisions",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>CADC - Framework Skill Comparison</span>"
    ]
  },
  {
    "objectID": "appendix_technical_notes.html",
    "href": "appendix_technical_notes.html",
    "title": "Appendix A — Technical Notes",
    "section": "",
    "text": "A.1 Drought Definition\nDrought is defined using a return period of 4 years (RP4):",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Technical Notes</span>"
    ]
  },
  {
    "objectID": "appendix_technical_notes.html#drought-definition",
    "href": "appendix_technical_notes.html#drought-definition",
    "title": "Appendix A — Technical Notes",
    "section": "",
    "text": "Approximately 6 drought years expected over the 25-year baseline (2000-2024)\nThreshold calculated separately for each forecast model and observation source\nThis aligns with typical anticipatory action trigger frameworks",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Technical Notes</span>"
    ]
  },
  {
    "objectID": "appendix_technical_notes.html#observation-sources",
    "href": "appendix_technical_notes.html#observation-sources",
    "title": "Appendix A — Technical Notes",
    "section": "A.2 Observation Sources",
    "text": "A.2 Observation Sources\n\n\n\n\n\n\n\n\n\nSource\nType\nCoverage\nUsed For\n\n\n\n\nERA5\nGlobal reanalysis\nGlobal\nSEAS5 native validation\n\n\nENACTS\nSatellite + stations\nRegional\nOperational standard for Guatemala\n\n\nCHIRPS\nSatellite + stations\nQuasi-global\nIndependent validation",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Technical Notes</span>"
    ]
  },
  {
    "objectID": "appendix_technical_notes.html#key-metrics-glossary",
    "href": "appendix_technical_notes.html#key-metrics-glossary",
    "title": "Appendix A — Technical Notes",
    "section": "A.3 Key Metrics Glossary",
    "text": "A.3 Key Metrics Glossary\n\nF1 Score: Harmonic mean of precision and recall. Balances false alarms and missed events.\nROC-AUC: Area under the receiver operating characteristic curve. Measures discrimination skill - can the forecast distinguish drought from non-drought years?\nSpearman correlation: Rank-based correlation. Do wetter forecasts correspond to wetter observations?\nInter-leadtime correlation: Do consecutive leadtimes agree with each other? A data quality check.\nCoefficient of Variation: How much do forecasts vary relative to their mean? Forecasts that barely vary contain no useful signal.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Technical Notes</span>"
    ]
  },
  {
    "objectID": "appendix_technical_notes.html#archived-exploring-larger-spatial-scales",
    "href": "appendix_technical_notes.html#archived-exploring-larger-spatial-scales",
    "title": "Appendix A — Technical Notes",
    "section": "A.4 Archived: Exploring Larger Spatial Scales",
    "text": "A.4 Archived: Exploring Larger Spatial Scales\n\n\n\n\n\n\nNoteHistorical Context\n\n\n\nThis analysis was initiated early in the research process, following the binary classification metrics assessment. The hypothesis was that seasonal forecasts might perform better when aggregated over larger spatial scales than administrative boundaries. However, as the investigation progressed through continuous metrics, multi-source validation, and internal consistency diagnostics, the root cause of poor Postrera skill became clear: the INSIVUMEH forecasts themselves lack signal, regardless of spatial aggregation. This section is retained for completeness but was not pursued further.\n\n\nBoth SEAS5 and INSIVUMEH may perform poorly at the admin-1 (Chiquimula) scale because seasonal forecasts typically have better skill at larger spatial scales. HydroBASINS provides standardized watershed boundaries at multiple levels that could serve as alternative analysis units.\n\n\nLoad HydroBASINS and Chiquimula boundary\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(cumulus)\n\n# Disable s2 for this section (HydroBASINS has some geometry issues with s2)\nsf_use_s2(FALSE)\n\n# Load Guatemala boundaries\nsf_gtm_adm0 &lt;- cumulus::download_fieldmaps_sf(iso3 = \"GTM\", layer = \"gtm_adm0\")$gtm_adm0\nsf_gtm_adm1 &lt;- cumulus::download_fieldmaps_sf(iso3 = \"GTM\", layer = \"gtm_adm1\")$gtm_adm1 |&gt;\n janitor::clean_names()\nsf_chiquimula &lt;- sf_gtm_adm1 |&gt; filter(adm1_pcode == \"GT20\")\n\n# Path to HydroBASINS data\nhybas_dir &lt;- \"../../data/hybas_na_lev01-12_v1c\"\n\n# Load levels 5 and 6\nsf_hybas_5 &lt;- st_read(file.path(hybas_dir, \"hybas_na_lev05_v1c.shp\"), quiet = TRUE)\nsf_hybas_6 &lt;- st_read(file.path(hybas_dir, \"hybas_na_lev06_v1c.shp\"), quiet = TRUE)\n\n# Ensure same CRS\nsf_chiquimula &lt;- st_transform(sf_chiquimula, st_crs(sf_hybas_5))\n\n# Get bounding box of Chiquimula with buffer for context\nchiq_bbox &lt;- st_bbox(sf_chiquimula)\nbbox_buffer &lt;- 1  # degrees\n\n# Create bbox polygon for filtering\nbbox_poly &lt;- st_as_sfc(st_bbox(c(\n  xmin = as.numeric(chiq_bbox[\"xmin\"]) - bbox_buffer,\n  xmax = as.numeric(chiq_bbox[\"xmax\"]) + bbox_buffer,\n  ymin = as.numeric(chiq_bbox[\"ymin\"]) - bbox_buffer,\n  ymax = as.numeric(chiq_bbox[\"ymax\"]) + bbox_buffer\n), crs = st_crs(sf_hybas_5)))\n\n# Filter basins to region around Chiquimula using intersection\nsf_hybas_5_region &lt;- sf_hybas_5[st_intersects(sf_hybas_5, bbox_poly, sparse = FALSE)[,1], ]\nsf_hybas_6_region &lt;- sf_hybas_6[st_intersects(sf_hybas_6, bbox_poly, sparse = FALSE)[,1], ]\n\n# Find basins that intersect Chiquimula\nsf_hybas_5_intersect &lt;- sf_hybas_5_region[st_intersects(sf_hybas_5_region, sf_chiquimula, sparse = FALSE)[,1], ]\nsf_hybas_6_intersect &lt;- sf_hybas_6_region[st_intersects(sf_hybas_6_region, sf_chiquimula, sparse = FALSE)[,1], ]\n\ncat(\"Level 5 basins intersecting Chiquimula:\", nrow(sf_hybas_5_intersect), \"\\n\")\n\n\nLevel 5 basins intersecting Chiquimula: 2 \n\n\nLoad HydroBASINS and Chiquimula boundary\ncat(\"Level 6 basins intersecting Chiquimula:\", nrow(sf_hybas_6_intersect), \"\\n\")\n\n\nLevel 6 basins intersecting Chiquimula: 2 \n\n\n\n\nMap comparing basin levels 5 and 6 over Chiquimula\nlibrary(patchwork)\n\n# Calculate areas for labels\nsf_hybas_5_intersect &lt;- sf_hybas_5_intersect |&gt;\n  mutate(area_km2 = as.numeric(st_area(geometry)) / 1e6)\n\nsf_hybas_6_intersect &lt;- sf_hybas_6_intersect |&gt;\n  mutate(area_km2 = as.numeric(st_area(geometry)) / 1e6)\n\nchiq_area &lt;- as.numeric(st_area(sf_chiquimula)) / 1e6\n\n# Transform admin0 to match CRS\nsf_gtm_adm0_t &lt;- st_transform(sf_gtm_adm0, st_crs(sf_hybas_5))\n\n# Create base plot function\ncreate_basin_map &lt;- function(sf_basins, level, sf_admin, sf_country) {\n\n  # Get centroids for labels\n  basin_centroids &lt;- st_centroid(sf_basins) |&gt;\n    mutate(\n      x = st_coordinates(geometry)[,1],\n      y = st_coordinates(geometry)[,2]\n    )\n\n  ggplot() +\n    geom_sf(data = sf_country, fill = \"grey95\", color = \"grey50\", linewidth = 0.3) +\n    geom_sf(data = sf_basins, aes(fill = area_km2), color = \"darkblue\", linewidth = 0.8) +\n    geom_sf(data = sf_admin, fill = NA, color = \"red\", linewidth = 1.2) +\n    geom_sf_text(\n      data = basin_centroids,\n      aes(label = paste0(round(area_km2), \" km²\")),\n      size = 3, fontface = \"bold\", color = \"white\"\n    ) +\n    scale_fill_viridis_c(option = \"mako\", direction = -1, name = \"Area (km²)\") +\n    labs(\n      title = paste0(\"HydroBASINS Level \", level),\n      subtitle = paste0(nrow(sf_basins), \" basin(s) intersecting Chiquimula\"),\n      caption = paste0(\"Red outline = Chiquimula (\", round(chiq_area), \" km²)\")\n    ) +\n    theme_minimal() +\n    theme(\n      legend.position = \"bottom\",\n      plot.caption = element_text(color = \"red\", face = \"bold\")\n    )\n}\n\np_lev5 &lt;- create_basin_map(sf_hybas_5_intersect, 5, sf_chiquimula, sf_gtm_adm0_t)\np_lev6 &lt;- create_basin_map(sf_hybas_6_intersect, 6, sf_chiquimula, sf_gtm_adm0_t)\n\np_lev5 + p_lev6 +\n  plot_annotation(\n    title = \"HydroBASINS Comparison: Potential Analysis Units\",\n    subtitle = \"Comparing basin scales that could provide better forecast skill than admin-1 boundaries\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Summary comparison\ntibble(\n  `Spatial Unit` = c(\n    \"Chiquimula (Admin-1)\",\n    paste0(\"Level 5 (\", nrow(sf_hybas_5_intersect), \" basin\", ifelse(nrow(sf_hybas_5_intersect) &gt; 1, \"s\", \"\"), \")\"),\n    paste0(\"Level 6 (\", nrow(sf_hybas_6_intersect), \" basin\", ifelse(nrow(sf_hybas_6_intersect) &gt; 1, \"s\", \"\"), \")\")\n  ),\n  `Total Area (km²)` = c(\n    round(chiq_area),\n    round(sum(sf_hybas_5_intersect$area_km2)),\n    round(sum(sf_hybas_6_intersect$area_km2))\n  ),\n  `Mean Basin Area (km²)` = c(\n    round(chiq_area),\n    round(mean(sf_hybas_5_intersect$area_km2)),\n    round(mean(sf_hybas_6_intersect$area_km2))\n  ),\n  Notes = c(\n    \"Current analysis unit\",\n    \"Larger basins - may have better forecast skill\",\n    \"Medium basins - balance of skill and resolution\"\n  )\n) |&gt;\n  knitr::kable(caption = \"Comparison of spatial units for forecast skill assessment\")\n\n\n\nComparison of spatial units for forecast skill assessment\n\n\n\n\n\n\n\n\nSpatial Unit\nTotal Area (km²)\nMean Basin Area (km²)\nNotes\n\n\n\n\nChiquimula (Admin-1)\n2415\n2415\nCurrent analysis unit\n\n\nLevel 5 (2 basins)\n225446\n112723\nLarger basins - may have better forecast skill\n\n\nLevel 6 (2 basins)\n34487\n17243\nMedium basins - balance of skill and resolution\n\n\n\n\n\n\nA.4.1 Original Interpretation\n\nLevel 5 provides larger drainage basins that may better match the spatial resolution at which seasonal forecasts have skill\nLevel 6 offers a middle ground between admin boundaries and larger regional basins\n\nThis avenue was not pursued further after the internal consistency analysis (Chapter 5) revealed that the INSIVUMEH Postrera forecasts lack meaningful signal at any spatial scale.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Technical Notes</span>"
    ]
  }
]