# Intro {#sec-journey}

## The Decision/Question

Which seasonal forecast should Guatemala's anticipatory action framework use for drought prediction in Chiquimula? Two options are available:

- **SEAS5**: ECMWF's global seasonal forecasting system
- **INSIVUMEH**: Regional forecasts from Guatemala's national meteorological service (CFSv2, CCSM4, and CESM1 models)

This book documents the reserch to answer this question for two agricultural seasons:

| Season | Months | Description |
|--------|--------|-------------|
| **Primera** | May-August (MJJA) | First maize planting |
| **Postrera** | September-November (SON) | Second maize planting |

## Research Progression

### Step 1: Start with Standard Metrics

The analysis began with **binary classification metrics** (Chapter 1) - the traditional approach where forecasts are scored as hits or misses against a drought threshold (return period of 4 years).

**Primera results were encouraging**: SEAS5 showed F1 scores around 50-60%, significantly better than random guessing. Several model-leadtime combinations beat the 90th percentile of bootstrap random simulations.

**Postrera was troubling**: No model showed statistically significant skill. F1 scores hovered near random guessing levels. Different models won at different leadtimes with no clear pattern.

*Remaining question*: Are the Postrera results genuinely inconclusive, or are binary metrics too noisy with only ~6 drought events in 25 years?

### Step 2: Try Continuous Metrics

Chapter 2 introduced **continuous metrics** that don't require threshold choices: Spearman correlation, RMSE, and ROC-AUC. These provide partial credit for near-misses and are more stable with small samples.

**Primera confirmation**: SEAS5 dominated across all continuous metrics. AUC values around 0.88 - meaning 88% probability of correctly ranking a drought year below a non-drought year. The signal was real.

**Postrera remained ambiguous**: Different metrics favored different models:

- Spearman: SEAS5 best at LT1
- RMSE: CCSM4 best
- AUC: SEAS5 at LT1, CCSM4 at LT2

No model showed AUC consistently above 0.7 (general threshold for "acceptable" discrimination skill).

*Remaining question*: Are these results sensitive to which observation dataset we validate against? Would a different "ground truth" give the same answer?

### Step 3: Bring in a Third Opinion

Chapter 3 introduced **CHIRPS** as an independent observation source alongside ERA5 and ENACTS. If forecasts perform well against multiple observation sources, the skill assessment is more credible.

**Primera verdict**: Skill was robust across all three observation sources. SEAS5 won most comparisons.

**Postrera raised a red flag**: CCSM4 showed *better* skill at longer leadtimes (LT2-3) than shorter ones (LT1). This is backwards - forecast skill should degrade with increasing leadtime, not improve. The pattern suggested we might be seeing noise rather than genuine skill.

*Remaining question*: What's causing the inverted skill-leadtime relationship for Postrera?

### Step 4: Check for Drift

Chapter 4 investigated whether **temporal trends** in forecasts or observations could explain the strange patterns.

**Findings**:

- ENACTS showed a drying trend for Primera, but other sources showed similar patterns
- Forecast error drift was modest (~5-8 mm/year)
- **Split-half stability** was the key finding: Primera correlations were stable between 2000-2012 and 2013-2024, but Postrera correlations were wildly different between periods

The drift analysis didn't explain the inverted skill pattern.

### Step 5: The Decider - Forecast Internal Consistency

Chapter 5 applied **internal consistency diagnostics** - asking not "do forecasts predict observations?" but "do forecasts behave like forecasts?"

**Inter-leadtime correlations**: For a well-behaved forecast, LT1 and LT2 predictions for the same target season should correlate - they're trying to predict the same thing.

- SEAS5: LT1-LT2 correlations of 0.6-0.9 (expected)
- INSIVUMEH Primera: Moderate correlations of 0.3-0.7 (acceptable)
- INSIVUMEH Postrera: **Near-zero or negative correlations** - when LT1 predicts wet, LT2 might predict dry for the same season

**Coefficient of Variation (CV)**: Forecasts should show interannual variability approaching observed variability.

- Observed Postrera CV: ~23%
- SEAS5 Postrera CV: ~15% (reasonable)
- INSIVUMEH Postrera CV: **3-7%** - forecasts barely varied from climatology

This **eliminated INSIVUMEH** from consideration for Postrera - their forecasts lack predictive signal. But SEAS5 Postrera skill also appeared limited. This raised a concern: Honduras and El Salvador rely solely on SEAS5 without a national forecast alternative.

### Step 6: ENSO Stratification

Chapter 6 examined whether forecast skill varies by ENSO phase. The key finding: Primera skill is consistent across ENSO phases, but **Postrera El Niño years remain challenging** for all models.

### Step 7: The Breakthrough - Regional Comparison & Baseline Extension

The poor Postrera skill had implications beyond Guatemala. Honduras and El Salvador use SEAS5 for their anticipatory action frameworks—if SEAS5 Postrera lacks skill, these frameworks are affected too.

Chapter 7 expanded the analysis across the Dry Corridor (Guatemala, Honduras, El Salvador) and tested different baseline periods. This revealed a **breakthrough finding**:

**The 2000-2024 baseline was masking Postrera skill.**

Extending to 1981-2024 (44 years) dramatically improved apparent Postrera skill:

| Leadtime | ROC-AUC (2000-2024) | ROC-AUC (1981-2024) |
|:--------:|:-------------------:|:-------------------:|
| LT0 | 0.66-0.81 | **0.80-0.87** |
| LT1 | 0.55-0.79 | **0.72-0.79** |
| LT2 | 0.48-0.62 | 0.63-0.77 |

The longer baseline provides more stable threshold estimates and reveals skill that was obscured by the shorter record.

## Final Recommendations

| Season | Recommended | Leadtimes | Confidence | Rationale |
|--------|-------------|-----------|------------|-----------|
| Primera | **SEAS5** | LT0-2 | High | Clear skill advantage across all metrics and observation sources |
| Postrera | **SEAS5** | **LT0-1** | **Moderate** | With 1981-2024 baseline, ROC-AUC 0.72-0.87 at LT0-1 |

### INSIVUMEH: Eliminated

INSIVUMEH forecasts are **not recommended** for either season due to weak internal consistency. For Postrera specifically, their forecasts exhibit:

- Near-zero inter-leadtime correlations (r = -0.1 to 0.2)
- Coefficient of variation 3-7× lower than observations
- No meaningful predictive signal

### The Postrera Solution

The initial Postrera analysis suggested limited skill, but extending the baseline period revealed **usable skill at shorter leadtimes**:

- **LT1 (August-issued)**: Use for initial planning with ~1 month lead time (ROC-AUC 0.72-0.79)
- **LT0 (September-issued)**: Use to confirm/update the forecast as season begins (ROC-AUC 0.80-0.87)
- **LT2-3**: Skill degrades substantially; less reliable for operational decisions

**Key operational requirements**:

1. **Use the 1981-2024 baseline** for drought threshold calculation (not 2000-2024)
2. **Focus on LT0 and LT1** for Postrera decisions

## Book Structure

The chapters document each step of this journey:

| Step | Question | Finding |
|------|----------|---------|
| 1. Binary Metrics | Do forecasts beat random guessing? | Primera yes (SEAS5 wins), Postrera inconclusive |
| 2. Continuous Metrics | Does skill hold with different metrics? | Primera confirmed (SEAS5 wins), Postrera ambiguous |
| 3. CHIRPS Tie-Breaker | Is skill robust across observation sources? | Primera robust, Postrera still ambiguous |
| 4. Temporal Drift | Are trends/drift affecting skill estimates? | Mild drift, unlikely to explain poor Postrera skill |
| 5. Internal Consistency | Are forecasts internally coherent? | INSIVUMEH Postrera forecasts lack signal |
| 6. ENSO Stratification | Does skill vary by ENSO phase? | Primera consistent; Postrera El Niño challenging |
| **7. Regional Comparison** | **Does baseline period matter?** | **Longer baseline reveals strong Postrera skill at LT0-1** |

