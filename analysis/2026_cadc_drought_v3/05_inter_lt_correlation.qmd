# Leadtime Diagnostics {#sec-leadtime}

## Motivation

In @sec-chirps, CCSM4 showed a suspicious pattern for Postrera: skill at LT2-3 exceeded skill at LT1. This is backwards—forecast skill should degrade with increasing leadtime, not improve.

This chapter investigates whether this pattern reflects genuine predictability or a data quality issue by examining:

1. **Inter-leadtime correlations**: Do consecutive leadtimes agree with each other?
2. **Coefficient of Variation (CV)**: Do forecasts show realistic interannual variability?

```{r}
#| label: setup
#| code-summary: "Setup"

library(tidyverse)
library(lubridate)
library(cumulus)
library(gghdx)
gghdx()

box::use(../../R/enacts)
box::use(../../R/seas5)

BASELINE_START <- 2000
BASELINE_END <- 2024
```

```{r}
#| label: load-data
#| code-summary: "Load forecast and observation data"

# Forecasts
df_insiv <- cumulus::blob_read(
  name = "ds-aa-lac-dry-corridor/data/processed/insivumeh_special/insivumeh_special_models_zonal_seasonal_chiquimula.parquet",
  container = "projects"
)
df_seas5 <- seas5$load_seas5_seasonal()

# Observations
df_enacts <- enacts$load_enacts_seasonal("chiquimula")

# Combine forecasts
df_fcst_all <- bind_rows(df_insiv, df_seas5) |>
  filter(year >= BASELINE_START, year <= BASELINE_END) |>
  mutate(model = str_remove(forecast_source, "INSIVUMEH_"))
```

## Inter-Leadtime Correlations

For a well-behaved forecast system, predictions at different leadtimes for the same target season should be correlated—they're all trying to predict the same thing. If LT1 and LT2 forecasts are uncorrelated, one or both must be essentially random.

```{r}
#| label: lt-correlation-function
#| code-summary: "Function to compute correlation matrix"

compute_lt_corr_matrix <- function(df, model_name, window_name) {
  df_wide <- df |>
    filter(model == model_name, window == window_name, !is.na(leadtime)) |>
    select(year, leadtime, value) |>
    pivot_wider(names_from = leadtime, values_from = value, names_prefix = "LT")

  # Add observations
  df_obs <- df_enacts |>
    filter(window == window_name, year >= BASELINE_START, year <= BASELINE_END) |>
    select(year, obs_mm)

  df_wide <- df_wide |>
    left_join(df_obs, by = "year") |>
    rename(OBS = obs_mm)

  # Compute correlation matrix - only keep LT and OBS columns
  cor_mat <- df_wide |>
    select(starts_with("LT"), OBS) |>
    cor(use = "pairwise.complete.obs")

  cor_mat
}
```

```{r}
#| label: seas5-lt-corr
#| code-summary: "SEAS5 inter-leadtime correlations"

models <- unique(df_fcst_all$model)
windows <- c("primera", "postrera")

# Compute all correlation matrices
corr_results <- expand_grid(model = models, window = windows) |>
  mutate(
    corr_mat = map2(model, window, ~compute_lt_corr_matrix(df_fcst_all, .x, .y))
  )
```

### SEAS5 Correlation Patterns

```{r}
#| label: seas5-corr-plot
#| fig-height: 4

# Extract SEAS5 matrices
seas5_primera <- corr_results |>
  filter(model == "SEAS5", window == "primera") |>
  pull(corr_mat) |>
  pluck(1)

seas5_postrera <- corr_results |>
  filter(model == "SEAS5", window == "postrera") |>
  pull(corr_mat) |>
  pluck(1)

# Convert to long format for plotting (lower triangle only)
mat_to_df <- function(mat, label) {
  var_order <- c("LT4", "LT3", "LT2", "LT1", "OBS")
  # Filter to only include columns that exist in the matrix
  var_order <- var_order[var_order %in% colnames(mat)]

  as.data.frame(mat) |>
    mutate(row = rownames(mat)) |>
    pivot_longer(-row, names_to = "col", values_to = "cor") |>
    mutate(
      label = label,
      row_idx = match(row, var_order),
      col_idx = match(col, var_order)
    ) |>
    # Keep lower triangle only (row_idx > col_idx)
    filter(row_idx > col_idx) |>
    select(-row_idx, -col_idx)
}

df_seas5_corr <- bind_rows(
  mat_to_df(seas5_primera, "Primera"),
  mat_to_df(seas5_postrera, "Postrera")
) |> 
  mutate(
    label = fct_relevel(label, "Primera","Postrera")
  )

df_seas5_corr |>
  mutate(
    row = factor(row, levels = rev(c("LT4", "LT3", "LT2", "LT1", "OBS"))),
    col = factor(col, levels = c("LT4", "LT3", "LT2", "LT1", "OBS"))
  ) |>
  ggplot(aes(x = col, y = row, fill = cor)) +
  geom_tile(color = "white") +
  geom_text(aes(label = sprintf("%.2f", cor)), size = 3, fontface = "bold", color = "black") +
  facet_wrap(~label, scales = "free_x") +
  scale_fill_gradient2(low = "#D73027", mid = "white", high = "#1A9850", midpoint = 0.4, limits = c(-1, 1)) +
  labs(
    title = "SEAS5: Inter-Leadtime Correlations",
    subtitle = "Adjacent leadtimes should correlate strongly",
    x = NULL, y = NULL, fill = "Correlation",
    caption = "Omitting leadtime 0 as available from INSVIUMEH provided models"
  ) +
  theme(legend.position = "right",plot.caption = element_text(hjust= 0,vjust=-1))
```

**SEAS5 shows expected behavior**: Adjacent leadtimes correlate at 0.6-0.9. LT1 correlates best with observations. This is a well-behaved forecast system.

### INSIVUMEH Correlation Patterns

```{r}
#| label: insiv-corr-plot
#| fig-height: 8

# Extract INSIVUMEH matrices for all models
insiv_models <- c("CFSv2", "CCSM4", "CESM1")

df_insiv_corr <- map_dfr(insiv_models, function(m) {
  map_dfr(windows, function(w) {
    mat <- corr_results |>
      filter(model == m, window == w) |>
      pull(corr_mat) |>
      pluck(1)

    if (is.null(mat)) return(NULL)

    mat_to_df(mat, m) |>
      mutate(window = w)
  })
})

df_insiv_corr |>
  filter(!is.na(row), !is.na(col)) |>
  mutate(
    row = factor(row, levels = rev(c("LT4", "LT3", "LT2", "LT1", "OBS"))),
    col = factor(col, levels = c("LT4", "LT3", "LT2", "LT1", "OBS")),
    window = str_to_title(window),
    window = fct_relevel(window, "Primera","Postrera")
  ) |>
  ggplot(aes(x = col, y = row, fill = cor)) +
  geom_tile(color = "white") +
  geom_text(aes(label = sprintf("%.2f", cor)), size = 2.5, fontface = "bold", color = "black") +
  facet_grid(label ~ window, scales = "free_x") +
  scale_fill_gradient2(low = "#D73027", mid = "white", high = "#1A9850", midpoint = 0.4, limits = c(-1, 1)) +
  labs(
    title = "INSIVUMEH Models: Inter-Leadtime Correlations",
    subtitle = "Postrera shows near-zero or negative correlations between leadtimes",
    x = NULL, y = NULL, fill = "Correlation"
  ) +
  theme(legend.position = "right")
```

**Critical finding**: INSIVUMEH Postrera forecasts show near-zero or negative correlations between leadtimes. When LT1 predicts wet, LT2 might predict dry for the same target season. This is a fundamental data quality issue—these forecasts are not internally consistent.

## Coefficient of Variation

Another diagnostic: do forecasts show realistic interannual variability? A forecast that barely varies from year to year isn't providing useful information, even if it occasionally correlates with observations.

```{r}
#| label: cv-calculation
#| code-summary: "Calculate CV for forecasts and observations"

# Forecast CV
fcst_cv <- df_fcst_all |>
  filter(leadtime == 1) |>
  group_by(model, window) |>
  summarise(
    mean_mm = mean(value),
    sd_mm = sd(value),
    cv_pct = sd_mm / mean_mm * 100,
    .groups = "drop"
  )

# Observation CV
obs_cv <- df_enacts |>
  filter(year >= BASELINE_START, year <= BASELINE_END) |>
  group_by(window) |>
  summarise(
    mean_mm = mean(obs_mm),
    sd_mm = sd(obs_mm),
    cv_pct = sd_mm / mean_mm * 100,
    .groups = "drop"
  ) |>
  mutate(model = "ENACTS (Observed)")

# Combine
cv_all <- bind_rows(
  fcst_cv |> mutate(type = "Forecast"),
  obs_cv |> mutate(type = "Observed")
)
```

```{r}
#| label: cv-plot
#| fig-height: 5

cv_all |>
  mutate(
    model = factor(model, levels = c("ENACTS (Observed)", "SEAS5", "CFSv2", "CCSM4", "CESM1"))
  ) |>
  ggplot(aes(x = model, y = cv_pct, fill = type)) +
  geom_col(alpha = 0.8) +
  geom_hline(
    data = obs_cv,
    aes(yintercept = cv_pct),
    linetype = "dashed", color = "grey30"
  ) +
  facet_wrap(~str_to_title(window)) +
  labs(
    title = "Coefficient of Variation: Forecasts vs Observations",
    subtitle = "Dashed line = observed variability. Forecasts should approach this level.",
    x = NULL, y = "CV (%)", fill = NULL
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")
```

```{r}
#| label: cv-table

cv_all |>
  select(model, window, cv_pct) |>
  pivot_wider(names_from = window, values_from = cv_pct) |>
  mutate(across(where(is.numeric), ~sprintf("%.1f%%", .x))) |>
  knitr::kable(
    caption = "Coefficient of Variation by Model and Season",
    col.names = c("Model", "Primera", "Postrera")
  )
```

**Key findings:**

- **Observed CV**: Primera ~14%, Postrera ~23%
- **SEAS5**: Reasonable CV for both seasons (8-15%), approaching observed variability
- **INSIVUMEH Primera**: Moderate CV (10-18%), usable signal
- **INSIVUMEH Postrera**: Very low CV (3-7%), forecasts barely vary from climatology

### Robustness Check: CV Across Leadtimes and Observation Sources

Does the low CV pattern for INSIVUMEH Postrera hold across all leadtimes? And how does it compare to multiple observation sources?

```{r}
#| label: cv-expanded-data
#| code-summary: "Load additional observation sources and calculate CV across leadtimes"

# Forecast CV across all leadtimes
fcst_cv_all_lt <- df_fcst_all |>
  group_by(model, window, leadtime) |>
  summarise(
    mean_mm = mean(value),
    sd_mm = sd(value),
    cv_pct = sd_mm / mean_mm * 100,
    .groups = "drop"
  )

# Load ERA5
con <- pg_con()
df_era5_raw <- tbl(con, "era5") |>
  filter(pcode == "GT20") |>
  collect() |>
  mutate(
    year = year(valid_date),
    month = month(valid_date),
    mean = mean * days_in_month(valid_date)
  )
DBI::dbDisconnect(con)

df_era5 <- bind_rows(
  df_era5_raw |>
    filter(month %in% PRIMERA_MONTHS) |>
    group_by(year) |>
    summarise(obs_mm = sum(mean), .groups = "drop") |>
    mutate(window = "primera"),
  df_era5_raw |>
    filter(month %in% POSTRERA_MONTHS) |>
    group_by(year) |>
    summarise(obs_mm = sum(mean), .groups = "drop") |>
    mutate(window = "postrera")
) |>
  filter(year >= BASELINE_START, year <= BASELINE_END)

# Load CHIRPS
df_chirps_raw <- cumulus::blob_read(
  name = "ds-aa-lac-dry-corridor/raw/chirps/2026_cadc_drought_v3_aoi_chirps_monthly_historical.parquet",
  container = "projects"
)

df_chirps <- df_chirps_raw |>
  filter(ADM1_NAME == "Chiquimula") |>
  mutate(
    year = year(date),
    month = month(date),
    window = case_when(
      month %in% PRIMERA_MONTHS ~ "primera",
      month %in% POSTRERA_MONTHS ~ "postrera",
      TRUE ~ NA_character_
    )
  ) |>
  filter(!is.na(window), year >= BASELINE_START, year <= BASELINE_END) |>
  group_by(year, window) |>
  summarise(obs_mm = sum(value, na.rm = TRUE), .groups = "drop")

# Calculate observed CV from all three sources
obs_cv_all <- bind_rows(
  df_enacts |>
    filter(year >= BASELINE_START, year <= BASELINE_END) |>
    group_by(window) |>
    summarise(cv_pct = sd(obs_mm) / mean(obs_mm) * 100, .groups = "drop") |>
    mutate(source = "ENACTS"),
  df_era5 |>
    group_by(window) |>
    summarise(cv_pct = sd(obs_mm) / mean(obs_mm) * 100, .groups = "drop") |>
    mutate(source = "ERA5"),
  df_chirps |>
    group_by(window) |>
    summarise(cv_pct = sd(obs_mm) / mean(obs_mm) * 100, .groups = "drop") |>
    mutate(source = "CHIRPS")
)
```

```{r}
#| label: cv-expanded-plot
#| fig-height: 6

fcst_cv_all_lt |>
  mutate(
    window = str_to_title(window),
    window = fct_relevel(window, "Primera", "Postrera")
  ) |>
  ggplot(aes(x = factor(leadtime), y = cv_pct, fill = model)) +
  geom_col(position = "dodge", alpha = 0.8) +
  geom_hline(
    data = obs_cv_all |> mutate(window = str_to_title(window)),
    aes(yintercept = cv_pct, linetype = source),
    color = "grey30"
  ) +
  facet_wrap(~window) +
  scale_linetype_manual(values = c("ENACTS" = "dashed", "ERA5" = "dotted", "CHIRPS" = "dotdash")) +
  labs(
    title = "Coefficient of Variation Across Leadtimes",
    subtitle = "Horizontal lines = observed CV from three sources",
    x = "Leadtime", y = "CV (%)", fill = "Model", linetype = "Observed"
  ) +
  theme(legend.position = "bottom")
```

```{r}
#| label: cv-expanded-table

obs_cv_all |>
  mutate(window = str_to_title(window)) |>
  pivot_wider(names_from = window, values_from = cv_pct) |>
  mutate(across(where(is.numeric), ~sprintf("%.1f%%", .x))) |>
  knitr::kable(
    caption = "Observed CV by Source",
    col.names = c("Source", "Primera", "Postrera")
  )
```

The pattern holds: **INSIVUMEH Postrera CV remains very low (3-7%) across all leadtimes**, far below observed variability from any source. This is not a leadtime-specific issue—the Postrera forecasts systematically lack interannual signal.

## Implications

The diagnostic tests reveal a fundamental issue with INSIVUMEH Postrera forecasts:

| Diagnostic | SEAS5 | INSIVUMEH Primera | INSIVUMEH Postrera |
|------------|-------|-------------------|-------------------|
| Inter-LT correlation | High (0.6-0.9) | Moderate (0.3-0.7) | Near-zero or negative |
| CV vs observed | ~70% of observed | ~80% of observed | ~20% of observed |
| Data quality | Good | Acceptable | **Problematic** |

This explains the suspicious patterns in @sec-chirps:

1. **Why CCSM4 LT3 > LT1 for Postrera**: If leadtimes are uncorrelated, apparent "skill" at any leadtime is random. By chance, LT3 happened to correlate better with observations than LT1 in this sample.

2. **Why INSIVUMEH Postrera shows erratic skill**: The forecasts contain almost no signal—they're essentially flat lines near climatology with random noise.

3. **Why skill scores are unreliable**: When forecasts don't vary, correlations are dominated by noise and small-sample coincidences.

## The Real Tie-Breaker for Postrera

The skill metrics in earlier chapters were inconclusive for Postrera—no model showed consistent skill, and the patterns were erratic. **These diagnostics break the tie.**

The issue isn't that INSIVUMEH Postrera forecasts lack skill—it's that they lack *signal*. A forecast that barely varies from climatology and shows no internal consistency between leadtimes cannot be operationally useful, regardless of what correlation metrics suggest.

**Postrera recommendation: SEAS5**

Not because SEAS5 has demonstrated strong Postrera skill (it hasn't), but because:

1. SEAS5 is internally consistent—different leadtimes agree with each other
2. SEAS5 shows realistic interannual variability
3. SEAS5 at least *behaves* like a forecast, even if predictability is limited

INSIVUMEH Postrera forecasts are essentially noise dressed up as predictions.

## Final Recommendations

| Season | Recommended | Rationale |
|--------|-------------|-----------|
| **Primera** | SEAS5 | Clear skill advantage across metrics and observation sources |
| **Postrera** | SEAS5 | INSIVUMEH forecasts are unreliable (low CV, inconsistent leadtimes) |

For Postrera specifically, consider:

- Acknowledging limited predictability in operational communications
- Using shorter-range forecasts when available
- Weighting climatological probabilities more heavily than dynamic forecasts
